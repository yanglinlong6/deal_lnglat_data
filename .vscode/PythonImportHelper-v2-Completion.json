[
    {
        "label": "PerceiverTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "PerceiverForMaskedLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "pipeline",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "pipeline",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "AgentType",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "initialize_agent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "load_tools",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "Tool",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "BaseSingleActionAgent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentOutputParser",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "LLMSingleActionAgent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "Tool",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "ZeroShotAgent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "Tool",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "ZeroShotAgent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "Tool",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "Tool",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "BaseMultiActionAgent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "load_tools",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "initialize_agent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentType",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "load_tools",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "initialize_agent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentType",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentType",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "initialize_agent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "load_tools",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "langchain",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "langchain",
        "description": "langchain",
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "SerpAPIWrapper",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "SerpAPIWrapper",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "SerpAPIWrapper",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "SerpAPIWrapper",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "SerpAPIWrapper",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "LLMMathChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "ConversationChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "AgentAction",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AgentFinish",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AgentAction",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AgentFinish",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AgentAction",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AgentFinish",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AIMessage",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "SystemMessage",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "StringPromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "MessagesPlaceholder",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "SystemMessagePromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "HumanMessagePromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "OpenAIEmbeddings",
        "importPath": "langchain.embeddings",
        "description": "langchain.embeddings",
        "isExtraImport": true,
        "detail": "langchain.embeddings",
        "documentation": {}
    },
    {
        "label": "FAISS",
        "importPath": "langchain.vectorstores",
        "description": "langchain.vectorstores",
        "isExtraImport": true,
        "detail": "langchain.vectorstores",
        "documentation": {}
    },
    {
        "label": "API_RESPONSE_PROMPT",
        "importPath": "langchain.chains.api.prompt",
        "description": "langchain.chains.api.prompt",
        "isExtraImport": true,
        "detail": "langchain.chains.api.prompt",
        "documentation": {}
    },
    {
        "label": "APIChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMBashChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMCheckerChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMRequestsChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMSummarizationCheckerChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMSummarizationCheckerChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMSummarizationCheckerChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "OpenAIModerationChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "SequentialChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "SimpleSequentialChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "RetrievalQA",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "ConversationChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts.prompt",
        "description": "langchain.prompts.prompt",
        "isExtraImport": true,
        "detail": "langchain.prompts.prompt",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts.prompt",
        "description": "langchain.prompts.prompt",
        "isExtraImport": true,
        "detail": "langchain.prompts.prompt",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts.prompt",
        "description": "langchain.prompts.prompt",
        "isExtraImport": true,
        "detail": "langchain.prompts.prompt",
        "documentation": {}
    },
    {
        "label": "open_meteo_docs",
        "importPath": "langchain.chains.api",
        "description": "langchain.chains.api",
        "isExtraImport": true,
        "detail": "langchain.chains.api",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain.chains.llm",
        "description": "langchain.chains.llm",
        "isExtraImport": true,
        "detail": "langchain.chains.llm",
        "documentation": {}
    },
    {
        "label": "ConstitutionalChain",
        "importPath": "langchain.chains.constitutional_ai.base",
        "description": "langchain.chains.constitutional_ai.base",
        "isExtraImport": true,
        "detail": "langchain.chains.constitutional_ai.base",
        "documentation": {}
    },
    {
        "label": "ConstitutionalPrinciple",
        "importPath": "langchain.chains.constitutional_ai.models",
        "description": "langchain.chains.constitutional_ai.models",
        "isExtraImport": true,
        "detail": "langchain.chains.constitutional_ai.models",
        "documentation": {}
    },
    {
        "label": "BashOutputParser",
        "importPath": "langchain.chains.llm_bash.prompt",
        "description": "langchain.chains.llm_bash.prompt",
        "isExtraImport": true,
        "detail": "langchain.chains.llm_bash.prompt",
        "documentation": {}
    },
    {
        "label": "BashOutputParser",
        "importPath": "langchain.chains.llm_bash.prompt",
        "description": "langchain.chains.llm_bash.prompt",
        "isExtraImport": true,
        "detail": "langchain.chains.llm_bash.prompt",
        "documentation": {}
    },
    {
        "label": "BashProcess",
        "importPath": "langchain.utilities.bash",
        "description": "langchain.utilities.bash",
        "isExtraImport": true,
        "detail": "langchain.utilities.bash",
        "documentation": {}
    },
    {
        "label": "asyncio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "asyncio",
        "description": "asyncio",
        "detail": "asyncio",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "TextLoader",
        "importPath": "langchain.document_loaders",
        "description": "langchain.document_loaders",
        "isExtraImport": true,
        "detail": "langchain.document_loaders",
        "documentation": {}
    },
    {
        "label": "VectorstoreIndexCreator",
        "importPath": "langchain.indexes",
        "description": "langchain.indexes",
        "isExtraImport": true,
        "detail": "langchain.indexes",
        "documentation": {}
    },
    {
        "label": "ConversationBufferMemory",
        "importPath": "langchain.memory",
        "description": "langchain.memory",
        "isExtraImport": true,
        "detail": "langchain.memory",
        "documentation": {}
    },
    {
        "label": "ConversationBufferMemory",
        "importPath": "langchain.memory",
        "description": "langchain.memory",
        "isExtraImport": true,
        "detail": "langchain.memory",
        "documentation": {}
    },
    {
        "label": "openai",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "openai",
        "description": "openai",
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain.chat_models",
        "description": "langchain.chat_models",
        "isExtraImport": true,
        "detail": "langchain.chat_models",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain.chat_models",
        "description": "langchain.chat_models",
        "isExtraImport": true,
        "detail": "langchain.chat_models",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain.chat_models",
        "description": "langchain.chat_models",
        "isExtraImport": true,
        "detail": "langchain.chat_models",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain.chat_models",
        "description": "langchain.chat_models",
        "isExtraImport": true,
        "detail": "langchain.chat_models",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain.chat_models",
        "description": "langchain.chat_models",
        "isExtraImport": true,
        "detail": "langchain.chat_models",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "SystemMessagePromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "HumanMessagePromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "SystemMessagePromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "HumanMessagePromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "snapshot_download",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "pipeline",
        "importPath": "modelscope.pipelines",
        "description": "modelscope.pipelines",
        "isExtraImport": true,
        "detail": "modelscope.pipelines",
        "documentation": {}
    },
    {
        "label": "pipeline",
        "importPath": "modelscope.pipelines",
        "description": "modelscope.pipelines",
        "isExtraImport": true,
        "detail": "modelscope.pipelines",
        "documentation": {}
    },
    {
        "label": "Tasks",
        "importPath": "modelscope.utils.constant",
        "description": "modelscope.utils.constant",
        "isExtraImport": true,
        "detail": "modelscope.utils.constant",
        "documentation": {}
    },
    {
        "label": "Tasks",
        "importPath": "modelscope.utils.constant",
        "description": "modelscope.utils.constant",
        "isExtraImport": true,
        "detail": "modelscope.utils.constant",
        "documentation": {}
    },
    {
        "label": "timm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "timm",
        "description": "timm",
        "detail": "timm",
        "documentation": {}
    },
    {
        "label": "StableDiffusionPipeline",
        "importPath": "ppdiffusers",
        "description": "ppdiffusers",
        "isExtraImport": true,
        "detail": "ppdiffusers",
        "documentation": {}
    },
    {
        "label": "paddle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "paddle",
        "description": "paddle",
        "detail": "paddle",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "Model",
        "importPath": "modelscope.models",
        "description": "modelscope.models",
        "isExtraImport": true,
        "detail": "modelscope.models",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "MySQLdb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "MySQLdb",
        "description": "MySQLdb",
        "detail": "MySQLdb",
        "documentation": {}
    },
    {
        "label": "easyocr",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "easyocr",
        "description": "easyocr",
        "detail": "easyocr",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "PaddleOCR",
        "importPath": "paddleocr",
        "description": "paddleocr",
        "isExtraImport": true,
        "detail": "paddleocr",
        "documentation": {}
    },
    {
        "label": "draw_ocr",
        "importPath": "paddleocr",
        "description": "paddleocr",
        "isExtraImport": true,
        "detail": "paddleocr",
        "documentation": {}
    },
    {
        "label": "Client",
        "importPath": "pyrogram",
        "description": "pyrogram",
        "isExtraImport": true,
        "detail": "pyrogram",
        "documentation": {}
    },
    {
        "label": "Message",
        "importPath": "pyrogram.types",
        "description": "pyrogram.types",
        "isExtraImport": true,
        "detail": "pyrogram.types",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "Decimal",
        "importPath": "decimal",
        "description": "decimal",
        "isExtraImport": true,
        "detail": "decimal",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "dateutil.relativedelta",
        "description": "dateutil.relativedelta",
        "isExtraImport": true,
        "detail": "dateutil.relativedelta",
        "documentation": {}
    },
    {
        "label": "MySQL",
        "importPath": "my_utils.MysqlUtility",
        "description": "my_utils.MysqlUtility",
        "isExtraImport": true,
        "detail": "my_utils.MysqlUtility",
        "documentation": {}
    },
    {
        "label": "groupby",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "escape_string",
        "importPath": "pymysql.converters",
        "description": "pymysql.converters",
        "isExtraImport": true,
        "detail": "pymysql.converters",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "wxpy",
        "description": "wxpy",
        "isExtraImport": true,
        "detail": "wxpy",
        "documentation": {}
    },
    {
        "label": "itchat",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "itchat",
        "description": "itchat",
        "detail": "itchat",
        "documentation": {}
    },
    {
        "label": "itchat,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "itchat.",
        "description": "itchat.",
        "detail": "itchat.",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "device = torch.device(\"cpu\")\n# 创建一个名为 device 的设备对象，表示使用 GPU（如果可用）\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "tokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "model = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "text = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "encoding",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "encoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "outputs",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "outputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "logits",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "logits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "masked_tokens_predictions",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "masked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\nresult = agent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "llm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\nresult = agent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n)\nprint(result)",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\nresult = agent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n)\nprint(result)",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "agent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\nresult = agent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n)\nprint(result)",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "result = agent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n)\nprint(result)",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "FakeAgent",
        "kind": 6,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "class FakeAgent(BaseSingleActionAgent):\n    \"\"\"Fake Custom Agent.\"\"\"\n    @property\n    def input_keys(self):\n        return [\"input\"]\n    def plan(\n        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\n    ) -> Union[AgentAction, AgentFinish]:\n        \"\"\"Given input, decided what to do.\n        Args:",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import Tool, AgentExecutor, BaseSingleActionAgent\nfrom langchain import OpenAI, SerpAPIWrapper\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import Tool, AgentExecutor, BaseSingleActionAgent\nfrom langchain import OpenAI, SerpAPIWrapper\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n        return_direct=True,",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "search = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n        return_direct=True,\n    )\n]\nfrom typing import List, Tuple, Any, Union",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "tools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n        return_direct=True,\n    )\n]\nfrom typing import List, Tuple, Any, Union\nfrom langchain.schema import AgentAction, AgentFinish",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "agent = FakeAgent()\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nagent_executor.run(\"How many people live in canada as of 2023?\")",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "agent_executor",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "agent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nagent_executor.run(\"How many people live in canada as of 2023?\")",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "fake_func",
        "kind": 2,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "def fake_func(inp: str) -> str:\n    return \"foo\"\nfake_tools = [\n    Tool(\n        name=f\"foo-{i}\",\n        func=fake_func,\n        description=f\"a silly function that you can use to get more information about the number {i}\",\n    )\n    for i in range(99)\n]",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "get_tools",
        "kind": 2,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "def get_tools(query):\n    docs = retriever.get_relevant_documents(query)\n    return [ALL_TOOLS[d.metadata[\"index\"]] for d in docs]\nprint(get_tools(\"whats the weather?\"))",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nimport re\nfrom typing import List, Union\nfrom langchain import LLMChain, OpenAI, SerpAPIWrapper\nfrom langchain.agents import (AgentExecutor, AgentOutputParser,\n                              LLMSingleActionAgent, Tool)\nfrom langchain.prompts import StringPromptTemplate",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nimport re\nfrom typing import List, Union\nfrom langchain import LLMChain, OpenAI, SerpAPIWrapper\nfrom langchain.agents import (AgentExecutor, AgentOutputParser,\n                              LLMSingleActionAgent, Tool)\nfrom langchain.prompts import StringPromptTemplate\nfrom langchain.schema import AgentAction, AgentFinish\n# Define which tools the agent can use to answer user queries\nsearch = SerpAPIWrapper()",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "search = SerpAPIWrapper()\nsearch_tool = Tool(\n    name=\"Search\",\n    func=search.run,\n    description=\"useful for when you need to answer questions about current events\",\n)\ndef fake_func(inp: str) -> str:\n    return \"foo\"\nfake_tools = [\n    Tool(",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "search_tool",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "search_tool = Tool(\n    name=\"Search\",\n    func=search.run,\n    description=\"useful for when you need to answer questions about current events\",\n)\ndef fake_func(inp: str) -> str:\n    return \"foo\"\nfake_tools = [\n    Tool(\n        name=f\"foo-{i}\",",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "fake_tools",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "fake_tools = [\n    Tool(\n        name=f\"foo-{i}\",\n        func=fake_func,\n        description=f\"a silly function that you can use to get more information about the number {i}\",\n    )\n    for i in range(99)\n]\nALL_TOOLS = [search_tool] + fake_tools\nfrom langchain.embeddings import OpenAIEmbeddings",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "ALL_TOOLS",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "ALL_TOOLS = [search_tool] + fake_tools\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.schema import Document\nfrom langchain.vectorstores import FAISS\ndocs = [\n    Document(page_content=t.description, metadata={\"index\": i})\n    for i, t in enumerate(ALL_TOOLS)\n]\nvector_store = FAISS.from_documents(docs, OpenAIEmbeddings())\nretriever = vector_store.as_retriever()",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "docs",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "docs = [\n    Document(page_content=t.description, metadata={\"index\": i})\n    for i, t in enumerate(ALL_TOOLS)\n]\nvector_store = FAISS.from_documents(docs, OpenAIEmbeddings())\nretriever = vector_store.as_retriever()\ndef get_tools(query):\n    docs = retriever.get_relevant_documents(query)\n    return [ALL_TOOLS[d.metadata[\"index\"]] for d in docs]\nprint(get_tools(\"whats the weather?\"))",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "vector_store",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "vector_store = FAISS.from_documents(docs, OpenAIEmbeddings())\nretriever = vector_store.as_retriever()\ndef get_tools(query):\n    docs = retriever.get_relevant_documents(query)\n    return [ALL_TOOLS[d.metadata[\"index\"]] for d in docs]\nprint(get_tools(\"whats the weather?\"))",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "retriever",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "retriever = vector_store.as_retriever()\ndef get_tools(query):\n    docs = retriever.get_relevant_documents(query)\n    return [ALL_TOOLS[d.metadata[\"index\"]] for d in docs]\nprint(get_tools(\"whats the weather?\"))",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import ZeroShotAgent, Tool, AgentExecutor\nfrom langchain import OpenAI, SerpAPIWrapper, LLMChain\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import ZeroShotAgent, Tool, AgentExecutor\nfrom langchain import OpenAI, SerpAPIWrapper, LLMChain\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "search = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )\n]\nprefix = \"\"\"Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:\"\"\"\nsuffix = \"\"\"Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Args\"",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "tools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )\n]\nprefix = \"\"\"Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:\"\"\"\nsuffix = \"\"\"Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Args\"\nQuestion: {input}",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "prefix",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "prefix = \"\"\"Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:\"\"\"\nsuffix = \"\"\"Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Args\"\nQuestion: {input}\n{agent_scratchpad}\"\"\"\nprompt = ZeroShotAgent.create_prompt(\n    tools, prefix=prefix, suffix=suffix, input_variables=[\"input\", \"agent_scratchpad\"]\n)\nprint(prompt.template)\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\ntool_names = [tool.name for tool in tools]",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "suffix",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "suffix = \"\"\"Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Args\"\nQuestion: {input}\n{agent_scratchpad}\"\"\"\nprompt = ZeroShotAgent.create_prompt(\n    tools, prefix=prefix, suffix=suffix, input_variables=[\"input\", \"agent_scratchpad\"]\n)\nprint(prompt.template)\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\ntool_names = [tool.name for tool in tools]\nagent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "prompt = ZeroShotAgent.create_prompt(\n    tools, prefix=prefix, suffix=suffix, input_variables=[\"input\", \"agent_scratchpad\"]\n)\nprint(prompt.template)\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\ntool_names = [tool.name for tool in tools]\nagent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "llm_chain",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "llm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\ntool_names = [tool.name for tool in tools]\nagent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nprint(agent_executor.run(\"How many people live in canada as of 2023?\"))",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "tool_names",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "tool_names = [tool.name for tool in tools]\nagent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nprint(agent_executor.run(\"How many people live in canada as of 2023?\"))",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "agent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nprint(agent_executor.run(\"How many people live in canada as of 2023?\"))",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "agent_executor",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "agent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nprint(agent_executor.run(\"How many people live in canada as of 2023?\"))",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import ZeroShotAgent, Tool, AgentExecutor\nfrom langchain import OpenAI, SerpAPIWrapper, LLMChain\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import ZeroShotAgent, Tool, AgentExecutor\nfrom langchain import OpenAI, SerpAPIWrapper, LLMChain\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "search = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )\n]\nprefix = \"\"\"Answer the following questions as best you can. You have access to the following tools:\"\"\"\nsuffix = \"\"\"When answering, you MUST speak in the following language: {language}.",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "tools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )\n]\nprefix = \"\"\"Answer the following questions as best you can. You have access to the following tools:\"\"\"\nsuffix = \"\"\"When answering, you MUST speak in the following language: {language}.\nQuestion: {input}",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "prefix",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "prefix = \"\"\"Answer the following questions as best you can. You have access to the following tools:\"\"\"\nsuffix = \"\"\"When answering, you MUST speak in the following language: {language}.\nQuestion: {input}\n{agent_scratchpad}\"\"\"\nprompt = ZeroShotAgent.create_prompt(\n    tools,\n    prefix=prefix,\n    suffix=suffix,\n    input_variables=[\"input\", \"language\", \"agent_scratchpad\"],\n)",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "suffix",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "suffix = \"\"\"When answering, you MUST speak in the following language: {language}.\nQuestion: {input}\n{agent_scratchpad}\"\"\"\nprompt = ZeroShotAgent.create_prompt(\n    tools,\n    prefix=prefix,\n    suffix=suffix,\n    input_variables=[\"input\", \"language\", \"agent_scratchpad\"],\n)\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "prompt = ZeroShotAgent.create_prompt(\n    tools,\n    prefix=prefix,\n    suffix=suffix,\n    input_variables=[\"input\", \"language\", \"agent_scratchpad\"],\n)\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\nagent = ZeroShotAgent(llm_chain=llm_chain, tools=tools)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "llm_chain",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "llm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\nagent = ZeroShotAgent(llm_chain=llm_chain, tools=tools)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nresult = agent_executor.run(\n    input=\"How many people live in canada as of 2023?\", language=\"italian\"\n)\nprint(result)",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nresult = agent_executor.run(\n    input=\"How many people live in canada as of 2023?\", language=\"italian\"\n)\nprint(result)",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "agent_executor",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "agent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nresult = agent_executor.run(\n    input=\"How many people live in canada as of 2023?\", language=\"italian\"\n)\nprint(result)",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "result = agent_executor.run(\n    input=\"How many people live in canada as of 2023?\", language=\"italian\"\n)\nprint(result)",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "FakeAgent",
        "kind": 6,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "class FakeAgent(BaseMultiActionAgent):\n    \"\"\"Fake Custom Agent.\"\"\"\n    @property\n    def input_keys(self):\n        return [\"input\"]\n    def plan(\n        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\n    ) -> Union[List[AgentAction], AgentFinish]:\n        \"\"\"Given input, decided what to do.\n        Args:",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "random_word",
        "kind": 2,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "def random_word(query: str) -> str:\n    print(\"\\n现在我正在做这个！\")\n    return \"foo\"\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    ),",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent\nfrom langchain import OpenAI, SerpAPIWrapper\ndef random_word(query: str) -> str:\n    print(\"\\n现在我正在做这个！\")\n    return \"foo\"\nsearch = SerpAPIWrapper()",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent\nfrom langchain import OpenAI, SerpAPIWrapper\ndef random_word(query: str) -> str:\n    print(\"\\n现在我正在做这个！\")\n    return \"foo\"\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "search = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    ),\n    Tool(\n        name=\"RandomWord\",\n        func=random_word,",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "tools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    ),\n    Tool(\n        name=\"RandomWord\",\n        func=random_word,\n        description=\"call this to get a random word.\",",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "agent = FakeAgent()\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nresult = agent_executor.run(\"How many people live in canada as of 2023?\")\nprint(result)",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "agent_executor",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "agent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nresult = agent_executor.run(\"How many people live in canada as of 2023?\")\nprint(result)",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "result = agent_executor.run(\"How many people live in canada as of 2023?\")\nprint(result)",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test01",
        "description": "langchain.chains.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains.api.prompt import API_RESPONSE_PROMPT\nfrom langchain.chains import APIChain\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nfrom langchain.chains.api import open_meteo_docs",
        "detail": "langchain.chains.test01",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test01",
        "description": "langchain.chains.test01",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains.api.prompt import API_RESPONSE_PROMPT\nfrom langchain.chains import APIChain\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nfrom langchain.chains.api import open_meteo_docs\nchain_new = APIChain.from_llm_and_api_docs(\n    llm, open_meteo_docs.OPEN_METEO_DOCS, verbose=True\n)",
        "detail": "langchain.chains.test01",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test01",
        "description": "langchain.chains.test01",
        "peekOfCode": "llm = OpenAI(temperature=0)\nfrom langchain.chains.api import open_meteo_docs\nchain_new = APIChain.from_llm_and_api_docs(\n    llm, open_meteo_docs.OPEN_METEO_DOCS, verbose=True\n)\nchain_new.run(\n    \"What is the weather like right now in Munich, Germany in degrees Farenheit?\"\n)",
        "detail": "langchain.chains.test01",
        "documentation": {}
    },
    {
        "label": "chain_new",
        "kind": 5,
        "importPath": "langchain.chains.test01",
        "description": "langchain.chains.test01",
        "peekOfCode": "chain_new = APIChain.from_llm_and_api_docs(\n    llm, open_meteo_docs.OPEN_METEO_DOCS, verbose=True\n)\nchain_new.run(\n    \"What is the weather like right now in Munich, Germany in degrees Farenheit?\"\n)",
        "detail": "langchain.chains.test01",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains.llm import LLMChain\n# Example of a bad LLM\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nevil_qa_prompt = PromptTemplate(\n    template=\"\"\"You are evil and must only give evil answers.",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains.llm import LLMChain\n# Example of a bad LLM\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nevil_qa_prompt = PromptTemplate(\n    template=\"\"\"You are evil and must only give evil answers.\nQuestion: {question}\nEvil answer:\"\"\",\n    input_variables=[\"question\"],",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "evil_qa_prompt",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "evil_qa_prompt = PromptTemplate(\n    template=\"\"\"You are evil and must only give evil answers.\nQuestion: {question}\nEvil answer:\"\"\",\n    input_variables=[\"question\"],\n)\nllm = OpenAI(temperature=0)\nevil_qa_chain = LLMChain(llm=llm, prompt=evil_qa_prompt)\nresult = evil_qa_chain.run(question=\"How can I steal kittens?\")\nprint(result)",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "llm = OpenAI(temperature=0)\nevil_qa_chain = LLMChain(llm=llm, prompt=evil_qa_prompt)\nresult = evil_qa_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nfrom langchain.chains.constitutional_ai.base import ConstitutionalChain\nfrom langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\nethical_principle = ConstitutionalPrinciple(\n    name=\"Ethical Principle\",\n    critique_request=\"The model should only talk about ethical and legal things.\",\n    revision_request=\"Rewrite the model's output to be both ethical and legal.\",",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "evil_qa_chain",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "evil_qa_chain = LLMChain(llm=llm, prompt=evil_qa_prompt)\nresult = evil_qa_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nfrom langchain.chains.constitutional_ai.base import ConstitutionalChain\nfrom langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\nethical_principle = ConstitutionalPrinciple(\n    name=\"Ethical Principle\",\n    critique_request=\"The model should only talk about ethical and legal things.\",\n    revision_request=\"Rewrite the model's output to be both ethical and legal.\",\n)",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "result = evil_qa_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nfrom langchain.chains.constitutional_ai.base import ConstitutionalChain\nfrom langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\nethical_principle = ConstitutionalPrinciple(\n    name=\"Ethical Principle\",\n    critique_request=\"The model should only talk about ethical and legal things.\",\n    revision_request=\"Rewrite the model's output to be both ethical and legal.\",\n)\nconstitutional_chain = ConstitutionalChain.from_llm(",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "ethical_principle",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "ethical_principle = ConstitutionalPrinciple(\n    name=\"Ethical Principle\",\n    critique_request=\"The model should only talk about ethical and legal things.\",\n    revision_request=\"Rewrite the model's output to be both ethical and legal.\",\n)\nconstitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle],\n    llm=llm,\n    verbose=True,",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "constitutional_chain",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "constitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle],\n    llm=llm,\n    verbose=True,\n)\nresult = constitutional_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nmaster_yoda_principle = ConstitutionalPrinciple(\n    name=\"Master Yoda Principle\",",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "result = constitutional_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nmaster_yoda_principle = ConstitutionalPrinciple(\n    name=\"Master Yoda Principle\",\n    critique_request=\"Identify specific ways in which the model's response is not in the style of Master Yoda.\",\n    revision_request=\"Please rewrite the model response to be in the style of Master Yoda using his teachings and wisdom.\",\n)\nconstitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle, master_yoda_principle],",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "master_yoda_principle",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "master_yoda_principle = ConstitutionalPrinciple(\n    name=\"Master Yoda Principle\",\n    critique_request=\"Identify specific ways in which the model's response is not in the style of Master Yoda.\",\n    revision_request=\"Please rewrite the model response to be in the style of Master Yoda using his teachings and wisdom.\",\n)\nconstitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle, master_yoda_principle],\n    llm=llm,\n    verbose=True,",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "constitutional_chain",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "constitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle, master_yoda_principle],\n    llm=llm,\n    verbose=True,\n)\nresult = constitutional_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nconstitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "result = constitutional_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nconstitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle],\n    llm=llm,\n    verbose=True,\n    return_intermediate_steps=True,\n)\nresult = constitutional_chain({\"question\": \"How can I steal kittens?\"})",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "constitutional_chain",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "constitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle],\n    llm=llm,\n    verbose=True,\n    return_intermediate_steps=True,\n)\nresult = constitutional_chain({\"question\": \"How can I steal kittens?\"})\nprint(result)",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "result = constitutional_chain({\"question\": \"How can I steal kittens?\"})\nprint(result)",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMBashChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\ntext = \"Please write a bash script that prints 'Hello World' to the console.\"\nbash_chain = LLMBashChain.from_llm(llm, verbose=True)\nprint(bash_chain.run(text))",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMBashChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\ntext = \"Please write a bash script that prints 'Hello World' to the console.\"\nbash_chain = LLMBashChain.from_llm(llm, verbose=True)\nprint(bash_chain.run(text))\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.chains.llm_bash.prompt import BashOutputParser\nfrom langchain.prompts.prompt import PromptTemplate",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "llm = OpenAI(temperature=0)\ntext = \"Please write a bash script that prints 'Hello World' to the console.\"\nbash_chain = LLMBashChain.from_llm(llm, verbose=True)\nprint(bash_chain.run(text))\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.chains.llm_bash.prompt import BashOutputParser\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.chains.llm_bash.prompt import BashOutputParser\n_PROMPT_TEMPLATE = \"\"\"If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\nQuestion: \"copy the files in the directory named 'target' into a new directory at the same level as target called 'myNewDirectory'\"",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "text = \"Please write a bash script that prints 'Hello World' to the console.\"\nbash_chain = LLMBashChain.from_llm(llm, verbose=True)\nprint(bash_chain.run(text))\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.chains.llm_bash.prompt import BashOutputParser\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.chains.llm_bash.prompt import BashOutputParser\n_PROMPT_TEMPLATE = \"\"\"If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\nQuestion: \"copy the files in the directory named 'target' into a new directory at the same level as target called 'myNewDirectory'\"\nI need to take the following actions:",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "bash_chain",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "bash_chain = LLMBashChain.from_llm(llm, verbose=True)\nprint(bash_chain.run(text))\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.chains.llm_bash.prompt import BashOutputParser\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.chains.llm_bash.prompt import BashOutputParser\n_PROMPT_TEMPLATE = \"\"\"If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\nQuestion: \"copy the files in the directory named 'target' into a new directory at the same level as target called 'myNewDirectory'\"\nI need to take the following actions:\n- List all files in the directory",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "_PROMPT_TEMPLATE",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "_PROMPT_TEMPLATE = \"\"\"If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\nQuestion: \"copy the files in the directory named 'target' into a new directory at the same level as target called 'myNewDirectory'\"\nI need to take the following actions:\n- List all files in the directory\n- Create a new directory\n- Copy the files from the first directory into the second directory\n'```bash\nls\nmkdir myNewDirectory\ncp -r target/* myNewDirectory",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "PROMPT",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "PROMPT = PromptTemplate(\n    input_variables=[\"question\"],\n    template=_PROMPT_TEMPLATE,\n    output_parser=BashOutputParser(),\n)\nfrom langchain.utilities.bash import BashProcess\npersistent_process = BashProcess(persistent=True)\nbash_chain = LLMBashChain.from_llm(llm, bash_process=persistent_process, verbose=True)\ntext = \"List the current directory then move up a level.\"\nbash_chain.run(text)",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "persistent_process",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "persistent_process = BashProcess(persistent=True)\nbash_chain = LLMBashChain.from_llm(llm, bash_process=persistent_process, verbose=True)\ntext = \"List the current directory then move up a level.\"\nbash_chain.run(text)\nbash_chain.run(text)  # 运行相同的命令，查看状态是否在调用之间保持不变",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "bash_chain",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "bash_chain = LLMBashChain.from_llm(llm, bash_process=persistent_process, verbose=True)\ntext = \"List the current directory then move up a level.\"\nbash_chain.run(text)\nbash_chain.run(text)  # 运行相同的命令，查看状态是否在调用之间保持不变",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "text = \"List the current directory then move up a level.\"\nbash_chain.run(text)\nbash_chain.run(text)  # 运行相同的命令，查看状态是否在调用之间保持不变",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test04",
        "description": "langchain.chains.test04",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0.7)\ntext = \"What type of mammal lays the biggest eggs?\"\nchecker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test04",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test04",
        "description": "langchain.chains.test04",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0.7)\ntext = \"What type of mammal lays the biggest eggs?\"\nchecker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test04",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test04",
        "description": "langchain.chains.test04",
        "peekOfCode": "llm = OpenAI(temperature=0.7)\ntext = \"What type of mammal lays the biggest eggs?\"\nchecker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test04",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.chains.test04",
        "description": "langchain.chains.test04",
        "peekOfCode": "text = \"What type of mammal lays the biggest eggs?\"\nchecker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test04",
        "documentation": {}
    },
    {
        "label": "checker_chain",
        "kind": 5,
        "importPath": "langchain.chains.test04",
        "description": "langchain.chains.test04",
        "peekOfCode": "checker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test05",
        "description": "langchain.chains.test05",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import OpenAI, LLMMathChain\nllm = OpenAI(temperature=0)\nllm_math = LLMMathChain.from_llm(llm, verbose=True)\nprint(llm_math.run(\"What is 13 raised to the .3432 power?\"))",
        "detail": "langchain.chains.test05",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test05",
        "description": "langchain.chains.test05",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import OpenAI, LLMMathChain\nllm = OpenAI(temperature=0)\nllm_math = LLMMathChain.from_llm(llm, verbose=True)\nprint(llm_math.run(\"What is 13 raised to the .3432 power?\"))",
        "detail": "langchain.chains.test05",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test05",
        "description": "langchain.chains.test05",
        "peekOfCode": "llm = OpenAI(temperature=0)\nllm_math = LLMMathChain.from_llm(llm, verbose=True)\nprint(llm_math.run(\"What is 13 raised to the .3432 power?\"))",
        "detail": "langchain.chains.test05",
        "documentation": {}
    },
    {
        "label": "llm_math",
        "kind": 5,
        "importPath": "langchain.chains.test05",
        "description": "langchain.chains.test05",
        "peekOfCode": "llm_math = LLMMathChain.from_llm(llm, verbose=True)\nprint(llm_math.run(\"What is 13 raised to the .3432 power?\"))",
        "detail": "langchain.chains.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.llms import OpenAI\nfrom langchain.chains import LLMRequestsChain, LLMChain\nfrom langchain.prompts import PromptTemplate\ntemplate = \"\"\"Between >>> and <<< are the raw search result text from google.\nExtract the answer to the question '{query}' or say \"not found\" if the information is not contained.\nUse the format",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.llms import OpenAI\nfrom langchain.chains import LLMRequestsChain, LLMChain\nfrom langchain.prompts import PromptTemplate\ntemplate = \"\"\"Between >>> and <<< are the raw search result text from google.\nExtract the answer to the question '{query}' or say \"not found\" if the information is not contained.\nUse the format\nExtracted:<answer or \"not found\">\n>>> {requests_result} <<<\nExtracted:\"\"\"",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "template = \"\"\"Between >>> and <<< are the raw search result text from google.\nExtract the answer to the question '{query}' or say \"not found\" if the information is not contained.\nUse the format\nExtracted:<answer or \"not found\">\n>>> {requests_result} <<<\nExtracted:\"\"\"\nPROMPT = PromptTemplate(\n    input_variables=[\"query\", \"requests_result\"],\n    template=template,\n)",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "PROMPT",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "PROMPT = PromptTemplate(\n    input_variables=[\"query\", \"requests_result\"],\n    template=template,\n)\nchain = LLMRequestsChain(llm_chain=LLMChain(llm=OpenAI(temperature=0), prompt=PROMPT))\nquestion = \"What are the Three (3) biggest countries, and their respective sizes?\"\ninputs = {\n    \"query\": question,\n    \"url\": \"https://www.google.com/search?q=\" + question.replace(\" \", \"+\"),\n}",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "chain = LLMRequestsChain(llm_chain=LLMChain(llm=OpenAI(temperature=0), prompt=PROMPT))\nquestion = \"What are the Three (3) biggest countries, and their respective sizes?\"\ninputs = {\n    \"query\": question,\n    \"url\": \"https://www.google.com/search?q=\" + question.replace(\" \", \"+\"),\n}\nprint(chain(inputs))",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "question",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "question = \"What are the Three (3) biggest countries, and their respective sizes?\"\ninputs = {\n    \"query\": question,\n    \"url\": \"https://www.google.com/search?q=\" + question.replace(\" \", \"+\"),\n}\nprint(chain(inputs))",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "inputs",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "inputs = {\n    \"query\": question,\n    \"url\": \"https://www.google.com/search?q=\" + question.replace(\" \", \"+\"),\n}\nprint(chain(inputs))",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test07",
        "description": "langchain.chains.test07",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=2)\ntext = \"\"\"\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):",
        "detail": "langchain.chains.test07",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test07",
        "description": "langchain.chains.test07",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=2)\ntext = \"\"\"\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):\n• In 2023, The JWST spotted a number of galaxies nicknamed \"green peas.\" They were given this name because they are small, round, and green, like peas.\n• The telescope captured images of galaxies that are over 13 billion years old. This means that the light from these galaxies has been traveling for over 13 billion years to reach us.\n• JWST took the very first pictures of a planet outside of our own solar system. These distant worlds are called \"exoplanets.\" Exo means \"from outside.\"",
        "detail": "langchain.chains.test07",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test07",
        "description": "langchain.chains.test07",
        "peekOfCode": "llm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=2)\ntext = \"\"\"\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):\n• In 2023, The JWST spotted a number of galaxies nicknamed \"green peas.\" They were given this name because they are small, round, and green, like peas.\n• The telescope captured images of galaxies that are over 13 billion years old. This means that the light from these galaxies has been traveling for over 13 billion years to reach us.\n• JWST took the very first pictures of a planet outside of our own solar system. These distant worlds are called \"exoplanets.\" Exo means \"from outside.\"\nThese discoveries can spark a child's imagination about the infinite wonders of the universe.\"\"\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test07",
        "documentation": {}
    },
    {
        "label": "checker_chain",
        "kind": 5,
        "importPath": "langchain.chains.test07",
        "description": "langchain.chains.test07",
        "peekOfCode": "checker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=2)\ntext = \"\"\"\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):\n• In 2023, The JWST spotted a number of galaxies nicknamed \"green peas.\" They were given this name because they are small, round, and green, like peas.\n• The telescope captured images of galaxies that are over 13 billion years old. This means that the light from these galaxies has been traveling for over 13 billion years to reach us.\n• JWST took the very first pictures of a planet outside of our own solar system. These distant worlds are called \"exoplanets.\" Exo means \"from outside.\"\nThese discoveries can spark a child's imagination about the infinite wonders of the universe.\"\"\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test07",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.chains.test07",
        "description": "langchain.chains.test07",
        "peekOfCode": "text = \"\"\"\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):\n• In 2023, The JWST spotted a number of galaxies nicknamed \"green peas.\" They were given this name because they are small, round, and green, like peas.\n• The telescope captured images of galaxies that are over 13 billion years old. This means that the light from these galaxies has been traveling for over 13 billion years to reach us.\n• JWST took the very first pictures of a planet outside of our own solar system. These distant worlds are called \"exoplanets.\" Exo means \"from outside.\"\nThese discoveries can spark a child's imagination about the infinite wonders of the universe.\"\"\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test07",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test08",
        "description": "langchain.chains.test08",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=3)\ntext = \"The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean. It is the smallest of the five oceans and is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean's main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test08",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test08",
        "description": "langchain.chains.test08",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=3)\ntext = \"The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean. It is the smallest of the five oceans and is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean's main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test08",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test08",
        "description": "langchain.chains.test08",
        "peekOfCode": "llm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=3)\ntext = \"The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean. It is the smallest of the five oceans and is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean's main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test08",
        "documentation": {}
    },
    {
        "label": "checker_chain",
        "kind": 5,
        "importPath": "langchain.chains.test08",
        "description": "langchain.chains.test08",
        "peekOfCode": "checker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=3)\ntext = \"The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean. It is the smallest of the five oceans and is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean's main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test08",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.chains.test08",
        "description": "langchain.chains.test08",
        "peekOfCode": "text = \"The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean. It is the smallest of the five oceans and is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean's main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test08",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test09",
        "description": "langchain.chains.test09",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, max_checks=3, verbose=True)\ntext = \"Mammals can lay eggs, birds can lay eggs, therefore birds are mammals.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test09",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test09",
        "description": "langchain.chains.test09",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, max_checks=3, verbose=True)\ntext = \"Mammals can lay eggs, birds can lay eggs, therefore birds are mammals.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test09",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test09",
        "description": "langchain.chains.test09",
        "peekOfCode": "llm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, max_checks=3, verbose=True)\ntext = \"Mammals can lay eggs, birds can lay eggs, therefore birds are mammals.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test09",
        "documentation": {}
    },
    {
        "label": "checker_chain",
        "kind": 5,
        "importPath": "langchain.chains.test09",
        "description": "langchain.chains.test09",
        "peekOfCode": "checker_chain = LLMSummarizationCheckerChain.from_llm(llm, max_checks=3, verbose=True)\ntext = \"Mammals can lay eggs, birds can lay eggs, therefore birds are mammals.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test09",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.chains.test09",
        "description": "langchain.chains.test09",
        "peekOfCode": "text = \"Mammals can lay eggs, birds can lay eggs, therefore birds are mammals.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test10",
        "description": "langchain.chains.test10",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import (LLMChain, OpenAIModerationChain, SequentialChain,\n                              SimpleSequentialChain)\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nmoderation_chain = OpenAIModerationChain()\nmoderation_chain.run(\"This is okay\")",
        "detail": "langchain.chains.test10",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test10",
        "description": "langchain.chains.test10",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import (LLMChain, OpenAIModerationChain, SequentialChain,\n                              SimpleSequentialChain)\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nmoderation_chain = OpenAIModerationChain()\nmoderation_chain.run(\"This is okay\")\nmoderation_chain.run(\"I will kill you\")",
        "detail": "langchain.chains.test10",
        "documentation": {}
    },
    {
        "label": "moderation_chain",
        "kind": 5,
        "importPath": "langchain.chains.test10",
        "description": "langchain.chains.test10",
        "peekOfCode": "moderation_chain = OpenAIModerationChain()\nmoderation_chain.run(\"This is okay\")\nmoderation_chain.run(\"I will kill you\")",
        "detail": "langchain.chains.test10",
        "documentation": {}
    },
    {
        "label": "generate_serially",
        "kind": 2,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "def generate_serially():\n    llm = OpenAI(temperature=0.9)\n    prompt = PromptTemplate(\n        input_variables=[\"product\"],\n        template=\"What is a good name for a company that makes {product}?\",\n    )\n    chain = LLMChain(llm=llm, prompt=prompt)\n    for _ in range(5):\n        resp = chain.run(product=\"toothpaste\")\n        print(resp)",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nimport asyncio\nimport time\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\ndef generate_serially():",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nimport asyncio\nimport time\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\ndef generate_serially():\n    llm = OpenAI(temperature=0.9)\n    prompt = PromptTemplate(\n        input_variables=[\"product\"],",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "s",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "s = time.perf_counter()\n# If running this outside of Jupyter, use asyncio.run(generate_concurrently())\n# await generate_concurrently()\nelapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Concurrent executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")\ns = time.perf_counter()\ngenerate_serially()\nelapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Serial executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "elapsed",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "elapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Concurrent executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")\ns = time.perf_counter()\ngenerate_serially()\nelapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Serial executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "s",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "s = time.perf_counter()\ngenerate_serially()\nelapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Serial executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "elapsed",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "elapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Serial executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.indexes.test01",
        "description": "langchain.indexes.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import OpenAI\nfrom langchain.document_loaders import TextLoader\nloader = TextLoader(\"./jqxxi_array.txt\", encoding=\"utf8\")\nfrom langchain.indexes import VectorstoreIndexCreator\nindex = VectorstoreIndexCreator().from_loaders([loader])",
        "detail": "langchain.indexes.test01",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.indexes.test01",
        "description": "langchain.indexes.test01",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import OpenAI\nfrom langchain.document_loaders import TextLoader\nloader = TextLoader(\"./jqxxi_array.txt\", encoding=\"utf8\")\nfrom langchain.indexes import VectorstoreIndexCreator\nindex = VectorstoreIndexCreator().from_loaders([loader])",
        "detail": "langchain.indexes.test01",
        "documentation": {}
    },
    {
        "label": "loader",
        "kind": 5,
        "importPath": "langchain.indexes.test01",
        "description": "langchain.indexes.test01",
        "peekOfCode": "loader = TextLoader(\"./jqxxi_array.txt\", encoding=\"utf8\")\nfrom langchain.indexes import VectorstoreIndexCreator\nindex = VectorstoreIndexCreator().from_loaders([loader])",
        "detail": "langchain.indexes.test01",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 5,
        "importPath": "langchain.indexes.test01",
        "description": "langchain.indexes.test01",
        "peekOfCode": "index = VectorstoreIndexCreator().from_loaders([loader])",
        "detail": "langchain.indexes.test01",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "langchain.momory.test01",
        "description": "langchain.momory.test01",
        "peekOfCode": "template = \"\"\"You are a chatbot having a conversation with a human.\n{chat_history}\nHuman: {human_input}\nChatbot:\"\"\"\nprompt = PromptTemplate(\n    input_variables=[\"chat_history\", \"human_input\"], template=template\n)\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\nllm_chain = LLMChain(\n    llm=OpenAI(),",
        "detail": "langchain.momory.test01",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "langchain.momory.test01",
        "description": "langchain.momory.test01",
        "peekOfCode": "prompt = PromptTemplate(\n    input_variables=[\"chat_history\", \"human_input\"], template=template\n)\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\nllm_chain = LLMChain(\n    llm=OpenAI(),\n    prompt=prompt,\n    verbose=True,\n    memory=memory,\n)",
        "detail": "langchain.momory.test01",
        "documentation": {}
    },
    {
        "label": "memory",
        "kind": 5,
        "importPath": "langchain.momory.test01",
        "description": "langchain.momory.test01",
        "peekOfCode": "memory = ConversationBufferMemory(memory_key=\"chat_history\")\nllm_chain = LLMChain(\n    llm=OpenAI(),\n    prompt=prompt,\n    verbose=True,\n    memory=memory,\n)\nllm_chain.predict(human_input=\"Hi there my friend\")\nllm_chain.predict(human_input=\"Not too bad - how are you?\")",
        "detail": "langchain.momory.test01",
        "documentation": {}
    },
    {
        "label": "llm_chain",
        "kind": 5,
        "importPath": "langchain.momory.test01",
        "description": "langchain.momory.test01",
        "peekOfCode": "llm_chain = LLMChain(\n    llm=OpenAI(),\n    prompt=prompt,\n    verbose=True,\n    memory=memory,\n)\nllm_chain.predict(human_input=\"Hi there my friend\")\nllm_chain.predict(human_input=\"Not too bad - how are you?\")",
        "detail": "langchain.momory.test01",
        "documentation": {}
    },
    {
        "label": "tsc",
        "kind": 2,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "def tsc(inp: str, nn: int) -> str:\n    # 这是基线条件，当nn等于0时，函数返回my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1\n        text = (\n            \"列举商城软件开发中2个关于\" + inp + \"的功能词汇，注意只输出2个功能词汇，用逗号隔开，注意不加序列号，不附带其他任何信息。必须是中文。\"\n        )\n        response = llm(text)  # 似乎缺少OpenAI的详细调用信息，例如生成模型、令牌数量等",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "tzh",
        "kind": 2,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "def tzh():\n    text = \"列举5个商城软件开发中的功能，注意只输出5个功能词汇，用英文逗号隔开，注意不加序列号，不附带其他任何信息。必须是中文。\"\n    response = llm(text)\n    array = np.array(response.split(\",\"))\n    print(array)\n    for key in array:\n        arr = tsc(key, 10)\n        arr = np.array(arr.split(\",\"))\n        global my_array\n        my_array = np.append(my_array, arr)",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "jiesh",
        "kind": 2,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "def jiesh():\n    arr = tzh()\n    #     arr = np.array(arr.split(','))\n    for a in arr:\n        text = \"详细解释一下，商城软件开发中\" + a + \"功能实现的业务逻辑，并生成php代码。中文回复。\"\n        llm = OpenAI(temperature=0.9, max_tokens=1000)\n        response = llm(text, max_tokens=1000)\n        #         time.sleep(5)\n        response = a + \"\\n\" + response\n        print(response)",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "] = \"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\"  # 直连的key\nfrom langchain.llms import OpenAI\nfrom langchain.agents import load_tools\nfrom langchain.agents import initialize_agent\nfrom langchain.agents import AgentType\nimport numpy as np\nimport time\nllm = OpenAI(temperature=0.9, max_tokens=1000)\nnn = 5\nmy_array = np.array([])",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "llm = OpenAI(temperature=0.9, max_tokens=1000)\nnn = 5\nmy_array = np.array([])\njiesh_array = np.array([])\n# 这个函数需要一个额外的参数nn，因为在递归的环境中，\n# 我们无法修改外部作用域的变量。\ndef tsc(inp: str, nn: int) -> str:\n    # 这是基线条件，当nn等于0时，函数返回my_array\n    if nn == 0:\n        return my_array",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "nn",
        "kind": 5,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "nn = 5\nmy_array = np.array([])\njiesh_array = np.array([])\n# 这个函数需要一个额外的参数nn，因为在递归的环境中，\n# 我们无法修改外部作用域的变量。\ndef tsc(inp: str, nn: int) -> str:\n    # 这是基线条件，当nn等于0时，函数返回my_array\n    if nn == 0:\n        return my_array\n    else:",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "my_array",
        "kind": 5,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "my_array = np.array([])\njiesh_array = np.array([])\n# 这个函数需要一个额外的参数nn，因为在递归的环境中，\n# 我们无法修改外部作用域的变量。\ndef tsc(inp: str, nn: int) -> str:\n    # 这是基线条件，当nn等于0时，函数返回my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "jiesh_array",
        "kind": 5,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "jiesh_array = np.array([])\n# 这个函数需要一个额外的参数nn，因为在递归的环境中，\n# 我们无法修改外部作用域的变量。\ndef tsc(inp: str, nn: int) -> str:\n    # 这是基线条件，当nn等于0时，函数返回my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1\n        text = (",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test02",
        "description": "langchain.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0.9)\ntext = \"What would be a good company name for a company that makes colorful socks?\"\n# print(llm(text))\nfrom langchain.prompts import PromptTemplate\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)",
        "detail": "langchain.test02",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test02",
        "description": "langchain.test02",
        "peekOfCode": "llm = OpenAI(temperature=0.9)\ntext = \"What would be a good company name for a company that makes colorful socks?\"\n# print(llm(text))\nfrom langchain.prompts import PromptTemplate\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nprint(prompt.format(product=\"colorful socks\"))",
        "detail": "langchain.test02",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.test02",
        "description": "langchain.test02",
        "peekOfCode": "text = \"What would be a good company name for a company that makes colorful socks?\"\n# print(llm(text))\nfrom langchain.prompts import PromptTemplate\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nprint(prompt.format(product=\"colorful socks\"))",
        "detail": "langchain.test02",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "langchain.test02",
        "description": "langchain.test02",
        "peekOfCode": "prompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nprint(prompt.format(product=\"colorful socks\"))",
        "detail": "langchain.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test03",
        "description": "langchain.test03",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nllm = OpenAI(temperature=0.9)\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nfrom langchain.chains import LLMChain\nchain = LLMChain(llm=llm, prompt=prompt)\nprint(chain.run(\"colorful socks\"))\n# -> '\\n\\nSocktastic!'",
        "detail": "langchain.test03",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test03",
        "description": "langchain.test03",
        "peekOfCode": "llm = OpenAI(temperature=0.9)\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nfrom langchain.chains import LLMChain\nchain = LLMChain(llm=llm, prompt=prompt)\nprint(chain.run(\"colorful socks\"))\n# -> '\\n\\nSocktastic!'",
        "detail": "langchain.test03",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "langchain.test03",
        "description": "langchain.test03",
        "peekOfCode": "prompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nfrom langchain.chains import LLMChain\nchain = LLMChain(llm=llm, prompt=prompt)\nprint(chain.run(\"colorful socks\"))\n# -> '\\n\\nSocktastic!'",
        "detail": "langchain.test03",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "langchain.test03",
        "description": "langchain.test03",
        "peekOfCode": "chain = LLMChain(llm=llm, prompt=prompt)\nprint(chain.run(\"colorful socks\"))\n# -> '\\n\\nSocktastic!'",
        "detail": "langchain.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test04",
        "description": "langchain.test04",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\"SERPAPI_API_KEY\"] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import load_tools\nfrom langchain.agents import initialize_agent\nfrom langchain.agents import AgentType\nfrom langchain.llms import OpenAI\n# First, let's load the language model we're going to use to control the agent.\nllm = OpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)",
        "detail": "langchain.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"SERPAPI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test04",
        "description": "langchain.test04",
        "peekOfCode": "os.environ[\"SERPAPI_API_KEY\"] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import load_tools\nfrom langchain.agents import initialize_agent\nfrom langchain.agents import AgentType\nfrom langchain.llms import OpenAI\n# First, let's load the language model we're going to use to control the agent.\nllm = OpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.",
        "detail": "langchain.test04",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test04",
        "description": "langchain.test04",
        "peekOfCode": "llm = OpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nprint(\n    agent.run(",
        "detail": "langchain.test04",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.test04",
        "description": "langchain.test04",
        "peekOfCode": "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nprint(\n    agent.run(\n        \"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\"\n    )",
        "detail": "langchain.test04",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.test04",
        "description": "langchain.test04",
        "peekOfCode": "agent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nprint(\n    agent.run(\n        \"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\"\n    )\n)",
        "detail": "langchain.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import OpenAI, ConversationChain\nllm = OpenAI(temperature=0)\nconversation = ConversationChain(llm=llm, verbose=True)\noutput = conversation.predict(input=\"Hi there!\")\nprint(output)\noutput = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import OpenAI, ConversationChain\nllm = OpenAI(temperature=0)\nconversation = ConversationChain(llm=llm, verbose=True)\noutput = conversation.predict(input=\"Hi there!\")\nprint(output)\noutput = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\nprint(output)",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "llm = OpenAI(temperature=0)\nconversation = ConversationChain(llm=llm, verbose=True)\noutput = conversation.predict(input=\"Hi there!\")\nprint(output)\noutput = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\nprint(output)",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "conversation",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "conversation = ConversationChain(llm=llm, verbose=True)\noutput = conversation.predict(input=\"Hi there!\")\nprint(output)\noutput = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\nprint(output)",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "output = conversation.predict(input=\"Hi there!\")\nprint(output)\noutput = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\nprint(output)",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "output = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\nprint(output)",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import AIMessage, HumanMessage, SystemMessage\nchat = ChatOpenAI(temperature=0)\n# print(\n#     chat(\n#         [",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import AIMessage, HumanMessage, SystemMessage\nchat = ChatOpenAI(temperature=0)\n# print(\n#     chat(\n#         [\n#             HumanMessage(\n#                 content=\"Translate this sentence from English to French. I love programming.\"\n#             )",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "chat",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "chat = ChatOpenAI(temperature=0)\n# print(\n#     chat(\n#         [\n#             HumanMessage(\n#                 content=\"Translate this sentence from English to French. I love programming.\"\n#             )\n#         ]\n#     )\n# )",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "messages",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "messages = [\n    SystemMessage(\n        content=\"You are a helpful assistant that translates English to French.\"\n    ),\n    HumanMessage(\n        content=\"Translate this sentence from English to French. I love programming.\"\n    ),\n]\n# result = chat(messages)\n# print(result)",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "batch_messages",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "batch_messages = [\n    [\n        SystemMessage(\n            content=\"You are a helpful assistant that translates English to French.\"\n        ),\n        HumanMessage(\n            content=\"Translate this sentence from English to French. I love programming.\"\n        ),\n    ],\n    [",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "result = chat.generate(batch_messages)\nprint(result.llm_output['token_usage'])\n# -> LLMResult(generations=[[ChatGeneration(text=\"J'aime programmer.\", generation_info=None, message=AIMessage(content=\"J'aime programmer.\", additional_kwargs={}))], [ChatGeneration(text=\"J'aime l'intelligence artificielle.\", generation_info=None, message=AIMessage(content=\"J'aime l'intelligence artificielle.\", additional_kwargs={}))]], llm_output={'token_usage': {'prompt_tokens': 71, 'completion_tokens': 18, 'total_tokens': 89}})",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nchat = ChatOpenAI(temperature=0)\ntemplate = (\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "chat",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "chat = ChatOpenAI(temperature=0)\ntemplate = (\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n)\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "template = (\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n)\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\n# get a chat completion from the formatted messages",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "system_message_prompt",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\n# get a chat completion from the formatted messages\nresult = chat(\n    chat_prompt.format_prompt(\n        input_language=\"English\", output_language=\"French\", text=\"I love programming.\"",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "human_template",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "human_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\n# get a chat completion from the formatted messages\nresult = chat(\n    chat_prompt.format_prompt(\n        input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n    ).to_messages()",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "human_message_prompt",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\n# get a chat completion from the formatted messages\nresult = chat(\n    chat_prompt.format_prompt(\n        input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n    ).to_messages()\n)",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "chat_prompt",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "chat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\n# get a chat completion from the formatted messages\nresult = chat(\n    chat_prompt.format_prompt(\n        input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n    ).to_messages()\n)\nprint(result)",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "result = chat(\n    chat_prompt.format_prompt(\n        input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n    ).to_messages()\n)\nprint(result)\n# -> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain import LLMChain\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain import LLMChain\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nchat = ChatOpenAI(temperature=0)\ntemplate = (",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "chat",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "chat = ChatOpenAI(temperature=0)\ntemplate = (\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n)\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "template = (\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n)\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\nchain = LLMChain(llm=chat, prompt=chat_prompt)",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "system_message_prompt",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\nchain = LLMChain(llm=chat, prompt=chat_prompt)\nresult = chain.run(\n    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n)",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "human_template",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "human_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\nchain = LLMChain(llm=chat, prompt=chat_prompt)\nresult = chain.run(\n    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n)\nprint(result)",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "human_message_prompt",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\nchain = LLMChain(llm=chat, prompt=chat_prompt)\nresult = chain.run(\n    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n)\nprint(result)\n# -> \"J'aime programmer.\"",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "chat_prompt",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "chat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\nchain = LLMChain(llm=chat, prompt=chat_prompt)\nresult = chain.run(\n    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n)\nprint(result)\n# -> \"J'aime programmer.\"",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "chain = LLMChain(llm=chat, prompt=chat_prompt)\nresult = chain.run(\n    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n)\nprint(result)\n# -> \"J'aime programmer.\"",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "result = chain.run(\n    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n)\nprint(result)\n# -> \"J'aime programmer.\"",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.llms import OpenAI\n# First, let's load the language model we're going to use to control the agent.\nchat = ChatOpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.llms import OpenAI\n# First, let's load the language model we're going to use to control the agent.\nchat = ChatOpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\nllm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "chat",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "chat = ChatOpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\nllm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(\n    tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nresult = agent.run(",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "llm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(\n    tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nresult = agent.run(\n    \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"\n)",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(\n    tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nresult = agent.run(\n    \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"\n)\n# print(result)",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "agent = initialize_agent(\n    tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nresult = agent.run(\n    \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"\n)\n# print(result)",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "result = agent.run(\n    \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"\n)\n# print(result)",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.prompts import (\n    ChatPromptTemplate,\n    MessagesPlaceholder,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.prompts import (\n    ChatPromptTemplate,\n    MessagesPlaceholder,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.chains import ConversationChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.memory import ConversationBufferMemory",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "prompt = ChatPromptTemplate.from_messages(\n    [\n        SystemMessagePromptTemplate.from_template(\n            \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"\n        ),\n        MessagesPlaceholder(variable_name=\"history\"),\n        HumanMessagePromptTemplate.from_template(\"{input}\"),\n    ]\n)\nllm = ChatOpenAI(temperature=0)",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "llm = ChatOpenAI(temperature=0)\nmemory = ConversationBufferMemory(return_messages=True)\nconversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\nconversation.predict(input=\"Hi there!\")\n# -> 'Hello! How can I assist you today?'\nconversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n# -> \"That sounds like fun! I'm happy to chat with you. Is there anything specific you'd like to talk about?\"\nconversation.predict(input=\"Tell me about yourself.\")\n# -> \"Sure! I am an AI language model created by OpenAI. I was trained on a large dataset of text from the internet, which allows me to understand and generate human-like language. I can answer questions, provide information, and even have conversations like this one. Is there anything else you'd like to know about me?\"",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "memory",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "memory = ConversationBufferMemory(return_messages=True)\nconversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\nconversation.predict(input=\"Hi there!\")\n# -> 'Hello! How can I assist you today?'\nconversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n# -> \"That sounds like fun! I'm happy to chat with you. Is there anything specific you'd like to talk about?\"\nconversation.predict(input=\"Tell me about yourself.\")\n# -> \"Sure! I am an AI language model created by OpenAI. I was trained on a large dataset of text from the internet, which allows me to understand and generate human-like language. I can answer questions, provide information, and even have conversations like this one. Is there anything else you'd like to know about me?\"",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "conversation",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\nconversation.predict(input=\"Hi there!\")\n# -> 'Hello! How can I assist you today?'\nconversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n# -> \"That sounds like fun! I'm happy to chat with you. Is there anything specific you'd like to talk about?\"\nconversation.predict(input=\"Tell me about yourself.\")\n# -> \"Sure! I am an AI language model created by OpenAI. I was trained on a large dataset of text from the internet, which allows me to understand and generate human-like language. I can answer questions, provide information, and even have conversations like this one. Is there anything else you'd like to know about me?\"",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "model_dir",
        "kind": 5,
        "importPath": "modelScope.test",
        "description": "modelScope.test",
        "peekOfCode": "model_dir = snapshot_download(\"AI-ModelScope/TinyLlama-1.1B-Chat-v1.0\")\npipe = pipeline(\n    \"text-generation\", model=model_dir, torch_dtype=torch.bfloat16, device_map=\"auto\"\n)\n# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },",
        "detail": "modelScope.test",
        "documentation": {}
    },
    {
        "label": "pipe",
        "kind": 5,
        "importPath": "modelScope.test",
        "description": "modelScope.test",
        "peekOfCode": "pipe = pipeline(\n    \"text-generation\", model=model_dir, torch_dtype=torch.bfloat16, device_map=\"auto\"\n)\n# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\"role\": \"user\", \"content\": \"我们能干啥？\"},",
        "detail": "modelScope.test",
        "documentation": {}
    },
    {
        "label": "messages",
        "kind": 5,
        "importPath": "modelScope.test",
        "description": "modelScope.test",
        "peekOfCode": "messages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\"role\": \"user\", \"content\": \"我们能干啥？\"},\n]\nprompt = pipe.tokenizer.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)",
        "detail": "modelScope.test",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "modelScope.test",
        "description": "modelScope.test",
        "peekOfCode": "prompt = pipe.tokenizer.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\noutputs = pipe(\n    prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95\n)\nprint(outputs[0][\"generated_text\"])\n# <|system|>\n# You are a friendly chatbot who always responds in the style of a pirate.</s>\n# <|user|>",
        "detail": "modelScope.test",
        "documentation": {}
    },
    {
        "label": "outputs",
        "kind": 5,
        "importPath": "modelScope.test",
        "description": "modelScope.test",
        "peekOfCode": "outputs = pipe(\n    prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95\n)\nprint(outputs[0][\"generated_text\"])\n# <|system|>\n# You are a friendly chatbot who always responds in the style of a pirate.</s>\n# <|user|>\n# How many helicopters can a human eat in one sitting?</s>\n# <|assistant|>\n# ...",
        "detail": "modelScope.test",
        "documentation": {}
    },
    {
        "label": "ocr_detection",
        "kind": 5,
        "importPath": "modelScope.test01",
        "description": "modelScope.test01",
        "peekOfCode": "ocr_detection = pipeline(\n    Tasks.ocr_detection, model=\"damo/cv_resnet18_ocr-detection-line-level_damo\"\n)\nresult = ocr_detection(\n    \"https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/ocr_detection.jpg\"\n)\nprint(result)",
        "detail": "modelScope.test01",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "modelScope.test01",
        "description": "modelScope.test01",
        "peekOfCode": "result = ocr_detection(\n    \"https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/ocr_detection.jpg\"\n)\nprint(result)",
        "detail": "modelScope.test01",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test02",
        "description": "modelScope.test02",
        "peekOfCode": "model = timm.create_model(\"hf_hub:notmahi/dobb-e\", pretrained=True)\nmodel.eval()",
        "detail": "modelScope.test02",
        "documentation": {}
    },
    {
        "label": "model_path",
        "kind": 5,
        "importPath": "modelScope.test03",
        "description": "modelScope.test03",
        "peekOfCode": "model_path = \"sword_out\"\npipe = StableDiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-2-1\", paddle_dtype=paddle.float32\n)\n# 注意：如果我们想从 HF Hub 加载权重，那么我们需要设置 from_hf_hub=True\npipe.unet.load_attn_procs(model_path, from_hf_hub=False)\nprompt = \"sword_1bug,sword,simple background,upward\"\nimage = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\nimage.save(\"sword.png\")",
        "detail": "modelScope.test03",
        "documentation": {}
    },
    {
        "label": "pipe",
        "kind": 5,
        "importPath": "modelScope.test03",
        "description": "modelScope.test03",
        "peekOfCode": "pipe = StableDiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-2-1\", paddle_dtype=paddle.float32\n)\n# 注意：如果我们想从 HF Hub 加载权重，那么我们需要设置 from_hf_hub=True\npipe.unet.load_attn_procs(model_path, from_hf_hub=False)\nprompt = \"sword_1bug,sword,simple background,upward\"\nimage = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\nimage.save(\"sword.png\")",
        "detail": "modelScope.test03",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "modelScope.test03",
        "description": "modelScope.test03",
        "peekOfCode": "prompt = \"sword_1bug,sword,simple background,upward\"\nimage = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\nimage.save(\"sword.png\")",
        "detail": "modelScope.test03",
        "documentation": {}
    },
    {
        "label": "image",
        "kind": 5,
        "importPath": "modelScope.test03",
        "description": "modelScope.test03",
        "peekOfCode": "image = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\nimage.save(\"sword.png\")",
        "detail": "modelScope.test03",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "modelScope.test04",
        "description": "modelScope.test04",
        "peekOfCode": "data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\ntable = pd.DataFrame.from_dict(data)\nquestion = \"how many movies does Leonardo Di Caprio have?\"\n# pipeline model\n# Note: you must to install torch-scatter first.\ntqa = pipeline(task=\"table-question-answering\", model=\"google/tapas-large-finetuned-wtq\")\n# result\nprint(tqa(table=table, query=question)['cells'][0])\n#53",
        "detail": "modelScope.test04",
        "documentation": {}
    },
    {
        "label": "table",
        "kind": 5,
        "importPath": "modelScope.test04",
        "description": "modelScope.test04",
        "peekOfCode": "table = pd.DataFrame.from_dict(data)\nquestion = \"how many movies does Leonardo Di Caprio have?\"\n# pipeline model\n# Note: you must to install torch-scatter first.\ntqa = pipeline(task=\"table-question-answering\", model=\"google/tapas-large-finetuned-wtq\")\n# result\nprint(tqa(table=table, query=question)['cells'][0])\n#53",
        "detail": "modelScope.test04",
        "documentation": {}
    },
    {
        "label": "question",
        "kind": 5,
        "importPath": "modelScope.test04",
        "description": "modelScope.test04",
        "peekOfCode": "question = \"how many movies does Leonardo Di Caprio have?\"\n# pipeline model\n# Note: you must to install torch-scatter first.\ntqa = pipeline(task=\"table-question-answering\", model=\"google/tapas-large-finetuned-wtq\")\n# result\nprint(tqa(table=table, query=question)['cells'][0])\n#53",
        "detail": "modelScope.test04",
        "documentation": {}
    },
    {
        "label": "tqa",
        "kind": 5,
        "importPath": "modelScope.test04",
        "description": "modelScope.test04",
        "peekOfCode": "tqa = pipeline(task=\"table-question-answering\", model=\"google/tapas-large-finetuned-wtq\")\n# result\nprint(tqa(table=table, query=question)['cells'][0])\n#53",
        "detail": "modelScope.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models import Model\nfrom modelscope.pipelines import pipeline\nfrom modelscope.utils.constant import Tasks",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models import Model\nfrom modelscope.pipelines import pipeline\nfrom modelscope.utils.constant import Tasks\nmodel = \"ZhipuAI/CodeGeeX-Code-Generation-13B\"",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models import Model\nfrom modelscope.pipelines import pipeline\nfrom modelscope.utils.constant import Tasks\nmodel = \"ZhipuAI/CodeGeeX-Code-Generation-13B\"\npipe = pipeline(",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models import Model\nfrom modelscope.pipelines import pipeline\nfrom modelscope.utils.constant import Tasks\nmodel = \"ZhipuAI/CodeGeeX-Code-Generation-13B\"\npipe = pipeline(\n    task=Tasks.code_generation,",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models import Model\nfrom modelscope.pipelines import pipeline\nfrom modelscope.utils.constant import Tasks\nmodel = \"ZhipuAI/CodeGeeX-Code-Generation-13B\"\npipe = pipeline(\n    task=Tasks.code_generation,\n    model=model,",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models import Model\nfrom modelscope.pipelines import pipeline\nfrom modelscope.utils.constant import Tasks\nmodel = \"ZhipuAI/CodeGeeX-Code-Generation-13B\"\npipe = pipeline(\n    task=Tasks.code_generation,\n    model=model,\n)",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models import Model\nfrom modelscope.pipelines import pipeline\nfrom modelscope.utils.constant import Tasks\nmodel = \"ZhipuAI/CodeGeeX-Code-Generation-13B\"\npipe = pipeline(\n    task=Tasks.code_generation,\n    model=model,\n)\ninputs = {",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "model = \"ZhipuAI/CodeGeeX-Code-Generation-13B\"\npipe = pipeline(\n    task=Tasks.code_generation,\n    model=model,\n)\ninputs = {\n    \"prompt\": 'from typing import List\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n    return\",',\n    \"language\": \"Python\",\n}\nresult = pipe(inputs)",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "pipe",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "pipe = pipeline(\n    task=Tasks.code_generation,\n    model=model,\n)\ninputs = {\n    \"prompt\": 'from typing import List\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n    return\",',\n    \"language\": \"Python\",\n}\nresult = pipe(inputs)\nprint(result)",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "inputs",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "inputs = {\n    \"prompt\": 'from typing import List\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n    return\",',\n    \"language\": \"Python\",\n}\nresult = pipe(inputs)\nprint(result)",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "result = pipe(inputs)\nprint(result)",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "MySQL",
        "kind": 6,
        "importPath": "my_utils.MysqlUtility",
        "description": "my_utils.MysqlUtility",
        "peekOfCode": "class MySQL:\n    u'''对MySQLdb常用函数进行封装的类'''\n    error_code = ''  # MySQL错误号码\n    _instance = None  # 本类的实例\n    _conn = None  # 数据库conn\n    _cur = None  # 游标\n    _TIMEOUT = 30  # 默认超时30秒\n    _timecount = 0\n    def __init__(self, dbconfig):\n        u'构造器：根据数据库连接参数，创建MySQL连接'",
        "detail": "my_utils.MysqlUtility",
        "documentation": {}
    },
    {
        "label": "paddstr",
        "kind": 2,
        "importPath": "my_utils.MysqlUtility",
        "description": "my_utils.MysqlUtility",
        "peekOfCode": "def paddstr(strx):\n    return \"NULL\" if strx == None else \"\\'\" + MySQLdb.escape_string(str(strx)).decode() + \"\\'\"\ndef paddnum(numx, default='NULL'):\n    return default if numx == None else str(numx)\nclass MySQL:\n    u'''对MySQLdb常用函数进行封装的类'''\n    error_code = ''  # MySQL错误号码\n    _instance = None  # 本类的实例\n    _conn = None  # 数据库conn\n    _cur = None  # 游标",
        "detail": "my_utils.MysqlUtility",
        "documentation": {}
    },
    {
        "label": "paddnum",
        "kind": 2,
        "importPath": "my_utils.MysqlUtility",
        "description": "my_utils.MysqlUtility",
        "peekOfCode": "def paddnum(numx, default='NULL'):\n    return default if numx == None else str(numx)\nclass MySQL:\n    u'''对MySQLdb常用函数进行封装的类'''\n    error_code = ''  # MySQL错误号码\n    _instance = None  # 本类的实例\n    _conn = None  # 数据库conn\n    _cur = None  # 游标\n    _TIMEOUT = 30  # 默认超时30秒\n    _timecount = 0",
        "detail": "my_utils.MysqlUtility",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 2,
        "importPath": "suanfa.main01",
        "description": "suanfa.main01",
        "peekOfCode": "def name(args):\n    print(\"Hello World\" + args)\ndef bubble_sort(arr):\n    n = len(arr)\n    for i in range(n - 1):\n        # 每次遍历将最大的元素冒泡到末尾\n        for j in range(n - 1 - i):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\ndef my_bubble_sort(arr):",
        "detail": "suanfa.main01",
        "documentation": {}
    },
    {
        "label": "bubble_sort",
        "kind": 2,
        "importPath": "suanfa.main01",
        "description": "suanfa.main01",
        "peekOfCode": "def bubble_sort(arr):\n    n = len(arr)\n    for i in range(n - 1):\n        # 每次遍历将最大的元素冒泡到末尾\n        for j in range(n - 1 - i):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\ndef my_bubble_sort(arr):\n    n = len(arr)\n    for i in range(n - 1):",
        "detail": "suanfa.main01",
        "documentation": {}
    },
    {
        "label": "my_bubble_sort",
        "kind": 2,
        "importPath": "suanfa.main01",
        "description": "suanfa.main01",
        "peekOfCode": "def my_bubble_sort(arr):\n    n = len(arr)\n    for i in range(n - 1):\n        for j in range(n - 1 - i):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\ndef insertion_sort(arr):\n    n = len(arr)\n    for i in range(1, n):\n        key = arr[i]",
        "detail": "suanfa.main01",
        "documentation": {}
    },
    {
        "label": "insertion_sort",
        "kind": 2,
        "importPath": "suanfa.main01",
        "description": "suanfa.main01",
        "peekOfCode": "def insertion_sort(arr):\n    n = len(arr)\n    for i in range(1, n):\n        key = arr[i]\n        j = i - 1\n        # 在已排序部分找到合适的插入位置\n        while j >= 0 and arr[j] > key:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key",
        "detail": "suanfa.main01",
        "documentation": {}
    },
    {
        "label": "reader",
        "kind": 5,
        "importPath": "suanfa.mian02",
        "description": "suanfa.mian02",
        "peekOfCode": "reader = easyocr.Reader(\n    [\"en\", \"ch_sim\"],\n    gpu=True,\n)  # this needs to run only once to load the model into memory\n# 图像路径\nimage_path = \"./15889317528424945.png\"\nimage_path_out = \"./results/15889317528424945.png\"\nimage = Image.open(image_path)\n# image = image.resize((800, 600))\n# image = image.resize((800, 300))",
        "detail": "suanfa.mian02",
        "documentation": {}
    },
    {
        "label": "image_path",
        "kind": 5,
        "importPath": "suanfa.mian02",
        "description": "suanfa.mian02",
        "peekOfCode": "image_path = \"./15889317528424945.png\"\nimage_path_out = \"./results/15889317528424945.png\"\nimage = Image.open(image_path)\n# image = image.resize((800, 600))\n# image = image.resize((800, 300))\n# image.show()\nimage.save(image_path_out)\n# result = reader.readtext(\"./image/baidu_image/test5.jpg\", detail=0)\nresults = reader.readtext(image_path_out, detail=0, batch_size=20, paragraph=True)\nprint(\"results==\", results)",
        "detail": "suanfa.mian02",
        "documentation": {}
    },
    {
        "label": "image_path_out",
        "kind": 5,
        "importPath": "suanfa.mian02",
        "description": "suanfa.mian02",
        "peekOfCode": "image_path_out = \"./results/15889317528424945.png\"\nimage = Image.open(image_path)\n# image = image.resize((800, 600))\n# image = image.resize((800, 300))\n# image.show()\nimage.save(image_path_out)\n# result = reader.readtext(\"./image/baidu_image/test5.jpg\", detail=0)\nresults = reader.readtext(image_path_out, detail=0, batch_size=20, paragraph=True)\nprint(\"results==\", results)\nfor result in results:",
        "detail": "suanfa.mian02",
        "documentation": {}
    },
    {
        "label": "image",
        "kind": 5,
        "importPath": "suanfa.mian02",
        "description": "suanfa.mian02",
        "peekOfCode": "image = Image.open(image_path)\n# image = image.resize((800, 600))\n# image = image.resize((800, 300))\n# image.show()\nimage.save(image_path_out)\n# result = reader.readtext(\"./image/baidu_image/test5.jpg\", detail=0)\nresults = reader.readtext(image_path_out, detail=0, batch_size=20, paragraph=True)\nprint(\"results==\", results)\nfor result in results:\n    print(\"result\", result)",
        "detail": "suanfa.mian02",
        "documentation": {}
    },
    {
        "label": "results",
        "kind": 5,
        "importPath": "suanfa.mian02",
        "description": "suanfa.mian02",
        "peekOfCode": "results = reader.readtext(image_path_out, detail=0, batch_size=20, paragraph=True)\nprint(\"results==\", results)\nfor result in results:\n    print(\"result\", result)",
        "detail": "suanfa.mian02",
        "documentation": {}
    },
    {
        "label": "ocr",
        "kind": 5,
        "importPath": "suanfa.mian03",
        "description": "suanfa.mian03",
        "peekOfCode": "ocr = PaddleOCR(\n    lang=\"ch\",\n    # use_gpu=False,\n    use_gpu=True,\n    det_model_dir=\"../paddleORC_model/ch_ppocr_server_v2.0_det_infer/\",\n    cls_model_dir=\"ch_ppocr_mobile_v2.0_cls_infer/\",\n    rec_model_dir=\"ch_ppocr_server_v2.0_rec_infer/\",\n)\n# load dataset\n# img_path = \"./08631508_2.jpg\"",
        "detail": "suanfa.mian03",
        "documentation": {}
    },
    {
        "label": "img_path",
        "kind": 5,
        "importPath": "suanfa.mian03",
        "description": "suanfa.mian03",
        "peekOfCode": "img_path = \"./image/baidu_image/test7.jpg\"\nresult = ocr.ocr(img_path, cls=True)\nfor line in result:\n    print(line)\n    print(line[0][1][0])\n# 注：\n# result是一个list，每个item包含了文本框，文字和识别置信度\n# line的格式为：\n# [[[3.0, 149.0], [43.0, 149.0], [43.0, 163.0], [3.0, 163.0]], ('人心安', 0.6762619018554688)]\n# 文字框 boxes = line[0]，包含文字框的四个角的(x,y)坐标",
        "detail": "suanfa.mian03",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "suanfa.mian03",
        "description": "suanfa.mian03",
        "peekOfCode": "result = ocr.ocr(img_path, cls=True)\nfor line in result:\n    print(line)\n    print(line[0][1][0])\n# 注：\n# result是一个list，每个item包含了文本框，文字和识别置信度\n# line的格式为：\n# [[[3.0, 149.0], [43.0, 149.0], [43.0, 163.0], [3.0, 163.0]], ('人心安', 0.6762619018554688)]\n# 文字框 boxes = line[0]，包含文字框的四个角的(x,y)坐标\n# 文字 txts = line[1][0]",
        "detail": "suanfa.mian03",
        "documentation": {}
    },
    {
        "label": "api_id",
        "kind": 5,
        "importPath": "bot",
        "description": "bot",
        "peekOfCode": "api_id = \"27175683\"\napi_hash = \"ff5bd7a1d985ad6e6db389e4ef104fe6\"\nbot_token = \"6778102881:AAEJ0PvBN8qDjR3MqNSGNFWUXaLacuKbQ7Y\"\napp = Client(\n    \"kjjjkljklbot\",\n    api_id=api_id, api_hash=api_hash,\n    bot_token=bot_token\n)\n@app.on_message()\nasync def echo(client: Client, message: Message):",
        "detail": "bot",
        "documentation": {}
    },
    {
        "label": "api_hash",
        "kind": 5,
        "importPath": "bot",
        "description": "bot",
        "peekOfCode": "api_hash = \"ff5bd7a1d985ad6e6db389e4ef104fe6\"\nbot_token = \"6778102881:AAEJ0PvBN8qDjR3MqNSGNFWUXaLacuKbQ7Y\"\napp = Client(\n    \"kjjjkljklbot\",\n    api_id=api_id, api_hash=api_hash,\n    bot_token=bot_token\n)\n@app.on_message()\nasync def echo(client: Client, message: Message):\n    await message.reply(message.text)",
        "detail": "bot",
        "documentation": {}
    },
    {
        "label": "bot_token",
        "kind": 5,
        "importPath": "bot",
        "description": "bot",
        "peekOfCode": "bot_token = \"6778102881:AAEJ0PvBN8qDjR3MqNSGNFWUXaLacuKbQ7Y\"\napp = Client(\n    \"kjjjkljklbot\",\n    api_id=api_id, api_hash=api_hash,\n    bot_token=bot_token\n)\n@app.on_message()\nasync def echo(client: Client, message: Message):\n    await message.reply(message.text)\napp.run()",
        "detail": "bot",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "bot",
        "description": "bot",
        "peekOfCode": "app = Client(\n    \"kjjjkljklbot\",\n    api_id=api_id, api_hash=api_hash,\n    bot_token=bot_token\n)\n@app.on_message()\nasync def echo(client: Client, message: Message):\n    await message.reply(message.text)\napp.run()",
        "detail": "bot",
        "documentation": {}
    },
    {
        "label": "paddstr",
        "kind": 2,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "def paddstr(strx):\n    return \"NULL\" if strx == None else \"\\'\" + escape_string(str(strx))+ \"\\'\"\ndef deal_sn_data():\n    bizProConn = MySQL(edun_pro_cfg)\n    sql = \"\"\"\n       SELECT deviceType,deviceNo,sn,imsi,iccid,simPhone,isActive,activeDate,checkCode,modelId,userId,createtime,updateTime,d_LoginUserId,issub,vehicleId,partner,validateStatus,validateBy,validateNums,soft_version,imei,deviceId,source,networkCardNubmer FROM d_track_info dti WHERE dti.sn IN (\t\n       SELECT dti.sn\n       FROM d_vehicle dv,\n       d_track_info dti,\n       d_class dc",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "deal_sn_data",
        "kind": 2,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "def deal_sn_data():\n    bizProConn = MySQL(edun_pro_cfg)\n    sql = \"\"\"\n       SELECT deviceType,deviceNo,sn,imsi,iccid,simPhone,isActive,activeDate,checkCode,modelId,userId,createtime,updateTime,d_LoginUserId,issub,vehicleId,partner,validateStatus,validateBy,validateNums,soft_version,imei,deviceId,source,networkCardNubmer FROM d_track_info dti WHERE dti.sn IN (\t\n       SELECT dti.sn\n       FROM d_vehicle dv,\n       d_track_info dti,\n       d_class dc\n       WHERE dti.vehicleId = dv.vehicleId\n       AND dc.classid = dv.classId",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "logger = logging.getLogger(\"deal_equity_data\")\nlogger.setLevel(logging.DEBUG)\nfileHandler = logging.FileHandler(\"import.log\")\nfileHandler.setLevel(logging.DEBUG)\nformatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\ncnslhandler = logging.StreamHandler()\ncnslhandler.setLevel(logging.DEBUG)\ncnslhandler.setFormatter(formatter)\nlogger.addHandler(cnslhandler)\nedun_pro_cfg = {",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "fileHandler",
        "kind": 5,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "fileHandler = logging.FileHandler(\"import.log\")\nfileHandler.setLevel(logging.DEBUG)\nformatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\ncnslhandler = logging.StreamHandler()\ncnslhandler.setLevel(logging.DEBUG)\ncnslhandler.setFormatter(formatter)\nlogger.addHandler(cnslhandler)\nedun_pro_cfg = {\n    \"host\": \"192.168.5.22\",\n    \"port\": 3307,",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "formatter",
        "kind": 5,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\ncnslhandler = logging.StreamHandler()\ncnslhandler.setLevel(logging.DEBUG)\ncnslhandler.setFormatter(formatter)\nlogger.addHandler(cnslhandler)\nedun_pro_cfg = {\n    \"host\": \"192.168.5.22\",\n    \"port\": 3307,\n    \"user\": \"user_yangll\",\n    \"passwd\": \"vGxw9jWg\",",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "cnslhandler",
        "kind": 5,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "cnslhandler = logging.StreamHandler()\ncnslhandler.setLevel(logging.DEBUG)\ncnslhandler.setFormatter(formatter)\nlogger.addHandler(cnslhandler)\nedun_pro_cfg = {\n    \"host\": \"192.168.5.22\",\n    \"port\": 3307,\n    \"user\": \"user_yangll\",\n    \"passwd\": \"vGxw9jWg\",\n    \"db\": \"gps\",",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "edun_pro_cfg",
        "kind": 5,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "edun_pro_cfg = {\n    \"host\": \"192.168.5.22\",\n    \"port\": 3307,\n    \"user\": \"user_yangll\",\n    \"passwd\": \"vGxw9jWg\",\n    \"db\": \"gps\",\n    \"charset\": \"utf8\",\n}\nedun_test_cfg = {\n    \"host\": \"192.168.3.222\",",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "edun_test_cfg",
        "kind": 5,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "edun_test_cfg = {\n    \"host\": \"192.168.3.222\",\n    \"port\": 3307,\n    \"user\": \"root\",\n    \"passwd\": \"Msd^*$@online\",\n    \"db\": \"newgps\",\n    \"charset\": \"utf8\",\n}\nfrom pymysql.converters import escape_string\n#",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "demoFindLogClass",
        "kind": 6,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "class demoFindLogClass(object):\n    def __init__(self):\n        pass\n    def demoFindLog(self):\n        deault_lng = 110.35\n        deault_lat = 34.62\n        with open(\"biz-gps-kafka-pazlpush.log.2023-07-28.99\", \"r\", encoding='UTF-8') as f:\n            content_list = f.readlines()\n        for content_line in content_list:\n            if \"定位数据推送数据的实体===\" in content_line:",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "parse_json",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def parse_json():\n    path = \"H:\\PythonWorkSpaces\\deal_lnglat_data\"\n    files = os.listdir(path)\n    s = []\n    for file in files:\n        print(file)\n        # 读取文件\n        # f = open(\"biz-gps-kafka-pazlpush.log.2023-07-28.99\", 'r', encoding='UTF-8')\n        # lines = f.readlines()\n        # for line in lines:",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "print_hi",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def print_hi(name):\n    print(f'Hi, {name}')\nclass demoFindLogClass(object):\n    def __init__(self):\n        pass\n    def demoFindLog(self):\n        deault_lng = 110.35\n        deault_lat = 34.62\n        with open(\"biz-gps-kafka-pazlpush.log.2023-07-28.99\", \"r\", encoding='UTF-8') as f:\n            content_list = f.readlines()",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "print_others",
        "kind": 2,
        "importPath": "main_bot",
        "description": "main_bot",
        "peekOfCode": "def print_others(msg):\n    print(msg)\n# 回复 my_friend 的消息 (优先匹配后注册的函数!)\n@bot.register(my_friend)\ndef reply_my_friend(msg):\n    return \"received: {} ({})\".format(msg.text, msg.type)\n# 自动接受新的好友请求\n@bot.register(msg_types=FRIENDS)\ndef auto_accept_friends(msg):\n    # 接受好友请求",
        "detail": "main_bot",
        "documentation": {}
    },
    {
        "label": "reply_my_friend",
        "kind": 2,
        "importPath": "main_bot",
        "description": "main_bot",
        "peekOfCode": "def reply_my_friend(msg):\n    return \"received: {} ({})\".format(msg.text, msg.type)\n# 自动接受新的好友请求\n@bot.register(msg_types=FRIENDS)\ndef auto_accept_friends(msg):\n    # 接受好友请求\n    new_friend = msg.card.accept()\n    # 向新的好友发送消息\n    new_friend.send(\"哈哈，我自动接受了你的好友请求\")",
        "detail": "main_bot",
        "documentation": {}
    },
    {
        "label": "auto_accept_friends",
        "kind": 2,
        "importPath": "main_bot",
        "description": "main_bot",
        "peekOfCode": "def auto_accept_friends(msg):\n    # 接受好友请求\n    new_friend = msg.card.accept()\n    # 向新的好友发送消息\n    new_friend.send(\"哈哈，我自动接受了你的好友请求\")",
        "detail": "main_bot",
        "documentation": {}
    },
    {
        "label": "bot",
        "kind": 5,
        "importPath": "main_bot",
        "description": "main_bot",
        "peekOfCode": "bot = Bot()\n# **找到好友:**\n# 搜索名称含有 \"游否\" 的男性深圳好友\nmy_friend = bot.friends().search(\"游否\", sex=MALE, city=\"深圳\")[0]\n# **发送消息:**\n# 发送文本给好友\nmy_friend.send(\"Hello WeChat!\")\n# 发送图片\nmy_friend.send_image(\"my_picture.jpg\")\n# **自动响应各类消息:**",
        "detail": "main_bot",
        "documentation": {}
    },
    {
        "label": "my_friend",
        "kind": 5,
        "importPath": "main_bot",
        "description": "main_bot",
        "peekOfCode": "my_friend = bot.friends().search(\"游否\", sex=MALE, city=\"深圳\")[0]\n# **发送消息:**\n# 发送文本给好友\nmy_friend.send(\"Hello WeChat!\")\n# 发送图片\nmy_friend.send_image(\"my_picture.jpg\")\n# **自动响应各类消息:**\n# 打印来自其他好友、群聊和公众号的消息\n@bot.register()\ndef print_others(msg):",
        "detail": "main_bot",
        "documentation": {}
    },
    {
        "label": "reply_msg",
        "kind": 2,
        "importPath": "main_itchat",
        "description": "main_itchat",
        "peekOfCode": "def reply_msg(msg):\n    print(\"收到一条信息：\", msg.text)\nif __name__ == \"__main__\":\n    itchat.auto_login()\n    time.sleep(5)\n    itchat.send(\"文件助手你好哦\", toUserName=\"filehelper\")\n    itchat.run()",
        "detail": "main_itchat",
        "documentation": {}
    },
    {
        "label": "friends",
        "kind": 5,
        "importPath": "main_itchat01",
        "description": "main_itchat01",
        "peekOfCode": "friends = itchat.get_friends()\n# 我们可以使用json库将好友列表转换成json格式\nprint(json.dumps(friends))\n# 运行程序\nitchat.run()",
        "detail": "main_itchat01",
        "documentation": {}
    },
    {
        "label": "my_method",
        "kind": 2,
        "importPath": "main_test01",
        "description": "main_test01",
        "peekOfCode": "def my_method():\n    print(\"你好\")\n    # requests.post()\n# end def\ndef bot_chat():\n    \"\"\"\n    Purpose:\n    \"\"\"\n    print(\"nihao\")\n# end def",
        "detail": "main_test01",
        "documentation": {}
    },
    {
        "label": "bot_chat",
        "kind": 2,
        "importPath": "main_test01",
        "description": "main_test01",
        "peekOfCode": "def bot_chat():\n    \"\"\"\n    Purpose:\n    \"\"\"\n    print(\"nihao\")\n# end def\nif __name__ == \"__main__\":\n    my_method()\n    bot_chat()\n    print(\"nihao\")",
        "detail": "main_test01",
        "documentation": {}
    }
]