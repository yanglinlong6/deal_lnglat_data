[
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "hmm",
        "importPath": "hmmlearn",
        "description": "hmmlearn",
        "isExtraImport": true,
        "detail": "hmmlearn",
        "documentation": {}
    },
    {
        "label": "AssistantAgent",
        "importPath": "autogen",
        "description": "autogen",
        "isExtraImport": true,
        "detail": "autogen",
        "documentation": {}
    },
    {
        "label": "UserProxyAgent",
        "importPath": "autogen",
        "description": "autogen",
        "isExtraImport": true,
        "detail": "autogen",
        "documentation": {}
    },
    {
        "label": "config_list_from_json",
        "importPath": "autogen",
        "description": "autogen",
        "isExtraImport": true,
        "detail": "autogen",
        "documentation": {}
    },
    {
        "label": "UserProxyAgent",
        "importPath": "autogen",
        "description": "autogen",
        "isExtraImport": true,
        "detail": "autogen",
        "documentation": {}
    },
    {
        "label": "ConversableAgent",
        "importPath": "autogen",
        "description": "autogen",
        "isExtraImport": true,
        "detail": "autogen",
        "documentation": {}
    },
    {
        "label": "config_list_from_json",
        "importPath": "autogen",
        "description": "autogen",
        "isExtraImport": true,
        "detail": "autogen",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "path",
        "importPath": "os",
        "description": "os",
        "isExtraImport": true,
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "ChatGLM3",
        "importPath": "langchain_community.llms.chatglm3",
        "description": "langchain_community.llms.chatglm3",
        "isExtraImport": true,
        "detail": "langchain_community.llms.chatglm3",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "ConversationalRetrievalChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "APIChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMBashChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMCheckerChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMRequestsChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMSummarizationCheckerChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMSummarizationCheckerChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMSummarizationCheckerChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "OpenAIModerationChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "SequentialChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "SimpleSequentialChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "RetrievalQA",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "ConversationChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "RetrievalQA",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "StringPromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "HumanMessagePromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "MessagesPlaceholder",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "SystemMessagePromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "AIMessage",
        "importPath": "langchain.schema.messages",
        "description": "langchain.schema.messages",
        "isExtraImport": true,
        "detail": "langchain.schema.messages",
        "documentation": {}
    },
    {
        "label": "ZhipuAI",
        "importPath": "zhipuai",
        "description": "zhipuai",
        "isExtraImport": true,
        "detail": "zhipuai",
        "documentation": {}
    },
    {
        "label": "ZhipuAI",
        "importPath": "zhipuai",
        "description": "zhipuai",
        "isExtraImport": true,
        "detail": "zhipuai",
        "documentation": {}
    },
    {
        "label": "ZhipuAI",
        "importPath": "zhipuai",
        "description": "zhipuai",
        "isExtraImport": true,
        "detail": "zhipuai",
        "documentation": {}
    },
    {
        "label": "ZhipuAI",
        "importPath": "zhipuai",
        "description": "zhipuai",
        "isExtraImport": true,
        "detail": "zhipuai",
        "documentation": {}
    },
    {
        "label": "ZhipuAI",
        "importPath": "zhipuai",
        "description": "zhipuai",
        "isExtraImport": true,
        "detail": "zhipuai",
        "documentation": {}
    },
    {
        "label": "ZhipuAI",
        "importPath": "zhipuai",
        "description": "zhipuai",
        "isExtraImport": true,
        "detail": "zhipuai",
        "documentation": {}
    },
    {
        "label": "TextLoader",
        "importPath": "langchain.document_loaders",
        "description": "langchain.document_loaders",
        "isExtraImport": true,
        "detail": "langchain.document_loaders",
        "documentation": {}
    },
    {
        "label": "TextLoader",
        "importPath": "langchain.document_loaders",
        "description": "langchain.document_loaders",
        "isExtraImport": true,
        "detail": "langchain.document_loaders",
        "documentation": {}
    },
    {
        "label": "OnlinePDFLoader",
        "importPath": "langchain.document_loaders",
        "description": "langchain.document_loaders",
        "isExtraImport": true,
        "detail": "langchain.document_loaders",
        "documentation": {}
    },
    {
        "label": "CharacterTextSplitter",
        "importPath": "langchain.text_splitter",
        "description": "langchain.text_splitter",
        "isExtraImport": true,
        "detail": "langchain.text_splitter",
        "documentation": {}
    },
    {
        "label": "RecursiveCharacterTextSplitter",
        "importPath": "langchain.text_splitter",
        "description": "langchain.text_splitter",
        "isExtraImport": true,
        "detail": "langchain.text_splitter",
        "documentation": {}
    },
    {
        "label": "OpenAIEmbeddings",
        "importPath": "langchain.embeddings.openai",
        "description": "langchain.embeddings.openai",
        "isExtraImport": true,
        "detail": "langchain.embeddings.openai",
        "documentation": {}
    },
    {
        "label": "OpenAIEmbeddings",
        "importPath": "langchain.embeddings.openai",
        "description": "langchain.embeddings.openai",
        "isExtraImport": true,
        "detail": "langchain.embeddings.openai",
        "documentation": {}
    },
    {
        "label": "DeepLake",
        "importPath": "langchain.vectorstores",
        "description": "langchain.vectorstores",
        "isExtraImport": true,
        "detail": "langchain.vectorstores",
        "documentation": {}
    },
    {
        "label": "DeepLake",
        "importPath": "langchain.vectorstores",
        "description": "langchain.vectorstores",
        "isExtraImport": true,
        "detail": "langchain.vectorstores",
        "documentation": {}
    },
    {
        "label": "FAISS",
        "importPath": "langchain.vectorstores",
        "description": "langchain.vectorstores",
        "isExtraImport": true,
        "detail": "langchain.vectorstores",
        "documentation": {}
    },
    {
        "label": "Chroma",
        "importPath": "langchain.vectorstores",
        "description": "langchain.vectorstores",
        "isExtraImport": true,
        "detail": "langchain.vectorstores",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain.chat_models",
        "description": "langchain.chat_models",
        "isExtraImport": true,
        "detail": "langchain.chat_models",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain.chat_models",
        "description": "langchain.chat_models",
        "isExtraImport": true,
        "detail": "langchain.chat_models",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain.chat_models",
        "description": "langchain.chat_models",
        "isExtraImport": true,
        "detail": "langchain.chat_models",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain.chat_models",
        "description": "langchain.chat_models",
        "isExtraImport": true,
        "detail": "langchain.chat_models",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain.chat_models",
        "description": "langchain.chat_models",
        "isExtraImport": true,
        "detail": "langchain.chat_models",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain.chat_models",
        "description": "langchain.chat_models",
        "isExtraImport": true,
        "detail": "langchain.chat_models",
        "documentation": {}
    },
    {
        "label": "gTTS",
        "importPath": "gtts",
        "description": "gtts",
        "isExtraImport": true,
        "detail": "gtts",
        "documentation": {}
    },
    {
        "label": "pyttsx3",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pyttsx3",
        "description": "pyttsx3",
        "detail": "pyttsx3",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "io",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "io",
        "description": "io",
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "pprint",
        "importPath": "pprint",
        "description": "pprint",
        "isExtraImport": true,
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "pprint",
        "importPath": "pprint",
        "description": "pprint",
        "isExtraImport": true,
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "pprint",
        "importPath": "pprint",
        "description": "pprint",
        "isExtraImport": true,
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "speech_recognition",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "speech_recognition",
        "description": "speech_recognition",
        "detail": "speech_recognition",
        "documentation": {}
    },
    {
        "label": "AutoModelForSequenceClassification",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "MsDataset",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoModelForSequenceClassification",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "Model",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoModelForSequenceClassification",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "MsDataset",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoModelForSequenceClassification",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "snapshot_download",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoModelForSequenceClassification",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "MsDataset",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoModelForSequenceClassification",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "default_data_collator",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "PerceiverTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "PerceiverForMaskedLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "PerceiverTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "PerceiverForMaskedLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "default_data_collator",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "pipeline",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "pipeline",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "default_data_collator",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Wav2Vec2ForCTC",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Wav2Vec2Processor",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Wav2Vec2Model",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "LoRAConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "TrainingArguments",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "LoRAConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "LoRAConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "LoRAConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "TrainingArguments",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "LoRAConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "ResTuningConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "PeftModel",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "PeftConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "LoraConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "LoRAConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "TrainingArguments",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "LoRAConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "LoraConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "read_config",
        "importPath": "modelscope.utils.hub",
        "description": "modelscope.utils.hub",
        "isExtraImport": true,
        "detail": "modelscope.utils.hub",
        "documentation": {}
    },
    {
        "label": "MsDataset",
        "importPath": "modelscope.msdatasets",
        "description": "modelscope.msdatasets",
        "isExtraImport": true,
        "detail": "modelscope.msdatasets",
        "documentation": {}
    },
    {
        "label": "build_trainer",
        "importPath": "modelscope.trainers",
        "description": "modelscope.trainers",
        "isExtraImport": true,
        "detail": "modelscope.trainers",
        "documentation": {}
    },
    {
        "label": "build_trainer",
        "importPath": "modelscope.trainers",
        "description": "modelscope.trainers",
        "isExtraImport": true,
        "detail": "modelscope.trainers",
        "documentation": {}
    },
    {
        "label": "tempfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tempfile",
        "description": "tempfile",
        "detail": "tempfile",
        "documentation": {}
    },
    {
        "label": "Trainers",
        "importPath": "modelscope.metainfo",
        "description": "modelscope.metainfo",
        "isExtraImport": true,
        "detail": "modelscope.metainfo",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Linear",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "CrossEntropyLoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "torch.optim",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.optim",
        "description": "torch.optim",
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "torch.optim",
        "description": "torch.optim",
        "isExtraImport": true,
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "StepLR",
        "importPath": "torch.optim.lr_scheduler",
        "description": "torch.optim.lr_scheduler",
        "isExtraImport": true,
        "detail": "torch.optim.lr_scheduler",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "default_collate",
        "importPath": "torch.utils.data.dataloader",
        "description": "torch.utils.data.dataloader",
        "isExtraImport": true,
        "detail": "torch.utils.data.dataloader",
        "documentation": {}
    },
    {
        "label": "AgentType",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "initialize_agent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "load_tools",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "Tool",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "BaseSingleActionAgent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentOutputParser",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "LLMSingleActionAgent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "Tool",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "ZeroShotAgent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "Tool",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "Tool",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "ZeroShotAgent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "BaseMultiActionAgent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "Tool",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "load_tools",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "initialize_agent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentType",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentType",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "initialize_agent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "load_tools",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentType",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "initialize_agent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "load_tools",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentType",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "initialize_agent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "load_tools",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentType",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "initialize_agent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "load_tools",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentType",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "initialize_agent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "load_tools",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "Ollama",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "Ollama",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "langchain",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "langchain",
        "description": "langchain",
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "SerpAPIWrapper",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "SerpAPIWrapper",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "SerpAPIWrapper",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "SerpAPIWrapper",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "SerpAPIWrapper",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "LLMMathChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "ConversationChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypedDict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Annotated",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "AgentAction",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AgentFinish",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AgentAction",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AgentFinish",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AgentAction",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AgentFinish",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AIMessage",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "SystemMessage",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "OpenAIEmbeddings",
        "importPath": "langchain.embeddings",
        "description": "langchain.embeddings",
        "isExtraImport": true,
        "detail": "langchain.embeddings",
        "documentation": {}
    },
    {
        "label": "GPT4AllEmbeddings",
        "importPath": "langchain.embeddings",
        "description": "langchain.embeddings",
        "isExtraImport": true,
        "detail": "langchain.embeddings",
        "documentation": {}
    },
    {
        "label": "API_RESPONSE_PROMPT",
        "importPath": "langchain.chains.api.prompt",
        "description": "langchain.chains.api.prompt",
        "isExtraImport": true,
        "detail": "langchain.chains.api.prompt",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts.prompt",
        "description": "langchain.prompts.prompt",
        "isExtraImport": true,
        "detail": "langchain.prompts.prompt",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts.prompt",
        "description": "langchain.prompts.prompt",
        "isExtraImport": true,
        "detail": "langchain.prompts.prompt",
        "documentation": {}
    },
    {
        "label": "open_meteo_docs",
        "importPath": "langchain.chains.api",
        "description": "langchain.chains.api",
        "isExtraImport": true,
        "detail": "langchain.chains.api",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain.chains.llm",
        "description": "langchain.chains.llm",
        "isExtraImport": true,
        "detail": "langchain.chains.llm",
        "documentation": {}
    },
    {
        "label": "ConstitutionalChain",
        "importPath": "langchain.chains.constitutional_ai.base",
        "description": "langchain.chains.constitutional_ai.base",
        "isExtraImport": true,
        "detail": "langchain.chains.constitutional_ai.base",
        "documentation": {}
    },
    {
        "label": "ConstitutionalPrinciple",
        "importPath": "langchain.chains.constitutional_ai.models",
        "description": "langchain.chains.constitutional_ai.models",
        "isExtraImport": true,
        "detail": "langchain.chains.constitutional_ai.models",
        "documentation": {}
    },
    {
        "label": "BashOutputParser",
        "importPath": "langchain.chains.llm_bash.prompt",
        "description": "langchain.chains.llm_bash.prompt",
        "isExtraImport": true,
        "detail": "langchain.chains.llm_bash.prompt",
        "documentation": {}
    },
    {
        "label": "BashProcess",
        "importPath": "langchain.utilities.bash",
        "description": "langchain.utilities.bash",
        "isExtraImport": true,
        "detail": "langchain.utilities.bash",
        "documentation": {}
    },
    {
        "label": "asyncio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "asyncio",
        "description": "asyncio",
        "detail": "asyncio",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "VectorstoreIndexCreator",
        "importPath": "langchain.indexes",
        "description": "langchain.indexes",
        "isExtraImport": true,
        "detail": "langchain.indexes",
        "documentation": {}
    },
    {
        "label": "ConversationBufferMemory",
        "importPath": "langchain.memory",
        "description": "langchain.memory",
        "isExtraImport": true,
        "detail": "langchain.memory",
        "documentation": {}
    },
    {
        "label": "ConversationBufferMemory",
        "importPath": "langchain.memory",
        "description": "langchain.memory",
        "isExtraImport": true,
        "detail": "langchain.memory",
        "documentation": {}
    },
    {
        "label": "openai",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "openai",
        "description": "openai",
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "AzureOpenAI",
        "importPath": "openai",
        "description": "openai",
        "isExtraImport": true,
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "AzureOpenAI",
        "importPath": "openai",
        "description": "openai",
        "isExtraImport": true,
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "HumanMessagePromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "SystemMessagePromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "HumanMessagePromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "SystemMessagePromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "TavilySearchResults",
        "importPath": "langchain_community.tools.tavily_search",
        "description": "langchain_community.tools.tavily_search",
        "isExtraImport": true,
        "detail": "langchain_community.tools.tavily_search",
        "documentation": {}
    },
    {
        "label": "ToolExecutor",
        "importPath": "langgraph.prebuilt",
        "description": "langgraph.prebuilt",
        "isExtraImport": true,
        "detail": "langgraph.prebuilt",
        "documentation": {}
    },
    {
        "label": "ToolInvocation",
        "importPath": "langgraph.prebuilt",
        "description": "langgraph.prebuilt",
        "isExtraImport": true,
        "detail": "langgraph.prebuilt",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "format_tool_to_openai_function",
        "importPath": "langchain.tools.render",
        "description": "langchain.tools.render",
        "isExtraImport": true,
        "detail": "langchain.tools.render",
        "documentation": {}
    },
    {
        "label": "operator",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "operator",
        "description": "operator",
        "detail": "operator",
        "documentation": {}
    },
    {
        "label": "BaseMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "FunctionMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "StateGraph",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "END",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "timm,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "timm.",
        "description": "timm.",
        "detail": "timm.",
        "documentation": {}
    },
    {
        "label": "SbertForSequenceClassification",
        "importPath": "modelscope.models.nlp",
        "description": "modelscope.models.nlp",
        "isExtraImport": true,
        "detail": "modelscope.models.nlp",
        "documentation": {}
    },
    {
        "label": "SbertForSequenceClassification",
        "importPath": "modelscope.models.nlp",
        "description": "modelscope.models.nlp",
        "isExtraImport": true,
        "detail": "modelscope.models.nlp",
        "documentation": {}
    },
    {
        "label": "SbertConfig",
        "importPath": "modelscope.models.nlp.structbert",
        "description": "modelscope.models.nlp.structbert",
        "isExtraImport": true,
        "detail": "modelscope.models.nlp.structbert",
        "documentation": {}
    },
    {
        "label": "SbertConfig",
        "importPath": "modelscope.models.nlp.structbert",
        "description": "modelscope.models.nlp.structbert",
        "isExtraImport": true,
        "detail": "modelscope.models.nlp.structbert",
        "documentation": {}
    },
    {
        "label": "Pipeline",
        "importPath": "pyannote.audio",
        "description": "pyannote.audio",
        "isExtraImport": true,
        "detail": "pyannote.audio",
        "documentation": {}
    },
    {
        "label": "timm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "timm",
        "description": "timm",
        "detail": "timm",
        "documentation": {}
    },
    {
        "label": "DatasetName",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "ModelType",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "SftArguments",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "sft_main",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "ModelType",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "get_vllm_engine",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "get_default_template_type",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "get_template",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "inference_vllm",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "pipeline",
        "importPath": "modelscope.pipelines",
        "description": "modelscope.pipelines",
        "isExtraImport": true,
        "detail": "modelscope.pipelines",
        "documentation": {}
    },
    {
        "label": "pipeline",
        "importPath": "modelscope.pipelines",
        "description": "modelscope.pipelines",
        "isExtraImport": true,
        "detail": "modelscope.pipelines",
        "documentation": {}
    },
    {
        "label": "Tasks",
        "importPath": "modelscope.utils.constant",
        "description": "modelscope.utils.constant",
        "isExtraImport": true,
        "detail": "modelscope.utils.constant",
        "documentation": {}
    },
    {
        "label": "Tasks",
        "importPath": "modelscope.utils.constant",
        "description": "modelscope.utils.constant",
        "isExtraImport": true,
        "detail": "modelscope.utils.constant",
        "documentation": {}
    },
    {
        "label": "StableDiffusionPipeline",
        "importPath": "ppdiffusers",
        "description": "ppdiffusers",
        "isExtraImport": true,
        "detail": "ppdiffusers",
        "documentation": {}
    },
    {
        "label": "paddle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "paddle",
        "description": "paddle",
        "detail": "paddle",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "Model",
        "importPath": "modelscope.models",
        "description": "modelscope.models",
        "isExtraImport": true,
        "detail": "modelscope.models",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift.tuners",
        "description": "swift.tuners",
        "isExtraImport": true,
        "detail": "swift.tuners",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "MySQLdb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "MySQLdb",
        "description": "MySQLdb",
        "detail": "MySQLdb",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "langchain.callbacks.manager",
        "description": "langchain.callbacks.manager",
        "isExtraImport": true,
        "detail": "langchain.callbacks.manager",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "langchain.callbacks.manager",
        "description": "langchain.callbacks.manager",
        "isExtraImport": true,
        "detail": "langchain.callbacks.manager",
        "documentation": {}
    },
    {
        "label": "StreamingStdOutCallbackHandler",
        "importPath": "langchain.callbacks.streaming_stdout",
        "description": "langchain.callbacks.streaming_stdout",
        "isExtraImport": true,
        "detail": "langchain.callbacks.streaming_stdout",
        "documentation": {}
    },
    {
        "label": "StreamingStdOutCallbackHandler",
        "importPath": "langchain.callbacks.streaming_stdout",
        "description": "langchain.callbacks.streaming_stdout",
        "isExtraImport": true,
        "detail": "langchain.callbacks.streaming_stdout",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "pyaudio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pyaudio",
        "description": "pyaudio",
        "detail": "pyaudio",
        "documentation": {}
    },
    {
        "label": "uuid",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "uuid",
        "description": "uuid",
        "detail": "uuid",
        "documentation": {}
    },
    {
        "label": "wave",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "wave",
        "description": "wave",
        "detail": "wave",
        "documentation": {}
    },
    {
        "label": "base64",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "base64",
        "description": "base64",
        "detail": "base64",
        "documentation": {}
    },
    {
        "label": "hashlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hashlib",
        "description": "hashlib",
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "reload",
        "importPath": "importlib ",
        "description": "importlib ",
        "isExtraImport": true,
        "detail": "importlib ",
        "documentation": {}
    },
    {
        "label": "parse",
        "importPath": "urllib",
        "description": "urllib",
        "isExtraImport": true,
        "detail": "urllib",
        "documentation": {}
    },
    {
        "label": "AudioSegment",
        "importPath": "pydub",
        "description": "pydub",
        "isExtraImport": true,
        "detail": "pydub",
        "documentation": {}
    },
    {
        "label": "playsound",
        "importPath": "playsound",
        "description": "playsound",
        "isExtraImport": true,
        "detail": "playsound",
        "documentation": {}
    },
    {
        "label": "Groq",
        "importPath": "groq",
        "description": "groq",
        "isExtraImport": true,
        "detail": "groq",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "OpenAIChat",
        "importPath": "langchain_community.llms",
        "description": "langchain_community.llms",
        "isExtraImport": true,
        "detail": "langchain_community.llms",
        "documentation": {}
    },
    {
        "label": "ASRExecutor",
        "importPath": "paddlespeech.cli.asr.infer",
        "description": "paddlespeech.cli.asr.infer",
        "isExtraImport": true,
        "detail": "paddlespeech.cli.asr.infer",
        "documentation": {}
    },
    {
        "label": "TTSExecutor",
        "importPath": "paddlespeech.cli.tts",
        "description": "paddlespeech.cli.tts",
        "isExtraImport": true,
        "detail": "paddlespeech.cli.tts",
        "documentation": {}
    },
    {
        "label": "AudioExecutor",
        "importPath": "paddlespeech.cli.audio",
        "description": "paddlespeech.cli.audio",
        "isExtraImport": true,
        "detail": "paddlespeech.cli.audio",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "easyocr",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "easyocr",
        "description": "easyocr",
        "detail": "easyocr",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "PaddleOCR",
        "importPath": "paddleocr",
        "description": "paddleocr",
        "isExtraImport": true,
        "detail": "paddleocr",
        "documentation": {}
    },
    {
        "label": "draw_ocr",
        "importPath": "paddleocr",
        "description": "paddleocr",
        "isExtraImport": true,
        "detail": "paddleocr",
        "documentation": {}
    },
    {
        "label": "soundfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "soundfile",
        "description": "soundfile",
        "detail": "soundfile",
        "documentation": {}
    },
    {
        "label": "matplotlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib",
        "description": "matplotlib",
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "models",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "Client",
        "importPath": "pyrogram",
        "description": "pyrogram",
        "isExtraImport": true,
        "detail": "pyrogram",
        "documentation": {}
    },
    {
        "label": "Message",
        "importPath": "pyrogram.types",
        "description": "pyrogram.types",
        "isExtraImport": true,
        "detail": "pyrogram.types",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "Decimal",
        "importPath": "decimal",
        "description": "decimal",
        "isExtraImport": true,
        "detail": "decimal",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "dateutil.relativedelta",
        "description": "dateutil.relativedelta",
        "isExtraImport": true,
        "detail": "dateutil.relativedelta",
        "documentation": {}
    },
    {
        "label": "MySQL",
        "importPath": "my_utils.MysqlUtility",
        "description": "my_utils.MysqlUtility",
        "isExtraImport": true,
        "detail": "my_utils.MysqlUtility",
        "documentation": {}
    },
    {
        "label": "groupby",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "escape_string",
        "importPath": "pymysql.converters",
        "description": "pymysql.converters",
        "isExtraImport": true,
        "detail": "pymysql.converters",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "wxpy",
        "description": "wxpy",
        "isExtraImport": true,
        "detail": "wxpy",
        "documentation": {}
    },
    {
        "label": "itchat",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "itchat",
        "description": "itchat",
        "detail": "itchat",
        "documentation": {}
    },
    {
        "label": "itchat,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "itchat.",
        "description": "itchat.",
        "detail": "itchat.",
        "documentation": {}
    },
    {
        "label": "n_states",
        "kind": 5,
        "importPath": "HMM.test01",
        "description": "HMM.test01",
        "peekOfCode": "n_states = 2\nn_observations = 3\n# HMM\nmodel = hmm.MultinomialHMM(n_components=n_states)\n# \nmodel.startprob_ = np.array([0.6, 0.4])\nmodel.transmat_ = np.array([[0.7, 0.3],\n                            [0.4, 0.6]])\nmodel.emissionprob_ = np.array([[0.1, 0.4, 0.5],\n                                [0.6, 0.3, 0.1]])",
        "detail": "HMM.test01",
        "documentation": {}
    },
    {
        "label": "n_observations",
        "kind": 5,
        "importPath": "HMM.test01",
        "description": "HMM.test01",
        "peekOfCode": "n_observations = 3\n# HMM\nmodel = hmm.MultinomialHMM(n_components=n_states)\n# \nmodel.startprob_ = np.array([0.6, 0.4])\nmodel.transmat_ = np.array([[0.7, 0.3],\n                            [0.4, 0.6]])\nmodel.emissionprob_ = np.array([[0.1, 0.4, 0.5],\n                                [0.6, 0.3, 0.1]])\n# ",
        "detail": "HMM.test01",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "HMM.test01",
        "description": "HMM.test01",
        "peekOfCode": "model = hmm.MultinomialHMM(n_components=n_states)\n# \nmodel.startprob_ = np.array([0.6, 0.4])\nmodel.transmat_ = np.array([[0.7, 0.3],\n                            [0.4, 0.6]])\nmodel.emissionprob_ = np.array([[0.1, 0.4, 0.5],\n                                [0.6, 0.3, 0.1]])\n# \n = np.array([[0], [1], [2], [1]])\n# ",
        "detail": "HMM.test01",
        "documentation": {}
    },
    {
        "label": "model.startprob_",
        "kind": 5,
        "importPath": "HMM.test01",
        "description": "HMM.test01",
        "peekOfCode": "model.startprob_ = np.array([0.6, 0.4])\nmodel.transmat_ = np.array([[0.7, 0.3],\n                            [0.4, 0.6]])\nmodel.emissionprob_ = np.array([[0.1, 0.4, 0.5],\n                                [0.6, 0.3, 0.1]])\n# \n = np.array([[0], [1], [2], [1]])\n# \nlogprob,  = model.decode(, algorithm=\"viterbi\")\nprint(\":\", )",
        "detail": "HMM.test01",
        "documentation": {}
    },
    {
        "label": "model.transmat_",
        "kind": 5,
        "importPath": "HMM.test01",
        "description": "HMM.test01",
        "peekOfCode": "model.transmat_ = np.array([[0.7, 0.3],\n                            [0.4, 0.6]])\nmodel.emissionprob_ = np.array([[0.1, 0.4, 0.5],\n                                [0.6, 0.3, 0.1]])\n# \n = np.array([[0], [1], [2], [1]])\n# \nlogprob,  = model.decode(, algorithm=\"viterbi\")\nprint(\":\", )",
        "detail": "HMM.test01",
        "documentation": {}
    },
    {
        "label": "model.emissionprob_",
        "kind": 5,
        "importPath": "HMM.test01",
        "description": "HMM.test01",
        "peekOfCode": "model.emissionprob_ = np.array([[0.1, 0.4, 0.5],\n                                [0.6, 0.3, 0.1]])\n# \n = np.array([[0], [1], [2], [1]])\n# \nlogprob,  = model.decode(, algorithm=\"viterbi\")\nprint(\":\", )",
        "detail": "HMM.test01",
        "documentation": {}
    },
    {
        "label": "",
        "kind": 5,
        "importPath": "HMM.test01",
        "description": "HMM.test01",
        "peekOfCode": " = np.array([[0], [1], [2], [1]])\n# \nlogprob,  = model.decode(, algorithm=\"viterbi\")\nprint(\":\", )",
        "detail": "HMM.test01",
        "documentation": {}
    },
    {
        "label": "config_list",
        "kind": 5,
        "importPath": "autogen.test01",
        "description": "autogen.test01",
        "peekOfCode": "config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")\n# You can also set config_list directly as a list, for example, config_list = [{'model': 'gpt-4', 'api_key': '<your OpenAI API key here>'},]\nassistant = AssistantAgent(\n    \"assistant\",\n    llm_config={\"config_list\": config_list},\n    code_execution_config={\"work_dir\": \".\", \"use_docker\": False},\n)\nuser_proxy = UserProxyAgent(\n    \"user_proxy\", code_execution_config={\"work_dir\": \".\", \"use_docker\": False}\n)  # IMPORTANT: set to True to run code in docker, recommended",
        "detail": "autogen.test01",
        "documentation": {}
    },
    {
        "label": "assistant",
        "kind": 5,
        "importPath": "autogen.test01",
        "description": "autogen.test01",
        "peekOfCode": "assistant = AssistantAgent(\n    \"assistant\",\n    llm_config={\"config_list\": config_list},\n    code_execution_config={\"work_dir\": \".\", \"use_docker\": False},\n)\nuser_proxy = UserProxyAgent(\n    \"user_proxy\", code_execution_config={\"work_dir\": \".\", \"use_docker\": False}\n)  # IMPORTANT: set to True to run code in docker, recommended\nuser_proxy.initiate_chat(\n    assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\"",
        "detail": "autogen.test01",
        "documentation": {}
    },
    {
        "label": "user_proxy",
        "kind": 5,
        "importPath": "autogen.test01",
        "description": "autogen.test01",
        "peekOfCode": "user_proxy = UserProxyAgent(\n    \"user_proxy\", code_execution_config={\"work_dir\": \".\", \"use_docker\": False}\n)  # IMPORTANT: set to True to run code in docker, recommended\nuser_proxy.initiate_chat(\n    assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\"\n)\n# This initiates an automated chat between the two agents to solve the task",
        "detail": "autogen.test01",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "autogen.test02",
        "description": "autogen.test02",
        "peekOfCode": "def main():\n    # Load LLM inference endpoints from an env variable or a file\n    # See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints\n    # and OAI_CONFIG_LIST_sample.\n    # For example, if you have created a OAI_CONFIG_LIST file in the current working directory, that file will be used.\n    config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")\n    # Create the agent that uses the LLM.\n    assistant = ConversableAgent(\n        \"agent\",\n        llm_config={\"config_list\": config_list},",
        "detail": "autogen.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "autogen.test02",
        "description": "autogen.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\ndef main():\n    # Load LLM inference endpoints from an env variable or a file\n    # See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints\n    # and OAI_CONFIG_LIST_sample.\n    # For example, if you have created a OAI_CONFIG_LIST file in the current working directory, that file will be used.\n    config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")\n    # Create the agent that uses the LLM.\n    assistant = ConversableAgent(\n        \"agent\",",
        "detail": "autogen.test02",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "chatglm.test",
        "description": "chatglm.test",
        "peekOfCode": "template = \"\"\"{question}\"\"\"\nprompt = PromptTemplate.from_template(template)\n# endpoint_url = \"http://127.0.0.1:8000/v1/chat/completions\"\n# endpoint_url = \"https://open.bigmodel.cn/api/paas/v4/chat/completions\"\nendpoint_url = \"https://open.bigmodel.cn/api/paas/v4/chat/completions\"\nmessages = [\n    AIMessage(content=\"\"),\n    AIMessage(content=\"\"),\n]\nllm = ChatGLM3(",
        "detail": "chatglm.test",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "chatglm.test",
        "description": "chatglm.test",
        "peekOfCode": "prompt = PromptTemplate.from_template(template)\n# endpoint_url = \"http://127.0.0.1:8000/v1/chat/completions\"\n# endpoint_url = \"https://open.bigmodel.cn/api/paas/v4/chat/completions\"\nendpoint_url = \"https://open.bigmodel.cn/api/paas/v4/chat/completions\"\nmessages = [\n    AIMessage(content=\"\"),\n    AIMessage(content=\"\"),\n]\nllm = ChatGLM3(\n    endpoint_url=endpoint_url,",
        "detail": "chatglm.test",
        "documentation": {}
    },
    {
        "label": "endpoint_url",
        "kind": 5,
        "importPath": "chatglm.test",
        "description": "chatglm.test",
        "peekOfCode": "endpoint_url = \"https://open.bigmodel.cn/api/paas/v4/chat/completions\"\nmessages = [\n    AIMessage(content=\"\"),\n    AIMessage(content=\"\"),\n]\nllm = ChatGLM3(\n    endpoint_url=endpoint_url,\n    max_tokens=80000,\n    prefix_messages=messages,\n    top_p=0.9,",
        "detail": "chatglm.test",
        "documentation": {}
    },
    {
        "label": "messages",
        "kind": 5,
        "importPath": "chatglm.test",
        "description": "chatglm.test",
        "peekOfCode": "messages = [\n    AIMessage(content=\"\"),\n    AIMessage(content=\"\"),\n]\nllm = ChatGLM3(\n    endpoint_url=endpoint_url,\n    max_tokens=80000,\n    prefix_messages=messages,\n    top_p=0.9,\n)",
        "detail": "chatglm.test",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "chatglm.test",
        "description": "chatglm.test",
        "peekOfCode": "llm = ChatGLM3(\n    endpoint_url=endpoint_url,\n    max_tokens=80000,\n    prefix_messages=messages,\n    top_p=0.9,\n)\nllm_chain = LLMChain(prompt=prompt, llm=llm)\nquestion = \"\"\nresult=llm_chain.run(question)\nprint(result)",
        "detail": "chatglm.test",
        "documentation": {}
    },
    {
        "label": "llm_chain",
        "kind": 5,
        "importPath": "chatglm.test",
        "description": "chatglm.test",
        "peekOfCode": "llm_chain = LLMChain(prompt=prompt, llm=llm)\nquestion = \"\"\nresult=llm_chain.run(question)\nprint(result)",
        "detail": "chatglm.test",
        "documentation": {}
    },
    {
        "label": "question",
        "kind": 5,
        "importPath": "chatglm.test",
        "description": "chatglm.test",
        "peekOfCode": "question = \"\"\nresult=llm_chain.run(question)\nprint(result)",
        "detail": "chatglm.test",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "chatglm.test01",
        "description": "chatglm.test01",
        "peekOfCode": "client = ZhipuAI(\n    api_key=\"2544ea141595d420ca10f891833096ac.deW5ICKFVh4JivTN\"\n)  # APIKey\nresponse = client.chat.completions.create(\n    model=\"glm-4\",  # \n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"slogan\",\n        },",
        "detail": "chatglm.test01",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "chatglm.test01",
        "description": "chatglm.test01",
        "peekOfCode": "response = client.chat.completions.create(\n    model=\"glm-4\",  # \n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"slogan\",\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"slogan\",",
        "detail": "chatglm.test01",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "chatglm.test02",
        "description": "chatglm.test02",
        "peekOfCode": "client = ZhipuAI(\n    api_key=\"2544ea141595d420ca10f891833096ac.deW5ICKFVh4JivTN\"\n)  # APIKey\nresponse = client.chat.completions.create(\n    model=\"glm-4\",  # \n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"\",\n        },",
        "detail": "chatglm.test02",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "chatglm.test02",
        "description": "chatglm.test02",
        "peekOfCode": "response = client.chat.completions.create(\n    model=\"glm-4\",  # \n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"\",",
        "detail": "chatglm.test02",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "chatglm.test03",
        "description": "chatglm.test03",
        "peekOfCode": "client = ZhipuAI(\n    api_key=\"2544ea141595d420ca10f891833096ac.deW5ICKFVh4JivTN\"\n)  # APIKey\nresponse = client.chat.completions.create(\n    model=\"glm-4\",  # \n    messages=[\n        {\"role\": \"user\", \"content\": \"\"},\n    ],\n    stream=True,\n)",
        "detail": "chatglm.test03",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "chatglm.test03",
        "description": "chatglm.test03",
        "peekOfCode": "response = client.chat.completions.create(\n    model=\"glm-4\",  # \n    messages=[\n        {\"role\": \"user\", \"content\": \"\"},\n    ],\n    stream=True,\n)\nfor chunk in response:\n    print(chunk.choices[0].delta)",
        "detail": "chatglm.test03",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "chatglm.test04",
        "description": "chatglm.test04",
        "peekOfCode": "client = ZhipuAI(\n    api_key=\"2544ea141595d420ca10f891833096ac.deW5ICKFVh4JivTN\"\n)  # APIKey\nresponse = client.chat.completions.create(\n    model=\"glm-4\",  # \n    messages=[\n        {\"role\": \"system\", \"content\": \"\"},\n        {\n            \"role\": \"user\",\n            \"content\": \"\",",
        "detail": "chatglm.test04",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "chatglm.test04",
        "description": "chatglm.test04",
        "peekOfCode": "response = client.chat.completions.create(\n    model=\"glm-4\",  # \n    messages=[\n        {\"role\": \"system\", \"content\": \"\"},\n        {\n            \"role\": \"user\",\n            \"content\": \"\",\n        },\n    ],\n    stream=True,",
        "detail": "chatglm.test04",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "chatglm.test05",
        "description": "chatglm.test05",
        "peekOfCode": "client = ZhipuAI(\n    api_key=\"2544ea141595d420ca10f891833096ac.deW5ICKFVh4JivTN\"\n)  # APIKey\nresponse = client.chat.completions.create(\n    model=\"glm-4\",  # \n    messages=[\n        {\"role\": \"user\", \"content\": \"\"},\n    ],\n    tools=[\n        {",
        "detail": "chatglm.test05",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "chatglm.test05",
        "description": "chatglm.test05",
        "peekOfCode": "response = client.chat.completions.create(\n    model=\"glm-4\",  # \n    messages=[\n        {\"role\": \"user\", \"content\": \"\"},\n    ],\n    tools=[\n        {\n            \"type\": \"retrieval\",\n            \"retrieval\": '''\n                \"knowledge_id\": \"your knowledge id\",",
        "detail": "chatglm.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"ACTIVELOOP_TOKEN\"\n] = \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwNjA4Mjc0NywiZXhwIjoxNzM4MzA5OTM0fQ.eyJpZCI6InlhbmdsaW5sb25nNiJ9.SNbGhqyihTBsiGhk6SOobDtvWwl0mxW-8YIokUGlvZdu2Z4-db4bnMWK_LIexTwZHHQeYmdHNlqE3gulUULTeg\"\n# Please manually enter OpenAI Key\nfrom langchain.document_loaders import TextLoader\n# root_dir = \"../\"\nroot_dir = \"H:\\\\WorkSpaces\\\\temp\\\\develop\\\\car-service-ordercenter\"",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"ACTIVELOOP_TOKEN\"\n] = \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwNjA4Mjc0NywiZXhwIjoxNzM4MzA5OTM0fQ.eyJpZCI6InlhbmdsaW5sb25nNiJ9.SNbGhqyihTBsiGhk6SOobDtvWwl0mxW-8YIokUGlvZdu2Z4-db4bnMWK_LIexTwZHHQeYmdHNlqE3gulUULTeg\"\n# Please manually enter OpenAI Key\nfrom langchain.document_loaders import TextLoader\n# root_dir = \"../\"\nroot_dir = \"H:\\\\WorkSpaces\\\\temp\\\\develop\\\\car-service-ordercenter\"\ndocs = []\nfor dirpath, dirnames, filenames in os.walk(root_dir):",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "] = \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwNjA4Mjc0NywiZXhwIjoxNzM4MzA5OTM0fQ.eyJpZCI6InlhbmdsaW5sb25nNiJ9.SNbGhqyihTBsiGhk6SOobDtvWwl0mxW-8YIokUGlvZdu2Z4-db4bnMWK_LIexTwZHHQeYmdHNlqE3gulUULTeg\"\n# Please manually enter OpenAI Key\nfrom langchain.document_loaders import TextLoader\n# root_dir = \"../\"\nroot_dir = \"H:\\\\WorkSpaces\\\\temp\\\\develop\\\\car-service-ordercenter\"\ndocs = []\nfor dirpath, dirnames, filenames in os.walk(root_dir):\n    print(\"filenames\", filenames)\n    for file in filenames:\n        # if file.endswith(\".py\") and \"/.venv/\" not in dirpath:",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "root_dir",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "root_dir = \"H:\\\\WorkSpaces\\\\temp\\\\develop\\\\car-service-ordercenter\"\ndocs = []\nfor dirpath, dirnames, filenames in os.walk(root_dir):\n    print(\"filenames\", filenames)\n    for file in filenames:\n        # if file.endswith(\".py\") and \"/.venv/\" not in dirpath:\n        if (\n            file.endswith(\".java\")\n            or file.endswith(\".xml\")\n            or file.endswith(\".properties\")",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "docs",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "docs = []\nfor dirpath, dirnames, filenames in os.walk(root_dir):\n    print(\"filenames\", filenames)\n    for file in filenames:\n        # if file.endswith(\".py\") and \"/.venv/\" not in dirpath:\n        if (\n            file.endswith(\".java\")\n            or file.endswith(\".xml\")\n            or file.endswith(\".properties\")\n            and \"/.venv/\" not in dirpath",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "text_splitter",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ntexts = text_splitter.split_documents(docs)\nprint(f\"{len(texts)}\")\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nembeddings = OpenAIEmbeddings()\nfrom langchain.vectorstores import DeepLake\ndb = DeepLake.from_documents(\n    # texts, embeddings, dataset_path=f\"hub://yanglinlong6/my-code\"\n    texts,\n    embeddings,",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "texts",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "texts = text_splitter.split_documents(docs)\nprint(f\"{len(texts)}\")\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nembeddings = OpenAIEmbeddings()\nfrom langchain.vectorstores import DeepLake\ndb = DeepLake.from_documents(\n    # texts, embeddings, dataset_path=f\"hub://yanglinlong6/my-code\"\n    texts,\n    embeddings,\n    dataset_path=f\"hub://yanglinlong6/car-service-ordercenter\",",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "embeddings",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "embeddings = OpenAIEmbeddings()\nfrom langchain.vectorstores import DeepLake\ndb = DeepLake.from_documents(\n    # texts, embeddings, dataset_path=f\"hub://yanglinlong6/my-code\"\n    texts,\n    embeddings,\n    dataset_path=f\"hub://yanglinlong6/car-service-ordercenter\",\n)\nprint(db)",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "db",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "db = DeepLake.from_documents(\n    # texts, embeddings, dataset_path=f\"hub://yanglinlong6/my-code\"\n    texts,\n    embeddings,\n    dataset_path=f\"hub://yanglinlong6/car-service-ordercenter\",\n)\nprint(db)",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "filter",
        "kind": 2,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "def filter(x):\n    # filter based on source code\n    if \"something\" in x[\"text\"].data()[\"value\"]:\n        return False\n    # filter based on path e.g. extension\n    metadata = x[\"metadata\"].data()[\"value\"]\n    return \"only_this\" in metadata[\"source\"] or \"also_that\" in metadata[\"source\"]\n### turn on below for custom filtering\n# retriever.search_kwargs['filter'] = filter\nfrom langchain.chains import ConversationalRetrievalChain",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"ACTIVELOOP_TOKEN\"\n] = \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwNjA4Mjc0NywiZXhwIjoxNzM4MzA5OTM0fQ.eyJpZCI6InlhbmdsaW5sb25nNiJ9.SNbGhqyihTBsiGhk6SOobDtvWwl0mxW-8YIokUGlvZdu2Z4-db4bnMWK_LIexTwZHHQeYmdHNlqE3gulUULTeg\"\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import DeepLake\nembeddings = OpenAIEmbeddings()\ndb = DeepLake(",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"ACTIVELOOP_TOKEN\"\n] = \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwNjA4Mjc0NywiZXhwIjoxNzM4MzA5OTM0fQ.eyJpZCI6InlhbmdsaW5sb25nNiJ9.SNbGhqyihTBsiGhk6SOobDtvWwl0mxW-8YIokUGlvZdu2Z4-db4bnMWK_LIexTwZHHQeYmdHNlqE3gulUULTeg\"\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import DeepLake\nembeddings = OpenAIEmbeddings()\ndb = DeepLake(\n    # dataset_path=f\"hub://yanglinlong6/my-code\",\n    dataset_path=f\"hub://yanglinlong6/car-service-ordercenter\",",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "] = \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwNjA4Mjc0NywiZXhwIjoxNzM4MzA5OTM0fQ.eyJpZCI6InlhbmdsaW5sb25nNiJ9.SNbGhqyihTBsiGhk6SOobDtvWwl0mxW-8YIokUGlvZdu2Z4-db4bnMWK_LIexTwZHHQeYmdHNlqE3gulUULTeg\"\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import DeepLake\nembeddings = OpenAIEmbeddings()\ndb = DeepLake(\n    # dataset_path=f\"hub://yanglinlong6/my-code\",\n    dataset_path=f\"hub://yanglinlong6/car-service-ordercenter\",\n    read_only=True,\n    embedding_function=embeddings,\n)",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "embeddings",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "embeddings = OpenAIEmbeddings()\ndb = DeepLake(\n    # dataset_path=f\"hub://yanglinlong6/my-code\",\n    dataset_path=f\"hub://yanglinlong6/car-service-ordercenter\",\n    read_only=True,\n    embedding_function=embeddings,\n)\nprint(\"db\", db)\n# Dataset(\n#     path=\"hub://yanglinlong6/langchain-code\",",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "db",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "db = DeepLake(\n    # dataset_path=f\"hub://yanglinlong6/my-code\",\n    dataset_path=f\"hub://yanglinlong6/car-service-ordercenter\",\n    read_only=True,\n    embedding_function=embeddings,\n)\nprint(\"db\", db)\n# Dataset(\n#     path=\"hub://yanglinlong6/langchain-code\",\n#     read_only=True,",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "retriever",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "retriever = db.as_retriever()\nretriever.search_kwargs[\"distance_metric\"] = \"cos\"\nretriever.search_kwargs[\"fetch_k\"] = 20\nretriever.search_kwargs[\"maximal_marginal_relevance\"] = True\nretriever.search_kwargs[\"k\"] = 20\ndef filter(x):\n    # filter based on source code\n    if \"something\" in x[\"text\"].data()[\"value\"]:\n        return False\n    # filter based on path e.g. extension",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "retriever.search_kwargs[\"distance_metric\"]",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "retriever.search_kwargs[\"distance_metric\"] = \"cos\"\nretriever.search_kwargs[\"fetch_k\"] = 20\nretriever.search_kwargs[\"maximal_marginal_relevance\"] = True\nretriever.search_kwargs[\"k\"] = 20\ndef filter(x):\n    # filter based on source code\n    if \"something\" in x[\"text\"].data()[\"value\"]:\n        return False\n    # filter based on path e.g. extension\n    metadata = x[\"metadata\"].data()[\"value\"]",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "retriever.search_kwargs[\"fetch_k\"]",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "retriever.search_kwargs[\"fetch_k\"] = 20\nretriever.search_kwargs[\"maximal_marginal_relevance\"] = True\nretriever.search_kwargs[\"k\"] = 20\ndef filter(x):\n    # filter based on source code\n    if \"something\" in x[\"text\"].data()[\"value\"]:\n        return False\n    # filter based on path e.g. extension\n    metadata = x[\"metadata\"].data()[\"value\"]\n    return \"only_this\" in metadata[\"source\"] or \"also_that\" in metadata[\"source\"]",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "retriever.search_kwargs[\"maximal_marginal_relevance\"]",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "retriever.search_kwargs[\"maximal_marginal_relevance\"] = True\nretriever.search_kwargs[\"k\"] = 20\ndef filter(x):\n    # filter based on source code\n    if \"something\" in x[\"text\"].data()[\"value\"]:\n        return False\n    # filter based on path e.g. extension\n    metadata = x[\"metadata\"].data()[\"value\"]\n    return \"only_this\" in metadata[\"source\"] or \"also_that\" in metadata[\"source\"]\n### turn on below for custom filtering",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "retriever.search_kwargs[\"k\"]",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "retriever.search_kwargs[\"k\"] = 20\ndef filter(x):\n    # filter based on source code\n    if \"something\" in x[\"text\"].data()[\"value\"]:\n        return False\n    # filter based on path e.g. extension\n    metadata = x[\"metadata\"].data()[\"value\"]\n    return \"only_this\" in metadata[\"source\"] or \"also_that\" in metadata[\"source\"]\n### turn on below for custom filtering\n# retriever.search_kwargs['filter'] = filter",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "model = ChatOpenAI(model_name=\"gpt-4\")  # 'ada' 'gpt-3.5-turbo' 'gpt-4',\nqa = ConversationalRetrievalChain.from_llm(model, retriever=retriever)\nquestions = [\n    # \"What is the class hierarchy?\",\n    # \"\"\n    # \",100\"\n    \"CarOrderApiController,150\"\n    # \"What classes are derived from the Chain class?\",\n    # \"What classes and functions in the ./langchain/utilities/ forlder are not covered by unit tests?\",\n    # \"What one improvement do you propose in code in relation to the class herarchy for the Chain class?\",",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "qa",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "qa = ConversationalRetrievalChain.from_llm(model, retriever=retriever)\nquestions = [\n    # \"What is the class hierarchy?\",\n    # \"\"\n    # \",100\"\n    \"CarOrderApiController,150\"\n    # \"What classes are derived from the Chain class?\",\n    # \"What classes and functions in the ./langchain/utilities/ forlder are not covered by unit tests?\",\n    # \"What one improvement do you propose in code in relation to the class herarchy for the Chain class?\",\n]",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "questions",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "questions = [\n    # \"What is the class hierarchy?\",\n    # \"\"\n    # \",100\"\n    \"CarOrderApiController,150\"\n    # \"What classes are derived from the Chain class?\",\n    # \"What classes and functions in the ./langchain/utilities/ forlder are not covered by unit tests?\",\n    # \"What one improvement do you propose in code in relation to the class herarchy for the Chain class?\",\n]\nchat_history = [(\"\", \" \")]",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "chat_history",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "chat_history = [(\"\", \" \")]\nfor question in questions:\n    result = qa({\"question\": question, \"chat_history\": chat_history})\n    # result = qa({\"question\": question})\n    chat_history.append((question, result[\"answer\"]))\n    print(f\"-> \\*\\*Question\\*\\*: {question} \")\n    print(f\"\\*\\*Answer\\*\\*: {result['answer']} \")",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "get_files_in_directory",
        "kind": 2,
        "importPath": "deeplake.test03",
        "description": "deeplake.test03",
        "peekOfCode": "def get_files_in_directory(directory):\n    file_paths = []\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            file_path = os.path.join(root, file)\n            file_paths.append(file_path)\n    return file_paths\ndirectory_path = \"H:\\\\WorkSpaces\\\\temp\\\\develop\\\\car-service-ordercenter\"\nfiles = get_files_in_directory(directory_path)\n# ",
        "detail": "deeplake.test03",
        "documentation": {}
    },
    {
        "label": "directory_path",
        "kind": 5,
        "importPath": "deeplake.test03",
        "description": "deeplake.test03",
        "peekOfCode": "directory_path = \"H:\\\\WorkSpaces\\\\temp\\\\develop\\\\car-service-ordercenter\"\nfiles = get_files_in_directory(directory_path)\n# \nfor file in files:\n    print(file)",
        "detail": "deeplake.test03",
        "documentation": {}
    },
    {
        "label": "files",
        "kind": 5,
        "importPath": "deeplake.test03",
        "description": "deeplake.test03",
        "peekOfCode": "files = get_files_in_directory(directory_path)\n# \nfor file in files:\n    print(file)",
        "detail": "deeplake.test03",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "gtts.test01",
        "description": "gtts.test01",
        "peekOfCode": "text = \",\"\nlang = 'zh-cn'\ntts = gTTS(text=text, lang=lang, slow=False)\ntts.save(\"hello.mp3\")\nos.system(\"start hello.mp3\")  #  Windows ",
        "detail": "gtts.test01",
        "documentation": {}
    },
    {
        "label": "lang",
        "kind": 5,
        "importPath": "gtts.test01",
        "description": "gtts.test01",
        "peekOfCode": "lang = 'zh-cn'\ntts = gTTS(text=text, lang=lang, slow=False)\ntts.save(\"hello.mp3\")\nos.system(\"start hello.mp3\")  #  Windows ",
        "detail": "gtts.test01",
        "documentation": {}
    },
    {
        "label": "tts",
        "kind": 5,
        "importPath": "gtts.test01",
        "description": "gtts.test01",
        "peekOfCode": "tts = gTTS(text=text, lang=lang, slow=False)\ntts.save(\"hello.mp3\")\nos.system(\"start hello.mp3\")  #  Windows ",
        "detail": "gtts.test01",
        "documentation": {}
    },
    {
        "label": "engine",
        "kind": 5,
        "importPath": "gtts.test02",
        "description": "gtts.test02",
        "peekOfCode": "engine = pyttsx3.init()\nengine.say(\"Hello, world!\")\nengine.runAndWait()",
        "detail": "gtts.test02",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "gtts.test03",
        "description": "gtts.test03",
        "peekOfCode": "text = \"Hello, world!\"\ncommand = f'echo \"{text}\" | festival --tts'\nprocess = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)\noutput, error = process.communicate()\nwith open('hello.wav', 'wb') as f:\n    f.write(output)\nsubprocess.call(['afplay', 'hello.wav'])  #  macOS ",
        "detail": "gtts.test03",
        "documentation": {}
    },
    {
        "label": "command",
        "kind": 5,
        "importPath": "gtts.test03",
        "description": "gtts.test03",
        "peekOfCode": "command = f'echo \"{text}\" | festival --tts'\nprocess = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)\noutput, error = process.communicate()\nwith open('hello.wav', 'wb') as f:\n    f.write(output)\nsubprocess.call(['afplay', 'hello.wav'])  #  macOS ",
        "detail": "gtts.test03",
        "documentation": {}
    },
    {
        "label": "process",
        "kind": 5,
        "importPath": "gtts.test03",
        "description": "gtts.test03",
        "peekOfCode": "process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)\noutput, error = process.communicate()\nwith open('hello.wav', 'wb') as f:\n    f.write(output)\nsubprocess.call(['afplay', 'hello.wav'])  #  macOS ",
        "detail": "gtts.test03",
        "documentation": {}
    },
    {
        "label": "engine",
        "kind": 5,
        "importPath": "gtts.test04",
        "description": "gtts.test04",
        "peekOfCode": "engine = pyttsx3.init()\nengine.setProperty('rate', 120)  # \nengine.setProperty('volume', 1.0)  # \ntext = \"Hello, world!\"\nengine.say(text)\nengine.runAndWait()",
        "detail": "gtts.test04",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "gtts.test04",
        "description": "gtts.test04",
        "peekOfCode": "text = \"Hello, world!\"\nengine.say(text)\nengine.runAndWait()",
        "detail": "gtts.test04",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "gtts.test05",
        "description": "gtts.test05",
        "peekOfCode": "text = \"Hello, world!\"\nurl = \"https://api.us-south.text-to-speech.watson.cloud.ibm.com/instances/<instance_id>/v1/synthesize\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer <access_token>\"\n}\ndata = {\"text\": text, \"voice\": \"en-US_AllisonVoice\", \"accept\": \"audio/wav\"}\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nif response.status_code == 200:\n    with open(\"output.wav\", \"wb\") as f:",
        "detail": "gtts.test05",
        "documentation": {}
    },
    {
        "label": "url",
        "kind": 5,
        "importPath": "gtts.test05",
        "description": "gtts.test05",
        "peekOfCode": "url = \"https://api.us-south.text-to-speech.watson.cloud.ibm.com/instances/<instance_id>/v1/synthesize\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer <access_token>\"\n}\ndata = {\"text\": text, \"voice\": \"en-US_AllisonVoice\", \"accept\": \"audio/wav\"}\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nif response.status_code == 200:\n    with open(\"output.wav\", \"wb\") as f:\n        f.write(response.content)",
        "detail": "gtts.test05",
        "documentation": {}
    },
    {
        "label": "headers",
        "kind": 5,
        "importPath": "gtts.test05",
        "description": "gtts.test05",
        "peekOfCode": "headers = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer <access_token>\"\n}\ndata = {\"text\": text, \"voice\": \"en-US_AllisonVoice\", \"accept\": \"audio/wav\"}\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nif response.status_code == 200:\n    with open(\"output.wav\", \"wb\") as f:\n        f.write(response.content)\n    # ",
        "detail": "gtts.test05",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "gtts.test05",
        "description": "gtts.test05",
        "peekOfCode": "data = {\"text\": text, \"voice\": \"en-US_AllisonVoice\", \"accept\": \"audio/wav\"}\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nif response.status_code == 200:\n    with open(\"output.wav\", \"wb\") as f:\n        f.write(response.content)\n    # \n    subprocess.run(['aplay', 'output.wav'])\nelse:\n    print(\"Error:\", response.text)",
        "detail": "gtts.test05",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "gtts.test05",
        "description": "gtts.test05",
        "peekOfCode": "response = requests.post(url, headers=headers, data=json.dumps(data))\nif response.status_code == 200:\n    with open(\"output.wav\", \"wb\") as f:\n        f.write(response.content)\n    # \n    subprocess.run(['aplay', 'output.wav'])\nelse:\n    print(\"Error:\", response.text)",
        "detail": "gtts.test05",
        "documentation": {}
    },
    {
        "label": "FileDriver",
        "kind": 6,
        "importPath": "gtts.test06",
        "description": "gtts.test06",
        "peekOfCode": "class FileDriver:\n    def __init__(self, voice=None, rate=None, volume=None, output_file=None):\n        super().__init__()\n        self._voice = voice\n        self._rate = rate\n        self._volume = volume\n        self._output_file = output_file\n    def connect(self):\n        engine = pyttsx3.init()\n        engine.setProperty('voice', self._voice)",
        "detail": "gtts.test06",
        "documentation": {}
    },
    {
        "label": "text_to_speech",
        "kind": 2,
        "importPath": "gtts.test06",
        "description": "gtts.test06",
        "peekOfCode": "def text_to_speech(text, voice=None, rate=None, volume=None, output_file=None):\n    with io.BytesIO() as output_stream:\n        driver = FileDriver(voice, rate, volume, output_stream)\n        engine = driver.connect()\n        engine.say(text)\n        engine.runAndWait()\n        engine.stop()\n        engine.disconnect({\"topic\": 1})\n        if output_file:\n            with open(output_file, 'wb') as f:",
        "detail": "gtts.test06",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "gtts.test06",
        "description": "gtts.test06",
        "peekOfCode": "text = \"Hello, world!  \"\noutput_file = \"hello_world.wav\"\ntext_to_speech(text, output_file=output_file)",
        "detail": "gtts.test06",
        "documentation": {}
    },
    {
        "label": "output_file",
        "kind": 5,
        "importPath": "gtts.test06",
        "description": "gtts.test06",
        "peekOfCode": "output_file = \"hello_world.wav\"\ntext_to_speech(text, output_file=output_file)",
        "detail": "gtts.test06",
        "documentation": {}
    },
    {
        "label": "AUDIO_FILE",
        "kind": 5,
        "importPath": "gtts.test07",
        "description": "gtts.test07",
        "peekOfCode": "AUDIO_FILE = path.join(path.dirname(path.realpath(__file__)), \"test.wav\")\n# AUDIO_FILE = path.join(path.dirname(path.realpath(__file__)), \"french.aiff\")\n# AUDIO_FILE = path.join(path.dirname(path.realpath(__file__)), \"chinese.flac\")\n# use the audio file as the audio source\nr = sr.Recognizer()\nwith sr.AudioFile(AUDIO_FILE) as source:\n    audio = r.record(source)  # read the entire audio file\n# recognize speech using Sphinx language=\"zh-CN\",\ntry:\n    print(\"Sphinx thinks you said \")",
        "detail": "gtts.test07",
        "documentation": {}
    },
    {
        "label": "r",
        "kind": 5,
        "importPath": "gtts.test07",
        "description": "gtts.test07",
        "peekOfCode": "r = sr.Recognizer()\nwith sr.AudioFile(AUDIO_FILE) as source:\n    audio = r.record(source)  # read the entire audio file\n# recognize speech using Sphinx language=\"zh-CN\",\ntry:\n    print(\"Sphinx thinks you said \")\n    print(r.recognize_sphinx(audio))\n    # pprint(r.recognize_google(audio, show_all=True))\n    audio_info = r.recognize_google(audio, language=\"zh-CN\", show_all=True)\n    # pprint(audio_info)",
        "detail": "gtts.test07",
        "documentation": {}
    },
    {
        "label": "tokenize_function",
        "kind": 2,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "def tokenize_function(examples):\n    return tokenizer(\n        examples[\"sentence1\"],\n        examples[\"sentence2\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=128,\n    )\ntrain_dataset = train_dataset.map(tokenize_function)\nval_dataset = val_dataset.map(tokenize_function)",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "model = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "lora_config",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "lora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)\nval_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "model = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)\nval_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()\n    .select(range(100))",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "train_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)\nval_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()\n    .select(range(100))\n)",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "val_dataset",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "val_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()\n    .select(range(100))\n)\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"sentence1\"],\n        examples[\"sentence2\"],\n        padding=\"max_length\",",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "train_dataset = train_dataset.map(tokenize_function)\nval_dataset = val_dataset.map(tokenize_function)\narguments = TrainingArguments(\n    output_dir=\"./outputs\",\n    per_device_train_batch_size=16,\n)\ntrainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "val_dataset",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "val_dataset = val_dataset.map(tokenize_function)\narguments = TrainingArguments(\n    output_dir=\"./outputs\",\n    per_device_train_batch_size=16,\n)\ntrainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "arguments",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "arguments = TrainingArguments(\n    output_dir=\"./outputs\",\n    per_device_train_batch_size=16,\n)\ntrainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=default_data_collator,",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "trainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=default_data_collator,\n)\ntrainer.train()",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test04",
        "description": "huggingface.test04",
        "peekOfCode": "model = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))",
        "detail": "huggingface.test04",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "huggingface.test04",
        "description": "huggingface.test04",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))",
        "detail": "huggingface.test04",
        "documentation": {}
    },
    {
        "label": "lora_config",
        "kind": 5,
        "importPath": "huggingface.test04",
        "description": "huggingface.test04",
        "peekOfCode": "lora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))",
        "detail": "huggingface.test04",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test04",
        "description": "huggingface.test04",
        "peekOfCode": "model = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))",
        "detail": "huggingface.test04",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "train_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\neval_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\nmodel_id = \"damo/nlp_structbert_sentence-similarity_chinese-base\"\n# modelcfg\ncfg = read_config(model_id)\n# \ncfg.train.max_epochs = 5\ncfg.preprocessor.train[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.preprocessor.val[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.train.work_dir = \"/tmp\"",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "eval_dataset",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "eval_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\nmodel_id = \"damo/nlp_structbert_sentence-similarity_chinese-base\"\n# modelcfg\ncfg = read_config(model_id)\n# \ncfg.train.max_epochs = 5\ncfg.preprocessor.train[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.preprocessor.val[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.train.work_dir = \"/tmp\"\ncfg_file = os.path.join(\"/tmp\", \"config.json\")",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "model_id",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "model_id = \"damo/nlp_structbert_sentence-similarity_chinese-base\"\n# modelcfg\ncfg = read_config(model_id)\n# \ncfg.train.max_epochs = 5\ncfg.preprocessor.train[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.preprocessor.val[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.train.work_dir = \"/tmp\"\ncfg_file = os.path.join(\"/tmp\", \"config.json\")\n# trainer",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "cfg",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "cfg = read_config(model_id)\n# \ncfg.train.max_epochs = 5\ncfg.preprocessor.train[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.preprocessor.val[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.train.work_dir = \"/tmp\"\ncfg_file = os.path.join(\"/tmp\", \"config.json\")\n# trainer\ncfg.dump(cfg_file)\nkwargs = dict(",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "cfg.train.max_epochs",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "cfg.train.max_epochs = 5\ncfg.preprocessor.train[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.preprocessor.val[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.train.work_dir = \"/tmp\"\ncfg_file = os.path.join(\"/tmp\", \"config.json\")\n# trainer\ncfg.dump(cfg_file)\nkwargs = dict(\n    model=model_id,\n    train_dataset=train_dataset,",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "cfg.preprocessor.train[\"label2id\"]",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "cfg.preprocessor.train[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.preprocessor.val[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.train.work_dir = \"/tmp\"\ncfg_file = os.path.join(\"/tmp\", \"config.json\")\n# trainer\ncfg.dump(cfg_file)\nkwargs = dict(\n    model=model_id,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "cfg.preprocessor.val[\"label2id\"]",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "cfg.preprocessor.val[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.train.work_dir = \"/tmp\"\ncfg_file = os.path.join(\"/tmp\", \"config.json\")\n# trainer\ncfg.dump(cfg_file)\nkwargs = dict(\n    model=model_id,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    cfg_file=cfg_file,",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "cfg.train.work_dir",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "cfg.train.work_dir = \"/tmp\"\ncfg_file = os.path.join(\"/tmp\", \"config.json\")\n# trainer\ncfg.dump(cfg_file)\nkwargs = dict(\n    model=model_id,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    cfg_file=cfg_file,\n)",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "cfg_file",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "cfg_file = os.path.join(\"/tmp\", \"config.json\")\n# trainer\ncfg.dump(cfg_file)\nkwargs = dict(\n    model=model_id,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    cfg_file=cfg_file,\n)\ntrainer = build_trainer(default_args=kwargs)",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "kwargs",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "kwargs = dict(\n    model=model_id,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    cfg_file=cfg_file,\n)\ntrainer = build_trainer(default_args=kwargs)\ntrainer.train()",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "trainer = build_trainer(default_args=kwargs)\ntrainer.train()",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "huggingface.test06",
        "description": "huggingface.test06",
        "peekOfCode": "def main():\n    # \n    from datasets import load_dataset\n    dataset_dict = load_dataset(\"luozhouyang/dureader\", \"robust\")\n    def concat_answer_context(dataset):\n        dataset[\"src_txt\"] = (\n            dataset[\"answers\"][\"text\"][0] + \"[SEP]\" + dataset[\"context\"]\n        )\n        return dataset\n    train_dataset = dataset_dict[\"train\"].map(concat_answer_context)",
        "detail": "huggingface.test06",
        "documentation": {}
    },
    {
        "label": "a",
        "kind": 5,
        "importPath": "huggingface.test07",
        "description": "huggingface.test07",
        "peekOfCode": "a = torch.tensor([1.0], requires_grad=True)\nb = torch.tensor([2.0], requires_grad=True)\nc = a * b\n# \nc.backward()\nprint(a.grad, b.grad)\n# tensor([2.]) tensor([1.])",
        "detail": "huggingface.test07",
        "documentation": {}
    },
    {
        "label": "b",
        "kind": 5,
        "importPath": "huggingface.test07",
        "description": "huggingface.test07",
        "peekOfCode": "b = torch.tensor([2.0], requires_grad=True)\nc = a * b\n# \nc.backward()\nprint(a.grad, b.grad)\n# tensor([2.]) tensor([1.])",
        "detail": "huggingface.test07",
        "documentation": {}
    },
    {
        "label": "c",
        "kind": 5,
        "importPath": "huggingface.test07",
        "description": "huggingface.test07",
        "peekOfCode": "c = a * b\n# \nc.backward()\nprint(a.grad, b.grad)\n# tensor([2.]) tensor([1.])",
        "detail": "huggingface.test07",
        "documentation": {}
    },
    {
        "label": "SubModule",
        "kind": 6,
        "importPath": "huggingface.test08",
        "description": "huggingface.test08",
        "peekOfCode": "class SubModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        # configLinear\n        # self.a = Linear(config.hidden_size, config.hidden_size)\n        self.a = Linear(4, 4)\nclass Module(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.sub = SubModule()",
        "detail": "huggingface.test08",
        "documentation": {}
    },
    {
        "label": "Module",
        "kind": 6,
        "importPath": "huggingface.test08",
        "description": "huggingface.test08",
        "peekOfCode": "class Module(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.sub = SubModule()\nmodule = Module()\nstate_dict = module.state_dict()  # key value\n# OrderedDict([('sub.a.weight', tensor([[-0.4148, -0.2303, -0.3650, -0.4019],\n#        [-0.2495,  0.1113,  0.3846,  0.3645],\n#        [ 0.0395, -0.0490, -0.1738,  0.0820],\n#        [ 0.4187,  0.4697, -0.4100, -0.4685]])), ('sub.a.bias', tensor([ 0.4756, -0.4298, -0.4380,  0.3344]))])",
        "detail": "huggingface.test08",
        "documentation": {}
    },
    {
        "label": "module",
        "kind": 5,
        "importPath": "huggingface.test08",
        "description": "huggingface.test08",
        "peekOfCode": "module = Module()\nstate_dict = module.state_dict()  # key value\n# OrderedDict([('sub.a.weight', tensor([[-0.4148, -0.2303, -0.3650, -0.4019],\n#        [-0.2495,  0.1113,  0.3846,  0.3645],\n#        [ 0.0395, -0.0490, -0.1738,  0.0820],\n#        [ 0.4187,  0.4697, -0.4100, -0.4685]])), ('sub.a.bias', tensor([ 0.4756, -0.4298, -0.4380,  0.3344]))])\n# SubModule\nsetattr(module, \"sub\", Linear(4, 4))\n# \n# ",
        "detail": "huggingface.test08",
        "documentation": {}
    },
    {
        "label": "state_dict",
        "kind": 5,
        "importPath": "huggingface.test08",
        "description": "huggingface.test08",
        "peekOfCode": "state_dict = module.state_dict()  # key value\n# OrderedDict([('sub.a.weight', tensor([[-0.4148, -0.2303, -0.3650, -0.4019],\n#        [-0.2495,  0.1113,  0.3846,  0.3645],\n#        [ 0.0395, -0.0490, -0.1738,  0.0820],\n#        [ 0.4187,  0.4697, -0.4100, -0.4685]])), ('sub.a.bias', tensor([ 0.4756, -0.4298, -0.4380,  0.3344]))])\n# SubModule\nsetattr(module, \"sub\", Linear(4, 4))\n# \n# ",
        "detail": "huggingface.test08",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test09",
        "description": "huggingface.test09",
        "peekOfCode": "model = AutoModelForCausalLM.from_pretrained(\n    \"qwen/Qwen-1_8B-Chat\", trust_remote_code=True\n)\nmodel.to(0)\n# model.to('cuda:0') \na = torch.tensor([1.0])\na = a.to(0)\n# model.totorch.nn.Module()in-place()\n# tensorin-place",
        "detail": "huggingface.test09",
        "documentation": {}
    },
    {
        "label": "a",
        "kind": 5,
        "importPath": "huggingface.test09",
        "description": "huggingface.test09",
        "peekOfCode": "a = torch.tensor([1.0])\na = a.to(0)\n# model.totorch.nn.Module()in-place()\n# tensorin-place",
        "detail": "huggingface.test09",
        "documentation": {}
    },
    {
        "label": "a",
        "kind": 5,
        "importPath": "huggingface.test09",
        "description": "huggingface.test09",
        "peekOfCode": "a = a.to(0)\n# model.totorch.nn.Module()in-place()\n# tensorin-place",
        "detail": "huggingface.test09",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "device = torch.device(\"cpu\")\n#  device  GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "tokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "model = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "text = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "encoding",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "encoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "outputs",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "outputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "logits",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "logits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "masked_tokens_predictions",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "masked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "MyModule",
        "kind": 6,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "class MyModule(torch.nn.Module):\n    def __init__(self, n_classes=2):\n        # \n        super().__init__()\n        # linearrelu\n        self.linear = torch.nn.Linear(16, n_classes)\n        self.relu = torch.nn.ReLU()\n    def forward(self, tensor, label):\n        # \n        output = {\"logits\": self.relu(self.linear(tensor))}",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "MyDataset",
        "kind": 6,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "class MyDataset(Dataset):\n    # 5\n    def __len__(self):\n        return 5\n    # index\n    def __getitem__(self, index):\n        return {\"tensor\": torch.rand(16), \"label\": torch.tensor(1)}\n# \nmodel = MyModule()\n# ",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "seed = 42\n# \ntorch.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\n# cudacublascudnn\n# CUDA\n# \nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nos.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_LAUNCH_BLOCKING\"]",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nos.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\ntorch.use_deterministic_algorithms(True)\n# Enable CUDNN deterministic mode\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n# torchtorch.nn.Module\nclass MyModule(torch.nn.Module):\n    def __init__(self, n_classes=2):\n        # ",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\ntorch.use_deterministic_algorithms(True)\n# Enable CUDNN deterministic mode\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n# torchtorch.nn.Module\nclass MyModule(torch.nn.Module):\n    def __init__(self, n_classes=2):\n        # \n        super().__init__()",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "torch.backends.cudnn.deterministic",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "torch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n# torchtorch.nn.Module\nclass MyModule(torch.nn.Module):\n    def __init__(self, n_classes=2):\n        # \n        super().__init__()\n        # linearrelu\n        self.linear = torch.nn.Linear(16, n_classes)\n        self.relu = torch.nn.ReLU()",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "torch.backends.cudnn.benchmark",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "torch.backends.cudnn.benchmark = False\n# torchtorch.nn.Module\nclass MyModule(torch.nn.Module):\n    def __init__(self, n_classes=2):\n        # \n        super().__init__()\n        # linearrelu\n        self.linear = torch.nn.Linear(16, n_classes)\n        self.relu = torch.nn.ReLU()\n    def forward(self, tensor, label):",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "model = MyModule()\n# \ndataset = MyDataset()\n# dataloader dataloaderbatch_sizebatch_size\n# collate_fnbatchpadding\ndataloader = DataLoader(dataset, batch_size=4, collate_fn=default_collate)\n# optimizerparameters\n# lr\noptimizer = AdamW(model.parameters(), lr=5e-4)\n# lr_scheduler learning_rate",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "dataset",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "dataset = MyDataset()\n# dataloader dataloaderbatch_sizebatch_size\n# collate_fnbatchpadding\ndataloader = DataLoader(dataset, batch_size=4, collate_fn=default_collate)\n# optimizerparameters\n# lr\noptimizer = AdamW(model.parameters(), lr=5e-4)\n# lr_scheduler learning_rate\nlr_scheduler = StepLR(optimizer, 2)\n# 3epoch",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "dataloader",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "dataloader = DataLoader(dataset, batch_size=4, collate_fn=default_collate)\n# optimizerparameters\n# lr\noptimizer = AdamW(model.parameters(), lr=5e-4)\n# lr_scheduler learning_rate\nlr_scheduler = StepLR(optimizer, 2)\n# 3epoch\nfor i in range(3):\n    # dataloader\n    for batch in dataloader:",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "optimizer",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "optimizer = AdamW(model.parameters(), lr=5e-4)\n# lr_scheduler learning_rate\nlr_scheduler = StepLR(optimizer, 2)\n# 3epoch\nfor i in range(3):\n    # dataloader\n    for batch in dataloader:\n        # forwardloss\n        output = model(**batch)\n        # backwardparameters",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "lr_scheduler",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "lr_scheduler = StepLR(optimizer, 2)\n# 3epoch\nfor i in range(3):\n    # dataloader\n    for batch in dataloader:\n        # forwardloss\n        output = model(**batch)\n        # backwardparameters\n        output[\"loss\"].backward()\n        # modellineargrad",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "huggingface.test11",
        "description": "huggingface.test11",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\nmodel = (\n    AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True).half().cuda()\n)\nmodel = model.eval()\nresponse, history = model.chat(tokenizer, \"\", history=[])\nprint(response)\n# ! ChatGLM-6B,,\nresponse, history = model.chat(tokenizer, \"\", history=history)\nprint(response)",
        "detail": "huggingface.test11",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test11",
        "description": "huggingface.test11",
        "peekOfCode": "model = (\n    AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True).half().cuda()\n)\nmodel = model.eval()\nresponse, history = model.chat(tokenizer, \"\", history=[])\nprint(response)\n# ! ChatGLM-6B,,\nresponse, history = model.chat(tokenizer, \"\", history=history)\nprint(response)",
        "detail": "huggingface.test11",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test11",
        "description": "huggingface.test11",
        "peekOfCode": "model = model.eval()\nresponse, history = model.chat(tokenizer, \"\", history=[])\nprint(response)\n# ! ChatGLM-6B,,\nresponse, history = model.chat(tokenizer, \"\", history=history)\nprint(response)",
        "detail": "huggingface.test11",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom transformers import PerceiverTokenizer, PerceiverForMaskedLM\n#  device  CPU\ndevice = torch.device(\"cpu\")",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom transformers import PerceiverTokenizer, PerceiverForMaskedLM\n#  device  CPU\ndevice = torch.device(\"cpu\")\n#  device  GPU",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom transformers import PerceiverTokenizer, PerceiverForMaskedLM\n#  device  CPU\ndevice = torch.device(\"cpu\")\n#  device  GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom transformers import PerceiverTokenizer, PerceiverForMaskedLM\n#  device  CPU\ndevice = torch.device(\"cpu\")\n#  device  GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom transformers import PerceiverTokenizer, PerceiverForMaskedLM\n#  device  CPU\ndevice = torch.device(\"cpu\")\n#  device  GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom transformers import PerceiverTokenizer, PerceiverForMaskedLM\n#  device  CPU\ndevice = torch.device(\"cpu\")\n#  device  GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom transformers import PerceiverTokenizer, PerceiverForMaskedLM\n#  device  CPU\ndevice = torch.device(\"cpu\")\n#  device  GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "device = torch.device(\"cpu\")\n#  device  GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "tokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "model = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "text = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "encoding",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "encoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "outputs",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "outputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "logits",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "logits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "masked_tokens_predictions",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "masked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\nresult = agent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "llm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\nresult = agent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n)\nprint(result)",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\nresult = agent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n)\nprint(result)",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "agent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\nresult = agent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n)\nprint(result)",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "result = agent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n)\nprint(result)",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "FakeAgent",
        "kind": 6,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "class FakeAgent(BaseSingleActionAgent):\n    \"\"\"Fake Custom Agent.\"\"\"\n    @property\n    def input_keys(self):\n        return [\"input\"]\n    def plan(\n        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\n    ) -> Union[AgentAction, AgentFinish]:\n        \"\"\"Given input, decided what to do.\n        Args:",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import Tool, AgentExecutor, BaseSingleActionAgent\nfrom langchain import OpenAI, SerpAPIWrapper\nsearch = SerpAPIWrapper()\ntools = [",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import Tool, AgentExecutor, BaseSingleActionAgent\nfrom langchain import OpenAI, SerpAPIWrapper\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import Tool, AgentExecutor, BaseSingleActionAgent\nfrom langchain import OpenAI, SerpAPIWrapper\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n        return_direct=True,",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "search = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n        return_direct=True,\n    )\n]\nfrom typing import List, Tuple, Any, Union",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "tools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n        return_direct=True,\n    )\n]\nfrom typing import List, Tuple, Any, Union\nfrom langchain.schema import AgentAction, AgentFinish",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "agent = FakeAgent()\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nagent_executor.run(\"How many people live in canada as of 2023?\")",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "agent_executor",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "agent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nagent_executor.run(\"How many people live in canada as of 2023?\")",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "fake_func",
        "kind": 2,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "def fake_func(inp: str) -> str:\n    return \"foo\"\nfake_tools = [\n    Tool(\n        name=f\"foo-{i}\",\n        func=fake_func,\n        description=f\"a silly function that you can use to get more information about the number {i}\",\n    )\n    for i in range(99)\n]",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "get_tools",
        "kind": 2,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "def get_tools(query):\n    docs = retriever.get_relevant_documents(query)\n    return [ALL_TOOLS[d.metadata[\"index\"]] for d in docs]\nprint(get_tools(\"whats the weather?\"))",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nimport re\nfrom typing import List, Union\nfrom langchain import LLMChain, OpenAI, SerpAPIWrapper\nfrom langchain.agents import (AgentExecutor, AgentOutputParser,",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nimport re\nfrom typing import List, Union\nfrom langchain import LLMChain, OpenAI, SerpAPIWrapper\nfrom langchain.agents import (AgentExecutor, AgentOutputParser,\n                              LLMSingleActionAgent, Tool)\nfrom langchain.prompts import StringPromptTemplate",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nimport re\nfrom typing import List, Union\nfrom langchain import LLMChain, OpenAI, SerpAPIWrapper\nfrom langchain.agents import (AgentExecutor, AgentOutputParser,\n                              LLMSingleActionAgent, Tool)\nfrom langchain.prompts import StringPromptTemplate\nfrom langchain.schema import AgentAction, AgentFinish\n# Define which tools the agent can use to answer user queries\nsearch = SerpAPIWrapper()",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "search = SerpAPIWrapper()\nsearch_tool = Tool(\n    name=\"Search\",\n    func=search.run,\n    description=\"useful for when you need to answer questions about current events\",\n)\ndef fake_func(inp: str) -> str:\n    return \"foo\"\nfake_tools = [\n    Tool(",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "search_tool",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "search_tool = Tool(\n    name=\"Search\",\n    func=search.run,\n    description=\"useful for when you need to answer questions about current events\",\n)\ndef fake_func(inp: str) -> str:\n    return \"foo\"\nfake_tools = [\n    Tool(\n        name=f\"foo-{i}\",",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "fake_tools",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "fake_tools = [\n    Tool(\n        name=f\"foo-{i}\",\n        func=fake_func,\n        description=f\"a silly function that you can use to get more information about the number {i}\",\n    )\n    for i in range(99)\n]\nALL_TOOLS = [search_tool] + fake_tools\nfrom langchain.embeddings import OpenAIEmbeddings",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "ALL_TOOLS",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "ALL_TOOLS = [search_tool] + fake_tools\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.schema import Document\nfrom langchain.vectorstores import FAISS\ndocs = [\n    Document(page_content=t.description, metadata={\"index\": i})\n    for i, t in enumerate(ALL_TOOLS)\n]\nvector_store = FAISS.from_documents(docs, OpenAIEmbeddings())\nretriever = vector_store.as_retriever()",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "docs",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "docs = [\n    Document(page_content=t.description, metadata={\"index\": i})\n    for i, t in enumerate(ALL_TOOLS)\n]\nvector_store = FAISS.from_documents(docs, OpenAIEmbeddings())\nretriever = vector_store.as_retriever()\ndef get_tools(query):\n    docs = retriever.get_relevant_documents(query)\n    return [ALL_TOOLS[d.metadata[\"index\"]] for d in docs]\nprint(get_tools(\"whats the weather?\"))",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "vector_store",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "vector_store = FAISS.from_documents(docs, OpenAIEmbeddings())\nretriever = vector_store.as_retriever()\ndef get_tools(query):\n    docs = retriever.get_relevant_documents(query)\n    return [ALL_TOOLS[d.metadata[\"index\"]] for d in docs]\nprint(get_tools(\"whats the weather?\"))",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "retriever",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "retriever = vector_store.as_retriever()\ndef get_tools(query):\n    docs = retriever.get_relevant_documents(query)\n    return [ALL_TOOLS[d.metadata[\"index\"]] for d in docs]\nprint(get_tools(\"whats the weather?\"))",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import ZeroShotAgent, Tool, AgentExecutor\nfrom langchain import OpenAI, SerpAPIWrapper, LLMChain\nsearch = SerpAPIWrapper()\ntools = [",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import ZeroShotAgent, Tool, AgentExecutor\nfrom langchain import OpenAI, SerpAPIWrapper, LLMChain\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import ZeroShotAgent, Tool, AgentExecutor\nfrom langchain import OpenAI, SerpAPIWrapper, LLMChain\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "search = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )\n]\nprefix = \"\"\"Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:\"\"\"\nsuffix = \"\"\"Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Args\"",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "tools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )\n]\nprefix = \"\"\"Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:\"\"\"\nsuffix = \"\"\"Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Args\"\nQuestion: {input}",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "prefix",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "prefix = \"\"\"Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:\"\"\"\nsuffix = \"\"\"Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Args\"\nQuestion: {input}\n{agent_scratchpad}\"\"\"\nprompt = ZeroShotAgent.create_prompt(\n    tools, prefix=prefix, suffix=suffix, input_variables=[\"input\", \"agent_scratchpad\"]\n)\nprint(prompt.template)\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\ntool_names = [tool.name for tool in tools]",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "suffix",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "suffix = \"\"\"Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Args\"\nQuestion: {input}\n{agent_scratchpad}\"\"\"\nprompt = ZeroShotAgent.create_prompt(\n    tools, prefix=prefix, suffix=suffix, input_variables=[\"input\", \"agent_scratchpad\"]\n)\nprint(prompt.template)\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\ntool_names = [tool.name for tool in tools]\nagent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "prompt = ZeroShotAgent.create_prompt(\n    tools, prefix=prefix, suffix=suffix, input_variables=[\"input\", \"agent_scratchpad\"]\n)\nprint(prompt.template)\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\ntool_names = [tool.name for tool in tools]\nagent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "llm_chain",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "llm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\ntool_names = [tool.name for tool in tools]\nagent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nprint(agent_executor.run(\"How many people live in canada as of 2023?\"))",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "tool_names",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "tool_names = [tool.name for tool in tools]\nagent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nprint(agent_executor.run(\"How many people live in canada as of 2023?\"))",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "agent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nprint(agent_executor.run(\"How many people live in canada as of 2023?\"))",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "agent_executor",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "agent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nprint(agent_executor.run(\"How many people live in canada as of 2023?\"))",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import LLMChain, OpenAI, SerpAPIWrapper\nfrom langchain.agents import AgentExecutor, Tool, ZeroShotAgent\nsearch = SerpAPIWrapper()\ntools = [",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import LLMChain, OpenAI, SerpAPIWrapper\nfrom langchain.agents import AgentExecutor, Tool, ZeroShotAgent\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import LLMChain, OpenAI, SerpAPIWrapper\nfrom langchain.agents import AgentExecutor, Tool, ZeroShotAgent\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "search = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )\n]\nprefix = \"\"\"Answer the following questions as best you can. You have access to the following tools:\"\"\"\nsuffix = \"\"\"When answering, you MUST speak in the following language: {language}.",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "tools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )\n]\nprefix = \"\"\"Answer the following questions as best you can. You have access to the following tools:\"\"\"\nsuffix = \"\"\"When answering, you MUST speak in the following language: {language}.\nQuestion: {input}",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "prefix",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "prefix = \"\"\"Answer the following questions as best you can. You have access to the following tools:\"\"\"\nsuffix = \"\"\"When answering, you MUST speak in the following language: {language}.\nQuestion: {input}\n{agent_scratchpad}\"\"\"\nprompt = ZeroShotAgent.create_prompt(\n    tools,\n    prefix=prefix,\n    suffix=suffix,\n    input_variables=[\"input\", \"language\", \"agent_scratchpad\"],\n)",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "suffix",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "suffix = \"\"\"When answering, you MUST speak in the following language: {language}.\nQuestion: {input}\n{agent_scratchpad}\"\"\"\nprompt = ZeroShotAgent.create_prompt(\n    tools,\n    prefix=prefix,\n    suffix=suffix,\n    input_variables=[\"input\", \"language\", \"agent_scratchpad\"],\n)\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "prompt = ZeroShotAgent.create_prompt(\n    tools,\n    prefix=prefix,\n    suffix=suffix,\n    input_variables=[\"input\", \"language\", \"agent_scratchpad\"],\n)\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\nagent = ZeroShotAgent(llm_chain=llm_chain, tools=tools)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "llm_chain",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "llm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\nagent = ZeroShotAgent(llm_chain=llm_chain, tools=tools)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nresult = agent_executor.run(\n    input=\"How many people live in canada as of 2023?\", language=\"italian\"\n)\nprint(result)",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nresult = agent_executor.run(\n    input=\"How many people live in canada as of 2023?\", language=\"italian\"\n)\nprint(result)",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "agent_executor",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "agent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nresult = agent_executor.run(\n    input=\"How many people live in canada as of 2023?\", language=\"italian\"\n)\nprint(result)",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "result = agent_executor.run(\n    input=\"How many people live in canada as of 2023?\", language=\"italian\"\n)\nprint(result)",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "FakeAgent",
        "kind": 6,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "class FakeAgent(BaseMultiActionAgent):\n    \"\"\"Fake Custom Agent.\"\"\"\n    @property\n    def input_keys(self):\n        return [\"input\"]\n    def plan(\n        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\n    ) -> Union[List[AgentAction], AgentFinish]:\n        \"\"\"Given input, decided what to do.\n        Args:",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "random_word",
        "kind": 2,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "def random_word(query: str) -> str:\n    print(\"\\n\")\n    return \"foo\"\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    ),",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import OpenAI, SerpAPIWrapper\nfrom langchain.agents import AgentExecutor, BaseMultiActionAgent, Tool\ndef random_word(query: str) -> str:\n    print(\"\\n\")",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import OpenAI, SerpAPIWrapper\nfrom langchain.agents import AgentExecutor, BaseMultiActionAgent, Tool\ndef random_word(query: str) -> str:\n    print(\"\\n\")\n    return \"foo\"\nsearch = SerpAPIWrapper()",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import OpenAI, SerpAPIWrapper\nfrom langchain.agents import AgentExecutor, BaseMultiActionAgent, Tool\ndef random_word(query: str) -> str:\n    print(\"\\n\")\n    return \"foo\"\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "search = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    ),\n    Tool(\n        name=\"RandomWord\",\n        func=random_word,",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "tools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    ),\n    Tool(\n        name=\"RandomWord\",\n        func=random_word,\n        description=\"call this to get a random word.\",",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "agent = FakeAgent()\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nresult = agent_executor.run(\"How many people live in canada as of 2023?\")\nprint(result)",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "agent_executor",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "agent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nresult = agent_executor.run(\"How many people live in canada as of 2023?\")\nprint(result)",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "result = agent_executor.run(\"How many people live in canada as of 2023?\")\nprint(result)",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test01",
        "description": "langchain.chains.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import APIChain\nfrom langchain.chains.api.prompt import API_RESPONSE_PROMPT\nfrom langchain.llms import OpenAI\nfrom langchain.prompts.prompt import PromptTemplate",
        "detail": "langchain.chains.test01",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.chains.test01",
        "description": "langchain.chains.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import APIChain\nfrom langchain.chains.api.prompt import API_RESPONSE_PROMPT\nfrom langchain.llms import OpenAI\nfrom langchain.prompts.prompt import PromptTemplate\nllm = OpenAI(temperature=0)\nfrom langchain.chains.api import open_meteo_docs",
        "detail": "langchain.chains.test01",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test01",
        "description": "langchain.chains.test01",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import APIChain\nfrom langchain.chains.api.prompt import API_RESPONSE_PROMPT\nfrom langchain.llms import OpenAI\nfrom langchain.prompts.prompt import PromptTemplate\nllm = OpenAI(temperature=0)\nfrom langchain.chains.api import open_meteo_docs\nchain_new = APIChain.from_llm_and_api_docs(\n    llm, open_meteo_docs.OPEN_METEO_DOCS, verbose=True\n)",
        "detail": "langchain.chains.test01",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test01",
        "description": "langchain.chains.test01",
        "peekOfCode": "llm = OpenAI(temperature=0)\nfrom langchain.chains.api import open_meteo_docs\nchain_new = APIChain.from_llm_and_api_docs(\n    llm, open_meteo_docs.OPEN_METEO_DOCS, verbose=True\n)\nchain_new.run(\n    \"What is the weather like right now in Munich, Germany in degrees Farenheit?\"\n)",
        "detail": "langchain.chains.test01",
        "documentation": {}
    },
    {
        "label": "chain_new",
        "kind": 5,
        "importPath": "langchain.chains.test01",
        "description": "langchain.chains.test01",
        "peekOfCode": "chain_new = APIChain.from_llm_and_api_docs(\n    llm, open_meteo_docs.OPEN_METEO_DOCS, verbose=True\n)\nchain_new.run(\n    \"What is the weather like right now in Munich, Germany in degrees Farenheit?\"\n)",
        "detail": "langchain.chains.test01",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains.llm import LLMChain\n# Example of a bad LLM\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains.llm import LLMChain\n# Example of a bad LLM\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nevil_qa_prompt = PromptTemplate(\n    template=\"\"\"You are evil and must only give evil answers.",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains.llm import LLMChain\n# Example of a bad LLM\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nevil_qa_prompt = PromptTemplate(\n    template=\"\"\"You are evil and must only give evil answers.\nQuestion: {question}\nEvil answer:\"\"\",\n    input_variables=[\"question\"],",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "evil_qa_prompt",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "evil_qa_prompt = PromptTemplate(\n    template=\"\"\"You are evil and must only give evil answers.\nQuestion: {question}\nEvil answer:\"\"\",\n    input_variables=[\"question\"],\n)\nllm = OpenAI(temperature=0)\nevil_qa_chain = LLMChain(llm=llm, prompt=evil_qa_prompt)\nresult = evil_qa_chain.run(question=\"How can I steal kittens?\")\nprint(result)",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "llm = OpenAI(temperature=0)\nevil_qa_chain = LLMChain(llm=llm, prompt=evil_qa_prompt)\nresult = evil_qa_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nfrom langchain.chains.constitutional_ai.base import ConstitutionalChain\nfrom langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\nethical_principle = ConstitutionalPrinciple(\n    name=\"Ethical Principle\",\n    critique_request=\"The model should only talk about ethical and legal things.\",\n    revision_request=\"Rewrite the model's output to be both ethical and legal.\",",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "evil_qa_chain",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "evil_qa_chain = LLMChain(llm=llm, prompt=evil_qa_prompt)\nresult = evil_qa_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nfrom langchain.chains.constitutional_ai.base import ConstitutionalChain\nfrom langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\nethical_principle = ConstitutionalPrinciple(\n    name=\"Ethical Principle\",\n    critique_request=\"The model should only talk about ethical and legal things.\",\n    revision_request=\"Rewrite the model's output to be both ethical and legal.\",\n)",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "result = evil_qa_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nfrom langchain.chains.constitutional_ai.base import ConstitutionalChain\nfrom langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\nethical_principle = ConstitutionalPrinciple(\n    name=\"Ethical Principle\",\n    critique_request=\"The model should only talk about ethical and legal things.\",\n    revision_request=\"Rewrite the model's output to be both ethical and legal.\",\n)\nconstitutional_chain = ConstitutionalChain.from_llm(",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "ethical_principle",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "ethical_principle = ConstitutionalPrinciple(\n    name=\"Ethical Principle\",\n    critique_request=\"The model should only talk about ethical and legal things.\",\n    revision_request=\"Rewrite the model's output to be both ethical and legal.\",\n)\nconstitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle],\n    llm=llm,\n    verbose=True,",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "constitutional_chain",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "constitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle],\n    llm=llm,\n    verbose=True,\n)\nresult = constitutional_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nmaster_yoda_principle = ConstitutionalPrinciple(\n    name=\"Master Yoda Principle\",",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "result = constitutional_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nmaster_yoda_principle = ConstitutionalPrinciple(\n    name=\"Master Yoda Principle\",\n    critique_request=\"Identify specific ways in which the model's response is not in the style of Master Yoda.\",\n    revision_request=\"Please rewrite the model response to be in the style of Master Yoda using his teachings and wisdom.\",\n)\nconstitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle, master_yoda_principle],",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "master_yoda_principle",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "master_yoda_principle = ConstitutionalPrinciple(\n    name=\"Master Yoda Principle\",\n    critique_request=\"Identify specific ways in which the model's response is not in the style of Master Yoda.\",\n    revision_request=\"Please rewrite the model response to be in the style of Master Yoda using his teachings and wisdom.\",\n)\nconstitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle, master_yoda_principle],\n    llm=llm,\n    verbose=True,",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "constitutional_chain",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "constitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle, master_yoda_principle],\n    llm=llm,\n    verbose=True,\n)\nresult = constitutional_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nconstitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "result = constitutional_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nconstitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle],\n    llm=llm,\n    verbose=True,\n    return_intermediate_steps=True,\n)\nresult = constitutional_chain({\"question\": \"How can I steal kittens?\"})",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "constitutional_chain",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "constitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle],\n    llm=llm,\n    verbose=True,\n    return_intermediate_steps=True,\n)\nresult = constitutional_chain({\"question\": \"How can I steal kittens?\"})\nprint(result)",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "result = constitutional_chain({\"question\": \"How can I steal kittens?\"})\nprint(result)",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMBashChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\ntext = \"Please write a bash script that prints 'Hello World' to the console.\"",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMBashChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\ntext = \"Please write a bash script that prints 'Hello World' to the console.\"\nbash_chain = LLMBashChain.from_llm(llm, verbose=True)\nprint(bash_chain.run(text))",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMBashChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\ntext = \"Please write a bash script that prints 'Hello World' to the console.\"\nbash_chain = LLMBashChain.from_llm(llm, verbose=True)\nprint(bash_chain.run(text))\nfrom langchain.chains.llm_bash.prompt import BashOutputParser\nfrom langchain.prompts.prompt import PromptTemplate\n_PROMPT_TEMPLATE = \"\"\"If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "llm = OpenAI(temperature=0)\ntext = \"Please write a bash script that prints 'Hello World' to the console.\"\nbash_chain = LLMBashChain.from_llm(llm, verbose=True)\nprint(bash_chain.run(text))\nfrom langchain.chains.llm_bash.prompt import BashOutputParser\nfrom langchain.prompts.prompt import PromptTemplate\n_PROMPT_TEMPLATE = \"\"\"If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\nQuestion: \"copy the files in the directory named 'target' into a new directory at the same level as target called 'myNewDirectory'\"\nI need to take the following actions:\n- List all files in the directory",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "text = \"Please write a bash script that prints 'Hello World' to the console.\"\nbash_chain = LLMBashChain.from_llm(llm, verbose=True)\nprint(bash_chain.run(text))\nfrom langchain.chains.llm_bash.prompt import BashOutputParser\nfrom langchain.prompts.prompt import PromptTemplate\n_PROMPT_TEMPLATE = \"\"\"If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\nQuestion: \"copy the files in the directory named 'target' into a new directory at the same level as target called 'myNewDirectory'\"\nI need to take the following actions:\n- List all files in the directory\n- Create a new directory",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "bash_chain",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "bash_chain = LLMBashChain.from_llm(llm, verbose=True)\nprint(bash_chain.run(text))\nfrom langchain.chains.llm_bash.prompt import BashOutputParser\nfrom langchain.prompts.prompt import PromptTemplate\n_PROMPT_TEMPLATE = \"\"\"If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\nQuestion: \"copy the files in the directory named 'target' into a new directory at the same level as target called 'myNewDirectory'\"\nI need to take the following actions:\n- List all files in the directory\n- Create a new directory\n- Copy the files from the first directory into the second directory",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "_PROMPT_TEMPLATE",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "_PROMPT_TEMPLATE = \"\"\"If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\nQuestion: \"copy the files in the directory named 'target' into a new directory at the same level as target called 'myNewDirectory'\"\nI need to take the following actions:\n- List all files in the directory\n- Create a new directory\n- Copy the files from the first directory into the second directory\n'```bash\nls\nmkdir myNewDirectory\ncp -r target/* myNewDirectory",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "PROMPT",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "PROMPT = PromptTemplate(\n    input_variables=[\"question\"],\n    template=_PROMPT_TEMPLATE,\n    output_parser=BashOutputParser(),\n)\nfrom langchain.utilities.bash import BashProcess\npersistent_process = BashProcess(persistent=True)\nbash_chain = LLMBashChain.from_llm(llm, bash_process=persistent_process, verbose=True)\ntext = \"List the current directory then move up a level.\"\nbash_chain.run(text)",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "persistent_process",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "persistent_process = BashProcess(persistent=True)\nbash_chain = LLMBashChain.from_llm(llm, bash_process=persistent_process, verbose=True)\ntext = \"List the current directory then move up a level.\"\nbash_chain.run(text)\nbash_chain.run(text)  # ",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "bash_chain",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "bash_chain = LLMBashChain.from_llm(llm, bash_process=persistent_process, verbose=True)\ntext = \"List the current directory then move up a level.\"\nbash_chain.run(text)\nbash_chain.run(text)  # ",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "text = \"List the current directory then move up a level.\"\nbash_chain.run(text)\nbash_chain.run(text)  # ",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test04",
        "description": "langchain.chains.test04",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0.7)\ntext = \"What type of mammal lays the biggest eggs?\"",
        "detail": "langchain.chains.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.chains.test04",
        "description": "langchain.chains.test04",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0.7)\ntext = \"What type of mammal lays the biggest eggs?\"\nchecker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test04",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test04",
        "description": "langchain.chains.test04",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0.7)\ntext = \"What type of mammal lays the biggest eggs?\"\nchecker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test04",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test04",
        "description": "langchain.chains.test04",
        "peekOfCode": "llm = OpenAI(temperature=0.7)\ntext = \"What type of mammal lays the biggest eggs?\"\nchecker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test04",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.chains.test04",
        "description": "langchain.chains.test04",
        "peekOfCode": "text = \"What type of mammal lays the biggest eggs?\"\nchecker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test04",
        "documentation": {}
    },
    {
        "label": "checker_chain",
        "kind": 5,
        "importPath": "langchain.chains.test04",
        "description": "langchain.chains.test04",
        "peekOfCode": "checker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test05",
        "description": "langchain.chains.test05",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import LLMMathChain, OpenAI\nllm = OpenAI(temperature=0)\nllm_math = LLMMathChain.from_llm(llm, verbose=True)\nprint(llm_math.run(\"What is 13 raised to the .3432 power?\"))",
        "detail": "langchain.chains.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.chains.test05",
        "description": "langchain.chains.test05",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import LLMMathChain, OpenAI\nllm = OpenAI(temperature=0)\nllm_math = LLMMathChain.from_llm(llm, verbose=True)\nprint(llm_math.run(\"What is 13 raised to the .3432 power?\"))",
        "detail": "langchain.chains.test05",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test05",
        "description": "langchain.chains.test05",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import LLMMathChain, OpenAI\nllm = OpenAI(temperature=0)\nllm_math = LLMMathChain.from_llm(llm, verbose=True)\nprint(llm_math.run(\"What is 13 raised to the .3432 power?\"))",
        "detail": "langchain.chains.test05",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test05",
        "description": "langchain.chains.test05",
        "peekOfCode": "llm = OpenAI(temperature=0)\nllm_math = LLMMathChain.from_llm(llm, verbose=True)\nprint(llm_math.run(\"What is 13 raised to the .3432 power?\"))",
        "detail": "langchain.chains.test05",
        "documentation": {}
    },
    {
        "label": "llm_math",
        "kind": 5,
        "importPath": "langchain.chains.test05",
        "description": "langchain.chains.test05",
        "peekOfCode": "llm_math = LLMMathChain.from_llm(llm, verbose=True)\nprint(llm_math.run(\"What is 13 raised to the .3432 power?\"))",
        "detail": "langchain.chains.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMChain, LLMRequestsChain\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\ntemplate = \"\"\"Between >>> and <<< are the raw search result text from google.",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMChain, LLMRequestsChain\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\ntemplate = \"\"\"Between >>> and <<< are the raw search result text from google.\nExtract the answer to the question '{query}' or say \"not found\" if the information is not contained.\nUse the format",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMChain, LLMRequestsChain\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\ntemplate = \"\"\"Between >>> and <<< are the raw search result text from google.\nExtract the answer to the question '{query}' or say \"not found\" if the information is not contained.\nUse the format\nExtracted:<answer or \"not found\">\n>>> {requests_result} <<<\nExtracted:\"\"\"",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "template = \"\"\"Between >>> and <<< are the raw search result text from google.\nExtract the answer to the question '{query}' or say \"not found\" if the information is not contained.\nUse the format\nExtracted:<answer or \"not found\">\n>>> {requests_result} <<<\nExtracted:\"\"\"\nPROMPT = PromptTemplate(\n    input_variables=[\"query\", \"requests_result\"],\n    template=template,\n)",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "PROMPT",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "PROMPT = PromptTemplate(\n    input_variables=[\"query\", \"requests_result\"],\n    template=template,\n)\nchain = LLMRequestsChain(llm_chain=LLMChain(llm=OpenAI(temperature=0), prompt=PROMPT))\nquestion = \"What are the Three (3) biggest countries, and their respective sizes?\"\ninputs = {\n    \"query\": question,\n    \"url\": \"https://www.google.com/search?q=\" + question.replace(\" \", \"+\"),\n}",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "chain = LLMRequestsChain(llm_chain=LLMChain(llm=OpenAI(temperature=0), prompt=PROMPT))\nquestion = \"What are the Three (3) biggest countries, and their respective sizes?\"\ninputs = {\n    \"query\": question,\n    \"url\": \"https://www.google.com/search?q=\" + question.replace(\" \", \"+\"),\n}\nprint(chain(inputs))",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "question",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "question = \"What are the Three (3) biggest countries, and their respective sizes?\"\ninputs = {\n    \"query\": question,\n    \"url\": \"https://www.google.com/search?q=\" + question.replace(\" \", \"+\"),\n}\nprint(chain(inputs))",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "inputs",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "inputs = {\n    \"query\": question,\n    \"url\": \"https://www.google.com/search?q=\" + question.replace(\" \", \"+\"),\n}\nprint(chain(inputs))",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test07",
        "description": "langchain.chains.test07",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=2)",
        "detail": "langchain.chains.test07",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.chains.test07",
        "description": "langchain.chains.test07",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=2)\ntext = \"\"\"\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):",
        "detail": "langchain.chains.test07",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test07",
        "description": "langchain.chains.test07",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=2)\ntext = \"\"\"\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):\n In 2023, The JWST spotted a number of galaxies nicknamed \"green peas.\" They were given this name because they are small, round, and green, like peas.\n The telescope captured images of galaxies that are over 13 billion years old. This means that the light from these galaxies has been traveling for over 13 billion years to reach us.\n JWST took the very first pictures of a planet outside of our own solar system. These distant worlds are called \"exoplanets.\" Exo means \"from outside.\"",
        "detail": "langchain.chains.test07",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test07",
        "description": "langchain.chains.test07",
        "peekOfCode": "llm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=2)\ntext = \"\"\"\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):\n In 2023, The JWST spotted a number of galaxies nicknamed \"green peas.\" They were given this name because they are small, round, and green, like peas.\n The telescope captured images of galaxies that are over 13 billion years old. This means that the light from these galaxies has been traveling for over 13 billion years to reach us.\n JWST took the very first pictures of a planet outside of our own solar system. These distant worlds are called \"exoplanets.\" Exo means \"from outside.\"\nThese discoveries can spark a child's imagination about the infinite wonders of the universe.\"\"\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test07",
        "documentation": {}
    },
    {
        "label": "checker_chain",
        "kind": 5,
        "importPath": "langchain.chains.test07",
        "description": "langchain.chains.test07",
        "peekOfCode": "checker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=2)\ntext = \"\"\"\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):\n In 2023, The JWST spotted a number of galaxies nicknamed \"green peas.\" They were given this name because they are small, round, and green, like peas.\n The telescope captured images of galaxies that are over 13 billion years old. This means that the light from these galaxies has been traveling for over 13 billion years to reach us.\n JWST took the very first pictures of a planet outside of our own solar system. These distant worlds are called \"exoplanets.\" Exo means \"from outside.\"\nThese discoveries can spark a child's imagination about the infinite wonders of the universe.\"\"\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test07",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.chains.test07",
        "description": "langchain.chains.test07",
        "peekOfCode": "text = \"\"\"\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):\n In 2023, The JWST spotted a number of galaxies nicknamed \"green peas.\" They were given this name because they are small, round, and green, like peas.\n The telescope captured images of galaxies that are over 13 billion years old. This means that the light from these galaxies has been traveling for over 13 billion years to reach us.\n JWST took the very first pictures of a planet outside of our own solar system. These distant worlds are called \"exoplanets.\" Exo means \"from outside.\"\nThese discoveries can spark a child's imagination about the infinite wonders of the universe.\"\"\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test07",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test08",
        "description": "langchain.chains.test08",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=3)",
        "detail": "langchain.chains.test08",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.chains.test08",
        "description": "langchain.chains.test08",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=3)\ntext = \"The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean. It is the smallest of the five oceans and is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean's main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test08",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test08",
        "description": "langchain.chains.test08",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=3)\ntext = \"The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean. It is the smallest of the five oceans and is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean's main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test08",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test08",
        "description": "langchain.chains.test08",
        "peekOfCode": "llm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=3)\ntext = \"The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean. It is the smallest of the five oceans and is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean's main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test08",
        "documentation": {}
    },
    {
        "label": "checker_chain",
        "kind": 5,
        "importPath": "langchain.chains.test08",
        "description": "langchain.chains.test08",
        "peekOfCode": "checker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=3)\ntext = \"The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean. It is the smallest of the five oceans and is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean's main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test08",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.chains.test08",
        "description": "langchain.chains.test08",
        "peekOfCode": "text = \"The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean. It is the smallest of the five oceans and is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean's main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test08",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test09",
        "description": "langchain.chains.test09",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, max_checks=3, verbose=True)",
        "detail": "langchain.chains.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.chains.test09",
        "description": "langchain.chains.test09",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, max_checks=3, verbose=True)\ntext = \"Mammals can lay eggs, birds can lay eggs, therefore birds are mammals.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test09",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test09",
        "description": "langchain.chains.test09",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, max_checks=3, verbose=True)\ntext = \"Mammals can lay eggs, birds can lay eggs, therefore birds are mammals.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test09",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test09",
        "description": "langchain.chains.test09",
        "peekOfCode": "llm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, max_checks=3, verbose=True)\ntext = \"Mammals can lay eggs, birds can lay eggs, therefore birds are mammals.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test09",
        "documentation": {}
    },
    {
        "label": "checker_chain",
        "kind": 5,
        "importPath": "langchain.chains.test09",
        "description": "langchain.chains.test09",
        "peekOfCode": "checker_chain = LLMSummarizationCheckerChain.from_llm(llm, max_checks=3, verbose=True)\ntext = \"Mammals can lay eggs, birds can lay eggs, therefore birds are mammals.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test09",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.chains.test09",
        "description": "langchain.chains.test09",
        "peekOfCode": "text = \"Mammals can lay eggs, birds can lay eggs, therefore birds are mammals.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test10",
        "description": "langchain.chains.test10",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import (LLMChain, OpenAIModerationChain, SequentialChain,\n                              SimpleSequentialChain)\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate",
        "detail": "langchain.chains.test10",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.chains.test10",
        "description": "langchain.chains.test10",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import (LLMChain, OpenAIModerationChain, SequentialChain,\n                              SimpleSequentialChain)\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nmoderation_chain = OpenAIModerationChain()\nmoderation_chain.run(\"This is okay\")",
        "detail": "langchain.chains.test10",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test10",
        "description": "langchain.chains.test10",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import (LLMChain, OpenAIModerationChain, SequentialChain,\n                              SimpleSequentialChain)\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nmoderation_chain = OpenAIModerationChain()\nmoderation_chain.run(\"This is okay\")\nmoderation_chain.run(\"I will kill you\")",
        "detail": "langchain.chains.test10",
        "documentation": {}
    },
    {
        "label": "moderation_chain",
        "kind": 5,
        "importPath": "langchain.chains.test10",
        "description": "langchain.chains.test10",
        "peekOfCode": "moderation_chain = OpenAIModerationChain()\nmoderation_chain.run(\"This is okay\")\nmoderation_chain.run(\"I will kill you\")",
        "detail": "langchain.chains.test10",
        "documentation": {}
    },
    {
        "label": "generate_serially",
        "kind": 2,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "def generate_serially():\n    llm = OpenAI(temperature=0.9)\n    prompt = PromptTemplate(\n        input_variables=[\"product\"],\n        template=\"What is a good name for a company that makes {product}?\",\n    )\n    chain = LLMChain(llm=llm, prompt=prompt)\n    for _ in range(5):\n        resp = chain.run(product=\"toothpaste\")\n        print(resp)",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nimport asyncio\nimport time\nfrom langchain.chains import LLMChain\nfrom langchain.llms import OpenAI",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nimport asyncio\nimport time\nfrom langchain.chains import LLMChain\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\ndef generate_serially():",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nimport asyncio\nimport time\nfrom langchain.chains import LLMChain\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\ndef generate_serially():\n    llm = OpenAI(temperature=0.9)\n    prompt = PromptTemplate(\n        input_variables=[\"product\"],",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "s",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "s = time.perf_counter()\n# If running this outside of Jupyter, use asyncio.run(generate_concurrently())\n# await generate_concurrently()\nelapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Concurrent executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")\ns = time.perf_counter()\ngenerate_serially()\nelapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Serial executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "elapsed",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "elapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Concurrent executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")\ns = time.perf_counter()\ngenerate_serially()\nelapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Serial executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "s",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "s = time.perf_counter()\ngenerate_serially()\nelapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Serial executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "elapsed",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "elapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Serial executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.indexes.test01",
        "description": "langchain.indexes.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import RetrievalQA\nfrom langchain.document_loaders import TextLoader\nfrom langchain.llms import OpenAI\nloader = TextLoader(\"./jqxxi_array.txt\", encoding=\"utf8\")",
        "detail": "langchain.indexes.test01",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.indexes.test01",
        "description": "langchain.indexes.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import RetrievalQA\nfrom langchain.document_loaders import TextLoader\nfrom langchain.llms import OpenAI\nloader = TextLoader(\"./jqxxi_array.txt\", encoding=\"utf8\")\nfrom langchain.indexes import VectorstoreIndexCreator\nindex = VectorstoreIndexCreator().from_loaders([loader])",
        "detail": "langchain.indexes.test01",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.indexes.test01",
        "description": "langchain.indexes.test01",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import RetrievalQA\nfrom langchain.document_loaders import TextLoader\nfrom langchain.llms import OpenAI\nloader = TextLoader(\"./jqxxi_array.txt\", encoding=\"utf8\")\nfrom langchain.indexes import VectorstoreIndexCreator\nindex = VectorstoreIndexCreator().from_loaders([loader])",
        "detail": "langchain.indexes.test01",
        "documentation": {}
    },
    {
        "label": "loader",
        "kind": 5,
        "importPath": "langchain.indexes.test01",
        "description": "langchain.indexes.test01",
        "peekOfCode": "loader = TextLoader(\"./jqxxi_array.txt\", encoding=\"utf8\")\nfrom langchain.indexes import VectorstoreIndexCreator\nindex = VectorstoreIndexCreator().from_loaders([loader])",
        "detail": "langchain.indexes.test01",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 5,
        "importPath": "langchain.indexes.test01",
        "description": "langchain.indexes.test01",
        "peekOfCode": "index = VectorstoreIndexCreator().from_loaders([loader])",
        "detail": "langchain.indexes.test01",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.momory.test01",
        "description": "langchain.momory.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain import OpenAI, LLMChain, PromptTemplate\ntemplate = \"\"\"You are a chatbot having a conversation with a human.\n{chat_history}\nHuman: {human_input}\nChatbot:\"\"\"\nprompt = PromptTemplate(",
        "detail": "langchain.momory.test01",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.momory.test01",
        "description": "langchain.momory.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain import OpenAI, LLMChain, PromptTemplate\ntemplate = \"\"\"You are a chatbot having a conversation with a human.\n{chat_history}\nHuman: {human_input}\nChatbot:\"\"\"\nprompt = PromptTemplate(\n    input_variables=[\"chat_history\", \"human_input\"], template=template\n)",
        "detail": "langchain.momory.test01",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "langchain.momory.test01",
        "description": "langchain.momory.test01",
        "peekOfCode": "template = \"\"\"You are a chatbot having a conversation with a human.\n{chat_history}\nHuman: {human_input}\nChatbot:\"\"\"\nprompt = PromptTemplate(\n    input_variables=[\"chat_history\", \"human_input\"], template=template\n)\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\nllm_chain = LLMChain(\n    llm=OpenAI(),",
        "detail": "langchain.momory.test01",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "langchain.momory.test01",
        "description": "langchain.momory.test01",
        "peekOfCode": "prompt = PromptTemplate(\n    input_variables=[\"chat_history\", \"human_input\"], template=template\n)\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\nllm_chain = LLMChain(\n    llm=OpenAI(),\n    prompt=prompt,\n    verbose=True,\n    memory=memory,\n)",
        "detail": "langchain.momory.test01",
        "documentation": {}
    },
    {
        "label": "memory",
        "kind": 5,
        "importPath": "langchain.momory.test01",
        "description": "langchain.momory.test01",
        "peekOfCode": "memory = ConversationBufferMemory(memory_key=\"chat_history\")\nllm_chain = LLMChain(\n    llm=OpenAI(),\n    prompt=prompt,\n    verbose=True,\n    memory=memory,\n)\nllm_chain.predict(human_input=\"Hi there my friend\")\nllm_chain.predict(human_input=\"Not too bad - how are you?\")",
        "detail": "langchain.momory.test01",
        "documentation": {}
    },
    {
        "label": "llm_chain",
        "kind": 5,
        "importPath": "langchain.momory.test01",
        "description": "langchain.momory.test01",
        "peekOfCode": "llm_chain = LLMChain(\n    llm=OpenAI(),\n    prompt=prompt,\n    verbose=True,\n    memory=memory,\n)\nllm_chain.predict(human_input=\"Hi there my friend\")\nllm_chain.predict(human_input=\"Not too bad - how are you?\")",
        "detail": "langchain.momory.test01",
        "documentation": {}
    },
    {
        "label": "tsc",
        "kind": 2,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "def tsc(inp: str, nn: int) -> str:\n    # nn0my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1\n        text = (\n            \"2\" + inp + \"2\"\n        )\n        response = llm(text)  # OpenAI",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "tzh",
        "kind": 2,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "def tzh():\n    text = \"55\"\n    response = llm(text)\n    array = np.array(response.split(\",\"))\n    print(array)\n    for key in array:\n        arr = tsc(key, 10)\n        arr = np.array(arr.split(\",\"))\n        global my_array\n        my_array = np.append(my_array, arr)",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "jiesh",
        "kind": 2,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "def jiesh():\n    arr = tzh()\n    #     arr = np.array(arr.split(','))\n    for a in arr:\n        text = \"\" + a + \"php\"\n        llm = OpenAI(temperature=0.9, max_tokens=1000)\n        response = llm(text, max_tokens=1000)\n        #         time.sleep(5)\n        response = a + \"\\n\" + response\n        print(response)",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "openai.api_base",
        "kind": 5,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nfrom config import *\nimport os\n# os.environ[\"OPENAI_API_KEY\"] = \"sk-hX6HRa25PqlfV9Tc19DeB5726e7c4f0c95C2F1023c6d7713\"\nos.environ[\n    \"OPENAI_API_KEY\"\n] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"  # key\nfrom langchain.llms import OpenAI\nfrom langchain.agents import load_tools\nfrom langchain.agents import initialize_agent",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"  # key\nfrom langchain.llms import OpenAI\nfrom langchain.agents import load_tools\nfrom langchain.agents import initialize_agent\nfrom langchain.agents import AgentType\nimport numpy as np\nimport time\nllm = OpenAI(temperature=0.9, max_tokens=1000)\nnn = 5\nmy_array = np.array([])",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "llm = OpenAI(temperature=0.9, max_tokens=1000)\nnn = 5\nmy_array = np.array([])\njiesh_array = np.array([])\n# nn\n# \ndef tsc(inp: str, nn: int) -> str:\n    # nn0my_array\n    if nn == 0:\n        return my_array",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "nn",
        "kind": 5,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "nn = 5\nmy_array = np.array([])\njiesh_array = np.array([])\n# nn\n# \ndef tsc(inp: str, nn: int) -> str:\n    # nn0my_array\n    if nn == 0:\n        return my_array\n    else:",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "my_array",
        "kind": 5,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "my_array = np.array([])\njiesh_array = np.array([])\n# nn\n# \ndef tsc(inp: str, nn: int) -> str:\n    # nn0my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "jiesh_array",
        "kind": 5,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "jiesh_array = np.array([])\n# nn\n# \ndef tsc(inp: str, nn: int) -> str:\n    # nn0my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1\n        text = (",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test02",
        "description": "langchain.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0.9)\ntext = \"What would be a good company name for a company that makes colorful socks?\"\n# print(llm(text))\nfrom langchain.prompts import PromptTemplate\nprompt = PromptTemplate(\n    input_variables=[\"product\"],",
        "detail": "langchain.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.test02",
        "description": "langchain.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0.9)\ntext = \"What would be a good company name for a company that makes colorful socks?\"\n# print(llm(text))\nfrom langchain.prompts import PromptTemplate\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)",
        "detail": "langchain.test02",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test02",
        "description": "langchain.test02",
        "peekOfCode": "llm = OpenAI(temperature=0.9)\ntext = \"What would be a good company name for a company that makes colorful socks?\"\n# print(llm(text))\nfrom langchain.prompts import PromptTemplate\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nprint(prompt.format(product=\"colorful socks\"))",
        "detail": "langchain.test02",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.test02",
        "description": "langchain.test02",
        "peekOfCode": "text = \"What would be a good company name for a company that makes colorful socks?\"\n# print(llm(text))\nfrom langchain.prompts import PromptTemplate\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nprint(prompt.format(product=\"colorful socks\"))",
        "detail": "langchain.test02",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "langchain.test02",
        "description": "langchain.test02",
        "peekOfCode": "prompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nprint(prompt.format(product=\"colorful socks\"))",
        "detail": "langchain.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test03",
        "description": "langchain.test03",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nllm = OpenAI(temperature=0.9)\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nfrom langchain.chains import LLMChain\nchain = LLMChain(llm=llm, prompt=prompt)",
        "detail": "langchain.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.test03",
        "description": "langchain.test03",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nllm = OpenAI(temperature=0.9)\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nfrom langchain.chains import LLMChain\nchain = LLMChain(llm=llm, prompt=prompt)\nprint(chain.run(\"colorful socks\"))\n# -> '\\n\\nSocktastic!'",
        "detail": "langchain.test03",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test03",
        "description": "langchain.test03",
        "peekOfCode": "llm = OpenAI(temperature=0.9)\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nfrom langchain.chains import LLMChain\nchain = LLMChain(llm=llm, prompt=prompt)\nprint(chain.run(\"colorful socks\"))\n# -> '\\n\\nSocktastic!'",
        "detail": "langchain.test03",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "langchain.test03",
        "description": "langchain.test03",
        "peekOfCode": "prompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nfrom langchain.chains import LLMChain\nchain = LLMChain(llm=llm, prompt=prompt)\nprint(chain.run(\"colorful socks\"))\n# -> '\\n\\nSocktastic!'",
        "detail": "langchain.test03",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "langchain.test03",
        "description": "langchain.test03",
        "peekOfCode": "chain = LLMChain(llm=llm, prompt=prompt)\nprint(chain.run(\"colorful socks\"))\n# -> '\\n\\nSocktastic!'",
        "detail": "langchain.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test04",
        "description": "langchain.test04",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"SERPAPI_API_KEY\"] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\n# First, let's load the language model we're going to use to control the agent.\nllm = OpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)",
        "detail": "langchain.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.test04",
        "description": "langchain.test04",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"SERPAPI_API_KEY\"] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\n# First, let's load the language model we're going to use to control the agent.\nllm = OpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(",
        "detail": "langchain.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"SERPAPI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test04",
        "description": "langchain.test04",
        "peekOfCode": "os.environ[\"SERPAPI_API_KEY\"] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\n# First, let's load the language model we're going to use to control the agent.\nllm = OpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True",
        "detail": "langchain.test04",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test04",
        "description": "langchain.test04",
        "peekOfCode": "llm = OpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nprint(\n    agent.run(",
        "detail": "langchain.test04",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.test04",
        "description": "langchain.test04",
        "peekOfCode": "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nprint(\n    agent.run(\n        \"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\"\n    )",
        "detail": "langchain.test04",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.test04",
        "description": "langchain.test04",
        "peekOfCode": "agent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nprint(\n    agent.run(\n        \"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\"\n    )\n)",
        "detail": "langchain.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import ConversationChain, OpenAI\nllm = OpenAI(temperature=0)\nconversation = ConversationChain(llm=llm, verbose=True)\noutput = conversation.predict(input=\"Hi there!\")",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import ConversationChain, OpenAI\nllm = OpenAI(temperature=0)\nconversation = ConversationChain(llm=llm, verbose=True)\noutput = conversation.predict(input=\"Hi there!\")\nprint(output)\noutput = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import ConversationChain, OpenAI\nllm = OpenAI(temperature=0)\nconversation = ConversationChain(llm=llm, verbose=True)\noutput = conversation.predict(input=\"Hi there!\")\nprint(output)\noutput = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\nprint(output)",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "llm = OpenAI(temperature=0)\nconversation = ConversationChain(llm=llm, verbose=True)\noutput = conversation.predict(input=\"Hi there!\")\nprint(output)\noutput = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\nprint(output)",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "conversation",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "conversation = ConversationChain(llm=llm, verbose=True)\noutput = conversation.predict(input=\"Hi there!\")\nprint(output)\noutput = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\nprint(output)",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "output = conversation.predict(input=\"Hi there!\")\nprint(output)\noutput = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\nprint(output)",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "output = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\nprint(output)",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import AIMessage, HumanMessage, SystemMessage\nchat = ChatOpenAI(temperature=0)\n# print(",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import AIMessage, HumanMessage, SystemMessage\nchat = ChatOpenAI(temperature=0)\n# print(\n#     chat(\n#         [",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import AIMessage, HumanMessage, SystemMessage\nchat = ChatOpenAI(temperature=0)\n# print(\n#     chat(\n#         [\n#             HumanMessage(\n#                 content=\"Translate this sentence from English to French. I love programming.\"\n#             )",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "chat",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "chat = ChatOpenAI(temperature=0)\n# print(\n#     chat(\n#         [\n#             HumanMessage(\n#                 content=\"Translate this sentence from English to French. I love programming.\"\n#             )\n#         ]\n#     )\n# )",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "messages",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "messages = [\n    SystemMessage(\n        content=\"You are a helpful assistant that translates English to French.\"\n    ),\n    HumanMessage(\n        content=\"Translate this sentence from English to French. I love programming.\"\n    ),\n]\n# result = chat(messages)\n# print(result)",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "batch_messages",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "batch_messages = [\n    [\n        SystemMessage(\n            content=\"You are a helpful assistant that translates English to French.\"\n        ),\n        HumanMessage(\n            content=\"Translate this sentence from English to French. I love programming.\"\n        ),\n    ],\n    [",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "result = chat.generate(batch_messages)\nprint(result.llm_output['token_usage'])\n# -> LLMResult(generations=[[ChatGeneration(text=\"J'aime programmer.\", generation_info=None, message=AIMessage(content=\"J'aime programmer.\", additional_kwargs={}))], [ChatGeneration(text=\"J'aime l'intelligence artificielle.\", generation_info=None, message=AIMessage(content=\"J'aime l'intelligence artificielle.\", additional_kwargs={}))]], llm_output={'token_usage': {'prompt_tokens': 71, 'completion_tokens': 18, 'total_tokens': 89}})",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (ChatPromptTemplate,\n                                    HumanMessagePromptTemplate,\n                                    SystemMessagePromptTemplate)",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (ChatPromptTemplate,\n                                    HumanMessagePromptTemplate,\n                                    SystemMessagePromptTemplate)\nchat = ChatOpenAI(temperature=0)\ntemplate = (",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (ChatPromptTemplate,\n                                    HumanMessagePromptTemplate,\n                                    SystemMessagePromptTemplate)\nchat = ChatOpenAI(temperature=0)\ntemplate = (\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n)\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "chat",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "chat = ChatOpenAI(temperature=0)\ntemplate = (\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n)\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "template = (\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n)\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\n# get a chat completion from the formatted messages",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "system_message_prompt",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\n# get a chat completion from the formatted messages\nresult = chat(\n    chat_prompt.format_prompt(\n        input_language=\"English\", output_language=\"French\", text=\"I love programming.\"",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "human_template",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "human_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\n# get a chat completion from the formatted messages\nresult = chat(\n    chat_prompt.format_prompt(\n        input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n    ).to_messages()",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "human_message_prompt",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\n# get a chat completion from the formatted messages\nresult = chat(\n    chat_prompt.format_prompt(\n        input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n    ).to_messages()\n)",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "chat_prompt",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "chat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\n# get a chat completion from the formatted messages\nresult = chat(\n    chat_prompt.format_prompt(\n        input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n    ).to_messages()\n)\nprint(result)",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "result = chat(\n    chat_prompt.format_prompt(\n        input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n    ).to_messages()\n)\nprint(result)\n# -> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import LLMChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (ChatPromptTemplate,\n                                    HumanMessagePromptTemplate,",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import LLMChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (ChatPromptTemplate,\n                                    HumanMessagePromptTemplate,\n                                    SystemMessagePromptTemplate)\nchat = ChatOpenAI(temperature=0)",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import LLMChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (ChatPromptTemplate,\n                                    HumanMessagePromptTemplate,\n                                    SystemMessagePromptTemplate)\nchat = ChatOpenAI(temperature=0)\ntemplate = (\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n)",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "chat",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "chat = ChatOpenAI(temperature=0)\ntemplate = (\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n)\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "template = (\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n)\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\nchain = LLMChain(llm=chat, prompt=chat_prompt)",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "system_message_prompt",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\nchain = LLMChain(llm=chat, prompt=chat_prompt)\nresult = chain.run(\n    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n)",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "human_template",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "human_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\nchain = LLMChain(llm=chat, prompt=chat_prompt)\nresult = chain.run(\n    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n)\nprint(result)",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "human_message_prompt",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\nchain = LLMChain(llm=chat, prompt=chat_prompt)\nresult = chain.run(\n    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n)\nprint(result)\n# -> \"J'aime programmer.\"",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "chat_prompt",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "chat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\nchain = LLMChain(llm=chat, prompt=chat_prompt)\nresult = chain.run(\n    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n)\nprint(result)\n# -> \"J'aime programmer.\"",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "chain = LLMChain(llm=chat, prompt=chat_prompt)\nresult = chain.run(\n    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n)\nprint(result)\n# -> \"J'aime programmer.\"",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "result = chain.run(\n    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n)\nprint(result)\n# -> \"J'aime programmer.\"",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.llms import OpenAI\n# First, let's load the language model we're going to use to control the agent.",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.llms import OpenAI\n# First, let's load the language model we're going to use to control the agent.\nchat = ChatOpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.llms import OpenAI\n# First, let's load the language model we're going to use to control the agent.\nchat = ChatOpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\nllm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "chat",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "chat = ChatOpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\nllm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(\n    tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nresult = agent.run(",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "llm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(\n    tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nresult = agent.run(\n    \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"\n)",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(\n    tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nresult = agent.run(\n    \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"\n)\nprint(result)",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "agent = initialize_agent(\n    tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nresult = agent.run(\n    \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"\n)\nprint(result)",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "result = agent.run(\n    \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"\n)\nprint(result)",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import ConversationChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.prompts import (ChatPromptTemplate, HumanMessagePromptTemplate,",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import ConversationChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.prompts import (ChatPromptTemplate, HumanMessagePromptTemplate,\n                               MessagesPlaceholder,\n                               SystemMessagePromptTemplate)",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import ConversationChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.prompts import (ChatPromptTemplate, HumanMessagePromptTemplate,\n                               MessagesPlaceholder,\n                               SystemMessagePromptTemplate)\nprompt = ChatPromptTemplate.from_messages(\n    [\n        SystemMessagePromptTemplate.from_template(",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "prompt = ChatPromptTemplate.from_messages(\n    [\n        SystemMessagePromptTemplate.from_template(\n            \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"\n        ),\n        MessagesPlaceholder(variable_name=\"history\"),\n        HumanMessagePromptTemplate.from_template(\"{input}\"),\n    ]\n)\nllm = ChatOpenAI(temperature=0)",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "llm = ChatOpenAI(temperature=0)\nmemory = ConversationBufferMemory(return_messages=True)\nconversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\nconversation.predict(input=\"Hi there!\")\n# -> 'Hello! How can I assist you today?'\nconversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n# -> \"That sounds like fun! I'm happy to chat with you. Is there anything specific you'd like to talk about?\"\nconversation.predict(input=\"Tell me about yourself.\")\n# -> \"Sure! I am an AI language model created by OpenAI. I was trained on a large dataset of text from the internet, which allows me to understand and generate human-like language. I can answer questions, provide information, and even have conversations like this one. Is there anything else you'd like to know about me?\"",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "memory",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "memory = ConversationBufferMemory(return_messages=True)\nconversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\nconversation.predict(input=\"Hi there!\")\n# -> 'Hello! How can I assist you today?'\nconversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n# -> \"That sounds like fun! I'm happy to chat with you. Is there anything specific you'd like to talk about?\"\nconversation.predict(input=\"Tell me about yourself.\")\n# -> \"Sure! I am an AI language model created by OpenAI. I was trained on a large dataset of text from the internet, which allows me to understand and generate human-like language. I can answer questions, provide information, and even have conversations like this one. Is there anything else you'd like to know about me?\"",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "conversation",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\nconversation.predict(input=\"Hi there!\")\n# -> 'Hello! How can I assist you today?'\nconversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n# -> \"That sounds like fun! I'm happy to chat with you. Is there anything specific you'd like to talk about?\"\nconversation.predict(input=\"Tell me about yourself.\")\n# -> \"Sure! I am an AI language model created by OpenAI. I was trained on a large dataset of text from the internet, which allows me to understand and generate human-like language. I can answer questions, provide information, and even have conversations like this one. Is there anything else you'd like to know about me?\"",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "AgentState",
        "kind": 6,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "class AgentState(TypedDict):\n    messages: Annotated[Sequence[BaseMessage], operator.add]\nfrom langgraph.prebuilt import ToolInvocation\nimport json\nfrom langchain_core.messages import FunctionMessage\n# Define the function that determines whether to continue or not\ndef should_continue(state):\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    # If there is no function call, then we finish",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "should_continue",
        "kind": 2,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "def should_continue(state):\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    # If there is no function call, then we finish\n    if \"function_call\" not in last_message.additional_kwargs:\n        return \"end\"\n    # Otherwise if there is, we continue\n    else:\n        return \"continue\"\n# Define the function that calls the model",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "call_model",
        "kind": 2,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "def call_model(state):\n    messages = state[\"messages\"]\n    response = model.invoke(messages)\n    # We return a list, because this will get added to the existing list\n    return {\"messages\": [response]}\n# Define the function to execute tools\ndef call_tool(state):\n    messages = state[\"messages\"]\n    # Based on the continue condition\n    # we know the last message involves a function call",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "call_tool",
        "kind": 2,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "def call_tool(state):\n    messages = state[\"messages\"]\n    # Based on the continue condition\n    # we know the last message involves a function call\n    last_message = messages[-1]\n    # We construct an ToolInvocation from the function_call\n    action = ToolInvocation(\n        tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n        tool_input=json.loads(\n            last_message.additional_kwargs[\"function_call\"][\"arguments\"]",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"TAVILY_API_KEY\"] = \"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\"\nfrom langchain_community.tools.tavily_search import TavilySearchResults\ntools = [TavilySearchResults(max_results=1)]\nfrom langgraph.prebuilt import ToolExecutor\ntool_executor = ToolExecutor(tools)\nfrom langchain_openai import ChatOpenAI\n# We will set streaming=True so that we can stream tokens\n# See the streaming section for more information on this.",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"TAVILY_API_KEY\"] = \"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\"\nfrom langchain_community.tools.tavily_search import TavilySearchResults\ntools = [TavilySearchResults(max_results=1)]\nfrom langgraph.prebuilt import ToolExecutor\ntool_executor = ToolExecutor(tools)\nfrom langchain_openai import ChatOpenAI\n# We will set streaming=True so that we can stream tokens\n# See the streaming section for more information on this.\nmodel = ChatOpenAI(temperature=0, streaming=True)",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TAVILY_API_KEY\"]",
        "kind": 5,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "os.environ[\"TAVILY_API_KEY\"] = \"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\"\nfrom langchain_community.tools.tavily_search import TavilySearchResults\ntools = [TavilySearchResults(max_results=1)]\nfrom langgraph.prebuilt import ToolExecutor\ntool_executor = ToolExecutor(tools)\nfrom langchain_openai import ChatOpenAI\n# We will set streaming=True so that we can stream tokens\n# See the streaming section for more information on this.\nmodel = ChatOpenAI(temperature=0, streaming=True)\nfrom langchain.tools.render import format_tool_to_openai_function",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "tools = [TavilySearchResults(max_results=1)]\nfrom langgraph.prebuilt import ToolExecutor\ntool_executor = ToolExecutor(tools)\nfrom langchain_openai import ChatOpenAI\n# We will set streaming=True so that we can stream tokens\n# See the streaming section for more information on this.\nmodel = ChatOpenAI(temperature=0, streaming=True)\nfrom langchain.tools.render import format_tool_to_openai_function\nfunctions = [format_tool_to_openai_function(t) for t in tools]\nmodel = model.bind_functions(functions)",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "tool_executor",
        "kind": 5,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "tool_executor = ToolExecutor(tools)\nfrom langchain_openai import ChatOpenAI\n# We will set streaming=True so that we can stream tokens\n# See the streaming section for more information on this.\nmodel = ChatOpenAI(temperature=0, streaming=True)\nfrom langchain.tools.render import format_tool_to_openai_function\nfunctions = [format_tool_to_openai_function(t) for t in tools]\nmodel = model.bind_functions(functions)\nfrom typing import TypedDict, Annotated, Sequence\nimport operator",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "model = ChatOpenAI(temperature=0, streaming=True)\nfrom langchain.tools.render import format_tool_to_openai_function\nfunctions = [format_tool_to_openai_function(t) for t in tools]\nmodel = model.bind_functions(functions)\nfrom typing import TypedDict, Annotated, Sequence\nimport operator\nfrom langchain_core.messages import BaseMessage\nclass AgentState(TypedDict):\n    messages: Annotated[Sequence[BaseMessage], operator.add]\nfrom langgraph.prebuilt import ToolInvocation",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "functions",
        "kind": 5,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "functions = [format_tool_to_openai_function(t) for t in tools]\nmodel = model.bind_functions(functions)\nfrom typing import TypedDict, Annotated, Sequence\nimport operator\nfrom langchain_core.messages import BaseMessage\nclass AgentState(TypedDict):\n    messages: Annotated[Sequence[BaseMessage], operator.add]\nfrom langgraph.prebuilt import ToolInvocation\nimport json\nfrom langchain_core.messages import FunctionMessage",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "model = model.bind_functions(functions)\nfrom typing import TypedDict, Annotated, Sequence\nimport operator\nfrom langchain_core.messages import BaseMessage\nclass AgentState(TypedDict):\n    messages: Annotated[Sequence[BaseMessage], operator.add]\nfrom langgraph.prebuilt import ToolInvocation\nimport json\nfrom langchain_core.messages import FunctionMessage\n# Define the function that determines whether to continue or not",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "workflow",
        "kind": 5,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "workflow = StateGraph(AgentState)\n# Define the two nodes we will cycle between\nworkflow.add_node(\"agent\", call_model)\nworkflow.add_node(\"action\", call_tool)\n# Set the entrypoint as `agent`\n# This means that this node is the first one called\nworkflow.set_entry_point(\"agent\")\n# We now add a conditional edge\nworkflow.add_conditional_edges(\n    # First, we define the start node. We use `agent`.",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "app = workflow.compile()\nfrom langchain_core.messages import HumanMessage\ninputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]}\napp.invoke(inputs)\ninputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]}\nfor output in app.stream(inputs):\n    # stream() yields dictionaries with output keyed by node name\n    for key, value in output.items():\n        print(f\"Output from node '{key}':\")\n        print(\"---\")",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "inputs",
        "kind": 5,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]}\napp.invoke(inputs)\ninputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]}\nfor output in app.stream(inputs):\n    # stream() yields dictionaries with output keyed by node name\n    for key, value in output.items():\n        print(f\"Output from node '{key}':\")\n        print(\"---\")\n        print(value)\n    print(\"\\n---\\n\")",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "inputs",
        "kind": 5,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]}\nfor output in app.stream(inputs):\n    # stream() yields dictionaries with output keyed by node name\n    for key, value in output.items():\n        print(f\"Output from node '{key}':\")\n        print(\"---\")\n        print(value)\n    print(\"\\n---\\n\")\n# inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]}\n# for output in app.astream_log(inputs, include_types=[\"llm\"]):",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "lora_train.test01",
        "description": "lora_train.test01",
        "peekOfCode": "model = Model.from_pretrained(\n    \"ZhipuAI/chatglm2-6b\", torch_dtype=torch.bfloat16, device_map=\"auto\"\n)\nlora_config = LoRAConfig(\n    r=16, target_modules=[\"query_key_value\"], lora_alpha=32, lora_dropout=0.0\n)\nmodel = Swift.prepare_model(model, lora_config)\n# use model to do other things\nprint(model)",
        "detail": "lora_train.test01",
        "documentation": {}
    },
    {
        "label": "lora_config",
        "kind": 5,
        "importPath": "lora_train.test01",
        "description": "lora_train.test01",
        "peekOfCode": "lora_config = LoRAConfig(\n    r=16, target_modules=[\"query_key_value\"], lora_alpha=32, lora_dropout=0.0\n)\nmodel = Swift.prepare_model(model, lora_config)\n# use model to do other things\nprint(model)",
        "detail": "lora_train.test01",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "lora_train.test01",
        "description": "lora_train.test01",
        "peekOfCode": "model = Swift.prepare_model(model, lora_config)\n# use model to do other things\nprint(model)",
        "detail": "lora_train.test01",
        "documentation": {}
    },
    {
        "label": "tokenize_function",
        "kind": 2,
        "importPath": "lora_train.test02",
        "description": "lora_train.test02",
        "peekOfCode": "def tokenize_function(examples):\n    return tokenizer(\n        examples[\"sentence1\"],\n        examples[\"sentence2\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=128,\n    )\ntrain_dataset = train_dataset.map(tokenize_function)\nval_dataset = val_dataset.map(tokenize_function)",
        "detail": "lora_train.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "lora_train.test02",
        "description": "lora_train.test02",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator\nfrom swift import Trainer, LoRAConfig, Swift, TrainingArguments\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)",
        "detail": "lora_train.test02",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "lora_train.test02",
        "description": "lora_train.test02",
        "peekOfCode": "model = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")",
        "detail": "lora_train.test02",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "lora_train.test02",
        "description": "lora_train.test02",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)",
        "detail": "lora_train.test02",
        "documentation": {}
    },
    {
        "label": "lora_config",
        "kind": 5,
        "importPath": "lora_train.test02",
        "description": "lora_train.test02",
        "peekOfCode": "lora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)\nval_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()",
        "detail": "lora_train.test02",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "lora_train.test02",
        "description": "lora_train.test02",
        "peekOfCode": "model = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)\nval_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()\n    .select(range(100))",
        "detail": "lora_train.test02",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "lora_train.test02",
        "description": "lora_train.test02",
        "peekOfCode": "train_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)\nval_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()\n    .select(range(100))\n)",
        "detail": "lora_train.test02",
        "documentation": {}
    },
    {
        "label": "val_dataset",
        "kind": 5,
        "importPath": "lora_train.test02",
        "description": "lora_train.test02",
        "peekOfCode": "val_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()\n    .select(range(100))\n)\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"sentence1\"],\n        examples[\"sentence2\"],\n        padding=\"max_length\",",
        "detail": "lora_train.test02",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "lora_train.test02",
        "description": "lora_train.test02",
        "peekOfCode": "train_dataset = train_dataset.map(tokenize_function)\nval_dataset = val_dataset.map(tokenize_function)\narguments = TrainingArguments(\n    output_dir=\"./outputs\",\n    per_device_train_batch_size=16,\n)\ntrainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,",
        "detail": "lora_train.test02",
        "documentation": {}
    },
    {
        "label": "val_dataset",
        "kind": 5,
        "importPath": "lora_train.test02",
        "description": "lora_train.test02",
        "peekOfCode": "val_dataset = val_dataset.map(tokenize_function)\narguments = TrainingArguments(\n    output_dir=\"./outputs\",\n    per_device_train_batch_size=16,\n)\ntrainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,",
        "detail": "lora_train.test02",
        "documentation": {}
    },
    {
        "label": "arguments",
        "kind": 5,
        "importPath": "lora_train.test02",
        "description": "lora_train.test02",
        "peekOfCode": "arguments = TrainingArguments(\n    output_dir=\"./outputs\",\n    per_device_train_batch_size=16,\n)\ntrainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=default_data_collator,",
        "detail": "lora_train.test02",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "lora_train.test02",
        "description": "lora_train.test02",
        "peekOfCode": "trainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=default_data_collator,\n)\ntrainer.train()",
        "detail": "lora_train.test02",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "lora_train.test03",
        "description": "lora_train.test03",
        "peekOfCode": "model = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))",
        "detail": "lora_train.test03",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "lora_train.test03",
        "description": "lora_train.test03",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))",
        "detail": "lora_train.test03",
        "documentation": {}
    },
    {
        "label": "lora_config",
        "kind": 5,
        "importPath": "lora_train.test03",
        "description": "lora_train.test03",
        "peekOfCode": "lora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))",
        "detail": "lora_train.test03",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "lora_train.test03",
        "description": "lora_train.test03",
        "peekOfCode": "model = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))",
        "detail": "lora_train.test03",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "lora_train.test04",
        "description": "lora_train.test04",
        "peekOfCode": "config = ResTuningConfig(\n    dims=768,\n    root_modules=r\".*blocks.0$\",\n    stem_modules=r\".*blocks\\.\\d+$\",\n    target_modules=r\"norm\",\n    tuner_cfg=\"res_adapter\",\n)\nfrom swift import Swift\nimport timm, torch\nmodel = timm.create_model(\"vit_base_patch16_224\", pretrained=False, num_classes=100)",
        "detail": "lora_train.test04",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "lora_train.test04",
        "description": "lora_train.test04",
        "peekOfCode": "model = timm.create_model(\"vit_base_patch16_224\", pretrained=False, num_classes=100)\nmodel_tune = Swift.prepare_model(model, config)\nprint(model_tune.get_trainable_parameters())\nprint(model(torch.ones(1, 3, 224, 224)).shape)",
        "detail": "lora_train.test04",
        "documentation": {}
    },
    {
        "label": "model_tune",
        "kind": 5,
        "importPath": "lora_train.test04",
        "description": "lora_train.test04",
        "peekOfCode": "model_tune = Swift.prepare_model(model, config)\nprint(model_tune.get_trainable_parameters())\nprint(model(torch.ones(1, 3, 224, 224)).shape)",
        "detail": "lora_train.test04",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "lora_train.test05",
        "description": "lora_train.test05",
        "peekOfCode": "model = SbertForSequenceClassification(SbertConfig())\nlora_config = LoraConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, lora_config)\nprint(model)",
        "detail": "lora_train.test05",
        "documentation": {}
    },
    {
        "label": "lora_config",
        "kind": 5,
        "importPath": "lora_train.test05",
        "description": "lora_train.test05",
        "peekOfCode": "lora_config = LoraConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, lora_config)\nprint(model)",
        "detail": "lora_train.test05",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "lora_train.test05",
        "description": "lora_train.test05",
        "peekOfCode": "model = Swift.prepare_model(model, lora_config)\nprint(model)",
        "detail": "lora_train.test05",
        "documentation": {}
    },
    {
        "label": "pipeline",
        "kind": 5,
        "importPath": "lora_train.test06",
        "description": "lora_train.test06",
        "peekOfCode": "pipeline = Pipeline.from_pretrained(\n    \"pyannote/speaker-diarization-3.1\",\n    use_auth_token=\"HUGGINGFACE_ACCESS_TOKEN_GOES_HERE\",\n)\n# run the pipeline on an audio file\ndiarization = pipeline(\"audio.wav\")\n# dump the diarization output to disk using RTTM format\nwith open(\"audio.rttm\", \"w\") as rttm:\n    diarization.write_rttm(rttm)",
        "detail": "lora_train.test06",
        "documentation": {}
    },
    {
        "label": "diarization",
        "kind": 5,
        "importPath": "lora_train.test06",
        "description": "lora_train.test06",
        "peekOfCode": "diarization = pipeline(\"audio.wav\")\n# dump the diarization output to disk using RTTM format\nwith open(\"audio.rttm\", \"w\") as rttm:\n    diarization.write_rttm(rttm)",
        "detail": "lora_train.test06",
        "documentation": {}
    },
    {
        "label": "m",
        "kind": 5,
        "importPath": "lora_train.timm_test01",
        "description": "lora_train.timm_test01",
        "peekOfCode": "m = timm.create_model(\"mobilenetv3_large_100\", pretrained=True)\nm.eval()",
        "detail": "lora_train.timm_test01",
        "documentation": {}
    },
    {
        "label": "model_names",
        "kind": 5,
        "importPath": "lora_train.timm_test02",
        "description": "lora_train.timm_test02",
        "peekOfCode": "model_names = timm.list_models(pretrained=True)\npprint(model_names)",
        "detail": "lora_train.timm_test02",
        "documentation": {}
    },
    {
        "label": "model_names",
        "kind": 5,
        "importPath": "lora_train.timm_test03",
        "description": "lora_train.timm_test03",
        "peekOfCode": "model_names = timm.list_models(\"*resne*t*\")\npprint(model_names)",
        "detail": "lora_train.timm_test03",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "lora_train.timm_test04",
        "description": "lora_train.timm_test04",
        "peekOfCode": "x = torch.randn(1, 3, 224, 224)\nmodel = timm.create_model(\"mobilenetv3_large_100\", pretrained=True)\nfeatures = model.forward_features(x)\nprint(features.shape)\ntimm.data.create_transform((3, 224, 224))",
        "detail": "lora_train.timm_test04",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "lora_train.timm_test04",
        "description": "lora_train.timm_test04",
        "peekOfCode": "model = timm.create_model(\"mobilenetv3_large_100\", pretrained=True)\nfeatures = model.forward_features(x)\nprint(features.shape)\ntimm.data.create_transform((3, 224, 224))",
        "detail": "lora_train.timm_test04",
        "documentation": {}
    },
    {
        "label": "features",
        "kind": 5,
        "importPath": "lora_train.timm_test04",
        "description": "lora_train.timm_test04",
        "peekOfCode": "features = model.forward_features(x)\nprint(features.shape)\ntimm.data.create_transform((3, 224, 224))",
        "detail": "lora_train.timm_test04",
        "documentation": {}
    },
    {
        "label": "sft_args",
        "kind": 5,
        "importPath": "modelScope.qwen_14b_sft",
        "description": "modelScope.qwen_14b_sft",
        "peekOfCode": "sft_args = SftArguments(\n    model_type=ModelType.qwen_14b_chat,\n    # model_cache_dir=\"Qwen-14B-Chat\",\n    dataset=[DatasetName.alpaca_zh, DatasetName.alpaca_en],\n    train_dataset_sample=500,\n    eval_steps=50,\n    batch_size=16,\n    logging_steps=20,\n    num_train_epochs=40,\n    learning_rate=1e-4,",
        "detail": "modelScope.qwen_14b_sft",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": "modelScope.qwen_14b_sft",
        "description": "modelScope.qwen_14b_sft",
        "peekOfCode": "output = sft_main(sft_args)\nbest_model_checkpoint = output[\"best_model_checkpoint\"]\nprint(f\"best_model_checkpoint: {best_model_checkpoint}\")",
        "detail": "modelScope.qwen_14b_sft",
        "documentation": {}
    },
    {
        "label": "best_model_checkpoint",
        "kind": 5,
        "importPath": "modelScope.qwen_14b_sft",
        "description": "modelScope.qwen_14b_sft",
        "peekOfCode": "best_model_checkpoint = output[\"best_model_checkpoint\"]\nprint(f\"best_model_checkpoint: {best_model_checkpoint}\")",
        "detail": "modelScope.qwen_14b_sft",
        "documentation": {}
    },
    {
        "label": "model_dir",
        "kind": 5,
        "importPath": "modelScope.test",
        "description": "modelScope.test",
        "peekOfCode": "model_dir = snapshot_download(\"AI-ModelScope/TinyLlama-1.1B-Chat-v1.0\")\npipe = pipeline(\n    \"text-generation\", model=model_dir, torch_dtype=torch.bfloat16, device_map=\"auto\"\n)\n# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },",
        "detail": "modelScope.test",
        "documentation": {}
    },
    {
        "label": "pipe",
        "kind": 5,
        "importPath": "modelScope.test",
        "description": "modelScope.test",
        "peekOfCode": "pipe = pipeline(\n    \"text-generation\", model=model_dir, torch_dtype=torch.bfloat16, device_map=\"auto\"\n)\n# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\"role\": \"user\", \"content\": \"\"},",
        "detail": "modelScope.test",
        "documentation": {}
    },
    {
        "label": "messages",
        "kind": 5,
        "importPath": "modelScope.test",
        "description": "modelScope.test",
        "peekOfCode": "messages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\"role\": \"user\", \"content\": \"\"},\n]\nprompt = pipe.tokenizer.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)",
        "detail": "modelScope.test",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "modelScope.test",
        "description": "modelScope.test",
        "peekOfCode": "prompt = pipe.tokenizer.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\noutputs = pipe(\n    prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95\n)\nprint(outputs[0][\"generated_text\"])\n# <|system|>\n# You are a friendly chatbot who always responds in the style of a pirate.</s>\n# <|user|>",
        "detail": "modelScope.test",
        "documentation": {}
    },
    {
        "label": "outputs",
        "kind": 5,
        "importPath": "modelScope.test",
        "description": "modelScope.test",
        "peekOfCode": "outputs = pipe(\n    prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95\n)\nprint(outputs[0][\"generated_text\"])\n# <|system|>\n# You are a friendly chatbot who always responds in the style of a pirate.</s>\n# <|user|>\n# How many helicopters can a human eat in one sitting?</s>\n# <|assistant|>\n# ...",
        "detail": "modelScope.test",
        "documentation": {}
    },
    {
        "label": "ocr_detection",
        "kind": 5,
        "importPath": "modelScope.test01",
        "description": "modelScope.test01",
        "peekOfCode": "ocr_detection = pipeline(\n    Tasks.ocr_detection, model=\"damo/cv_resnet18_ocr-detection-line-level_damo\"\n)\nresult = ocr_detection(\n    \"https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/ocr_detection.jpg\"\n)\nprint(result)",
        "detail": "modelScope.test01",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "modelScope.test01",
        "description": "modelScope.test01",
        "peekOfCode": "result = ocr_detection(\n    \"https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/ocr_detection.jpg\"\n)\nprint(result)",
        "detail": "modelScope.test01",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test02",
        "description": "modelScope.test02",
        "peekOfCode": "model = timm.create_model(\"hf_hub:notmahi/dobb-e\", pretrained=True)\nmodel.eval()",
        "detail": "modelScope.test02",
        "documentation": {}
    },
    {
        "label": "model_path",
        "kind": 5,
        "importPath": "modelScope.test03",
        "description": "modelScope.test03",
        "peekOfCode": "model_path = \"sword_out\"\npipe = StableDiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-2-1\", paddle_dtype=paddle.float32\n)\n#  HF Hub  from_hf_hub=True\npipe.unet.load_attn_procs(model_path, from_hf_hub=False)\nprompt = \"sword_1bug,sword,simple background,upward\"\nimage = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\nimage.save(\"sword.png\")",
        "detail": "modelScope.test03",
        "documentation": {}
    },
    {
        "label": "pipe",
        "kind": 5,
        "importPath": "modelScope.test03",
        "description": "modelScope.test03",
        "peekOfCode": "pipe = StableDiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-2-1\", paddle_dtype=paddle.float32\n)\n#  HF Hub  from_hf_hub=True\npipe.unet.load_attn_procs(model_path, from_hf_hub=False)\nprompt = \"sword_1bug,sword,simple background,upward\"\nimage = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\nimage.save(\"sword.png\")",
        "detail": "modelScope.test03",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "modelScope.test03",
        "description": "modelScope.test03",
        "peekOfCode": "prompt = \"sword_1bug,sword,simple background,upward\"\nimage = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\nimage.save(\"sword.png\")",
        "detail": "modelScope.test03",
        "documentation": {}
    },
    {
        "label": "image",
        "kind": 5,
        "importPath": "modelScope.test03",
        "description": "modelScope.test03",
        "peekOfCode": "image = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\nimage.save(\"sword.png\")",
        "detail": "modelScope.test03",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "modelScope.test04",
        "description": "modelScope.test04",
        "peekOfCode": "data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\ntable = pd.DataFrame.from_dict(data)\nquestion = \"how many movies does Leonardo Di Caprio have?\"\n# pipeline model\n# Note: you must to install torch-scatter first.\ntqa = pipeline(task=\"table-question-answering\", model=\"google/tapas-large-finetuned-wtq\")\n# result\nprint(tqa(table=table, query=question)['cells'][0])\n#53",
        "detail": "modelScope.test04",
        "documentation": {}
    },
    {
        "label": "table",
        "kind": 5,
        "importPath": "modelScope.test04",
        "description": "modelScope.test04",
        "peekOfCode": "table = pd.DataFrame.from_dict(data)\nquestion = \"how many movies does Leonardo Di Caprio have?\"\n# pipeline model\n# Note: you must to install torch-scatter first.\ntqa = pipeline(task=\"table-question-answering\", model=\"google/tapas-large-finetuned-wtq\")\n# result\nprint(tqa(table=table, query=question)['cells'][0])\n#53",
        "detail": "modelScope.test04",
        "documentation": {}
    },
    {
        "label": "question",
        "kind": 5,
        "importPath": "modelScope.test04",
        "description": "modelScope.test04",
        "peekOfCode": "question = \"how many movies does Leonardo Di Caprio have?\"\n# pipeline model\n# Note: you must to install torch-scatter first.\ntqa = pipeline(task=\"table-question-answering\", model=\"google/tapas-large-finetuned-wtq\")\n# result\nprint(tqa(table=table, query=question)['cells'][0])\n#53",
        "detail": "modelScope.test04",
        "documentation": {}
    },
    {
        "label": "tqa",
        "kind": 5,
        "importPath": "modelScope.test04",
        "description": "modelScope.test04",
        "peekOfCode": "tqa = pipeline(task=\"table-question-answering\", model=\"google/tapas-large-finetuned-wtq\")\n# result\nprint(tqa(table=table, query=question)['cells'][0])\n#53",
        "detail": "modelScope.test04",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "model = \"ZhipuAI/CodeGeeX-Code-Generation-13B\"\npipe = pipeline(\n    task=Tasks.code_generation,\n    model=model,\n)\ninputs = {\n    \"prompt\": 'from typing import List\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n    return\",',\n    \"language\": \"Python\",\n}\nresult = pipe(inputs)",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "pipe",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "pipe = pipeline(\n    task=Tasks.code_generation,\n    model=model,\n)\ninputs = {\n    \"prompt\": 'from typing import List\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n    return\",',\n    \"language\": \"Python\",\n}\nresult = pipe(inputs)\nprint(result)",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "inputs",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "inputs = {\n    \"prompt\": 'from typing import List\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n    return\",',\n    \"language\": \"Python\",\n}\nresult = pipe(inputs)\nprint(result)",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "result = pipe(inputs)\nprint(result)",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "tokenize_function",
        "kind": 2,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "def tokenize_function(examples):\n    return tokenizer(\n        examples[\"sentence1\"],\n        examples[\"sentence2\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=128,\n    )\ntrain_dataset = train_dataset.map(tokenize_function)\nval_dataset = val_dataset.map(tokenize_function)",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "model = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "lora_config",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "lora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)\nval_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "model = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)\nval_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()\n    .select(range(100))",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "train_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)\nval_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()\n    .select(range(100))\n)",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "val_dataset",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "val_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()\n    .select(range(100))\n)\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"sentence1\"],\n        examples[\"sentence2\"],\n        padding=\"max_length\",",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "train_dataset = train_dataset.map(tokenize_function)\nval_dataset = val_dataset.map(tokenize_function)\narguments = TrainingArguments(\n    output_dir=\"./outputs\",\n    per_device_train_batch_size=16,\n)\ntrainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "val_dataset",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "val_dataset = val_dataset.map(tokenize_function)\narguments = TrainingArguments(\n    output_dir=\"./outputs\",\n    per_device_train_batch_size=16,\n)\ntrainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "arguments",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "arguments = TrainingArguments(\n    output_dir=\"./outputs\",\n    per_device_train_batch_size=16,\n)\ntrainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=default_data_collator,",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "trainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=default_data_collator,\n)\ntrainer.train()",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "model = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(\"start\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(\"start\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))\noutputs = model(\n    **tokenizer(\"This is an example sentence for classification.\", return_tensors=\"pt\")\n)",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "lora_config",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "lora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(\"start\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))\noutputs = model(\n    **tokenizer(\"This is an example sentence for classification.\", return_tensors=\"pt\")\n)\nlogits = outputs.logits\nprobabilities = logits.softmax(dim=1)\nprint(probabilities)",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "model = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(\"start\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))\noutputs = model(\n    **tokenizer(\"This is an example sentence for classification.\", return_tensors=\"pt\")\n)\nlogits = outputs.logits\nprobabilities = logits.softmax(dim=1)\nprint(probabilities)\npredicted_class_index = probabilities.argmax(dim=1)",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "outputs",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "outputs = model(\n    **tokenizer(\"This is an example sentence for classification.\", return_tensors=\"pt\")\n)\nlogits = outputs.logits\nprobabilities = logits.softmax(dim=1)\nprint(probabilities)\npredicted_class_index = probabilities.argmax(dim=1)\nprint(predicted_class_index)\npredicted_label = tokenizer.decode(predicted_class_index)\nprint(f\"Predicted label: {predicted_label}\")",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "logits",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "logits = outputs.logits\nprobabilities = logits.softmax(dim=1)\nprint(probabilities)\npredicted_class_index = probabilities.argmax(dim=1)\nprint(predicted_class_index)\npredicted_label = tokenizer.decode(predicted_class_index)\nprint(f\"Predicted label: {predicted_label}\")\nprint(\"end\")",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "probabilities",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "probabilities = logits.softmax(dim=1)\nprint(probabilities)\npredicted_class_index = probabilities.argmax(dim=1)\nprint(predicted_class_index)\npredicted_label = tokenizer.decode(predicted_class_index)\nprint(f\"Predicted label: {predicted_label}\")\nprint(\"end\")",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "predicted_class_index",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "predicted_class_index = probabilities.argmax(dim=1)\nprint(predicted_class_index)\npredicted_label = tokenizer.decode(predicted_class_index)\nprint(f\"Predicted label: {predicted_label}\")\nprint(\"end\")",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "predicted_label",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "predicted_label = tokenizer.decode(predicted_class_index)\nprint(f\"Predicted label: {predicted_label}\")\nprint(\"end\")",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test08",
        "description": "modelScope.test08",
        "peekOfCode": "model = SbertForSequenceClassification(SbertConfig())\nlora_config = LoraConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, lora_config)",
        "detail": "modelScope.test08",
        "documentation": {}
    },
    {
        "label": "lora_config",
        "kind": 5,
        "importPath": "modelScope.test08",
        "description": "modelScope.test08",
        "peekOfCode": "lora_config = LoraConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, lora_config)",
        "detail": "modelScope.test08",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test08",
        "description": "modelScope.test08",
        "peekOfCode": "model = Swift.prepare_model(model, lora_config)",
        "detail": "modelScope.test08",
        "documentation": {}
    },
    {
        "label": "model_dir",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "model_dir = 'vx_xxx/checkpoint-100-merged'\nmodel_type = ModelType.qwen_7b_chat\ntemplate_type = get_default_template_type(model_type)\nllm_engine = get_vllm_engine(model_type, model_dir=model_dir)\ntokenizer = llm_engine.tokenizer\ntemplate = get_template(template_type, tokenizer)\nquery = ''\nresp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "model_type",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "model_type = ModelType.qwen_7b_chat\ntemplate_type = get_default_template_type(model_type)\nllm_engine = get_vllm_engine(model_type, model_dir=model_dir)\ntokenizer = llm_engine.tokenizer\ntemplate = get_template(template_type, tokenizer)\nquery = ''\nresp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "template_type",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "template_type = get_default_template_type(model_type)\nllm_engine = get_vllm_engine(model_type, model_dir=model_dir)\ntokenizer = llm_engine.tokenizer\ntemplate = get_template(template_type, tokenizer)\nquery = ''\nresp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "llm_engine",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "llm_engine = get_vllm_engine(model_type, model_dir=model_dir)\ntokenizer = llm_engine.tokenizer\ntemplate = get_template(template_type, tokenizer)\nquery = ''\nresp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "tokenizer = llm_engine.tokenizer\ntemplate = get_template(template_type, tokenizer)\nquery = ''\nresp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "template = get_template(template_type, tokenizer)\nquery = ''\nresp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "query",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "query = ''\nresp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "resp",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "resp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "MySQL",
        "kind": 6,
        "importPath": "my_utils.MysqlUtility",
        "description": "my_utils.MysqlUtility",
        "peekOfCode": "class MySQL:\n    u'''MySQLdb'''\n    error_code = ''  # MySQL\n    _instance = None  # \n    _conn = None  # conn\n    _cur = None  # \n    _TIMEOUT = 30  # 30\n    _timecount = 0\n    def __init__(self, dbconfig):\n        u'MySQL'",
        "detail": "my_utils.MysqlUtility",
        "documentation": {}
    },
    {
        "label": "paddstr",
        "kind": 2,
        "importPath": "my_utils.MysqlUtility",
        "description": "my_utils.MysqlUtility",
        "peekOfCode": "def paddstr(strx):\n    return \"NULL\" if strx == None else \"\\'\" + MySQLdb.escape_string(str(strx)).decode() + \"\\'\"\ndef paddnum(numx, default='NULL'):\n    return default if numx == None else str(numx)\nclass MySQL:\n    u'''MySQLdb'''\n    error_code = ''  # MySQL\n    _instance = None  # \n    _conn = None  # conn\n    _cur = None  # ",
        "detail": "my_utils.MysqlUtility",
        "documentation": {}
    },
    {
        "label": "paddnum",
        "kind": 2,
        "importPath": "my_utils.MysqlUtility",
        "description": "my_utils.MysqlUtility",
        "peekOfCode": "def paddnum(numx, default='NULL'):\n    return default if numx == None else str(numx)\nclass MySQL:\n    u'''MySQLdb'''\n    error_code = ''  # MySQL\n    _instance = None  # \n    _conn = None  # conn\n    _cur = None  # \n    _TIMEOUT = 30  # 30\n    _timecount = 0",
        "detail": "my_utils.MysqlUtility",
        "documentation": {}
    },
    {
        "label": "ollama_host",
        "kind": 5,
        "importPath": "ollama.test01",
        "description": "ollama.test01",
        "peekOfCode": "ollama_host = \"localhost\"\n# Ollama\nollama_port = 11434\n# Ollama\nollama_model = \"llama2\"\nfrom langchain.llms import Ollama\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nif __name__ == \"__main__\":\n    # APIOllama",
        "detail": "ollama.test01",
        "documentation": {}
    },
    {
        "label": "ollama_port",
        "kind": 5,
        "importPath": "ollama.test01",
        "description": "ollama.test01",
        "peekOfCode": "ollama_port = 11434\n# Ollama\nollama_model = \"llama2\"\nfrom langchain.llms import Ollama\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nif __name__ == \"__main__\":\n    # APIOllama\n    # StreamingStdOutCallbackHandler \n    llm = Ollama(",
        "detail": "ollama.test01",
        "documentation": {}
    },
    {
        "label": "ollama_model",
        "kind": 5,
        "importPath": "ollama.test01",
        "description": "ollama.test01",
        "peekOfCode": "ollama_model = \"llama2\"\nfrom langchain.llms import Ollama\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nif __name__ == \"__main__\":\n    # APIOllama\n    # StreamingStdOutCallbackHandler \n    llm = Ollama(\n        base_url=f\"http://{ollama_host}:{ollama_port}\",\n        model=ollama_model,",
        "detail": "ollama.test01",
        "documentation": {}
    },
    {
        "label": "SuppressStdout",
        "kind": 6,
        "importPath": "ollama.test02",
        "description": "ollama.test02",
        "peekOfCode": "class SuppressStdout:\n    # __enter__ \n    def __enter__(self):\n        self._original_stdout = sys.stdout\n        self._original_stderr = sys.stderr\n        sys.stdout = open(os.devnull, \"w\")\n        sys.stderr = open(os.devnull, \"w\")\n    # __exit__ \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        sys.stdout.close()",
        "detail": "ollama.test02",
        "documentation": {}
    },
    {
        "label": "ollama_host",
        "kind": 5,
        "importPath": "ollama.test02",
        "description": "ollama.test02",
        "peekOfCode": "ollama_host = \"localhost\"\n# Ollama\nollama_port = 11434\n# Ollama\nollama_model = \"llama2\"\nfrom langchain.document_loaders import OnlinePDFLoader\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import GPT4AllEmbeddings\nfrom langchain import PromptTemplate\nfrom langchain.llms import Ollama",
        "detail": "ollama.test02",
        "documentation": {}
    },
    {
        "label": "ollama_port",
        "kind": 5,
        "importPath": "ollama.test02",
        "description": "ollama.test02",
        "peekOfCode": "ollama_port = 11434\n# Ollama\nollama_model = \"llama2\"\nfrom langchain.document_loaders import OnlinePDFLoader\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import GPT4AllEmbeddings\nfrom langchain import PromptTemplate\nfrom langchain.llms import Ollama\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler",
        "detail": "ollama.test02",
        "documentation": {}
    },
    {
        "label": "ollama_model",
        "kind": 5,
        "importPath": "ollama.test02",
        "description": "ollama.test02",
        "peekOfCode": "ollama_model = \"llama2\"\nfrom langchain.document_loaders import OnlinePDFLoader\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import GPT4AllEmbeddings\nfrom langchain import PromptTemplate\nfrom langchain.llms import Ollama\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.chains import RetrievalQA\nimport sys",
        "detail": "ollama.test02",
        "documentation": {}
    },
    {
        "label": "loader",
        "kind": 5,
        "importPath": "ollama.test02",
        "description": "ollama.test02",
        "peekOfCode": "loader = OnlinePDFLoader(\n    \"https://d18rn0p25nwr6d.cloudfront.net/CIK-0001813756/975b3e9b-268e-4798-a9e4-2a9a7c92dc10.pdf\"\n)\ndata = loader.load()\n# RecursiveCharacterTextSplitter\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n# chunk_size \n# chunk_overlap \ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\nall_splits = text_splitter.split_documents(data)",
        "detail": "ollama.test02",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "ollama.test02",
        "description": "ollama.test02",
        "peekOfCode": "data = loader.load()\n# RecursiveCharacterTextSplitter\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n# chunk_size \n# chunk_overlap \ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\nall_splits = text_splitter.split_documents(data)\n# ChromaGPT4ALL\nwith SuppressStdout():\n    vectorstore = Chroma.from_documents(",
        "detail": "ollama.test02",
        "documentation": {}
    },
    {
        "label": "text_splitter",
        "kind": 5,
        "importPath": "ollama.test02",
        "description": "ollama.test02",
        "peekOfCode": "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\nall_splits = text_splitter.split_documents(data)\n# ChromaGPT4ALL\nwith SuppressStdout():\n    vectorstore = Chroma.from_documents(\n        documents=all_splits, embedding=GPT4AllEmbeddings()\n    )\nwhile True:\n    query = input(\"\\nQuery: \")\n    # ",
        "detail": "ollama.test02",
        "documentation": {}
    },
    {
        "label": "all_splits",
        "kind": 5,
        "importPath": "ollama.test02",
        "description": "ollama.test02",
        "peekOfCode": "all_splits = text_splitter.split_documents(data)\n# ChromaGPT4ALL\nwith SuppressStdout():\n    vectorstore = Chroma.from_documents(\n        documents=all_splits, embedding=GPT4AllEmbeddings()\n    )\nwhile True:\n    query = input(\"\\nQuery: \")\n    # \n    if query == \"exit\":",
        "detail": "ollama.test02",
        "documentation": {}
    },
    {
        "label": "truncate",
        "kind": 2,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "def truncate(q):\n    if q is None:\n        return None\n    size = len(q)\n    return q if size <= 20 else q[0:10] + str(size) + q[size-10:size]\ndef encrypt(signStr):\n    hash_algorithm = hashlib.sha256()\n    hash_algorithm.update(signStr.encode('utf-8'))\n    return hash_algorithm.hexdigest()\ndef do_request(data):",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "encrypt",
        "kind": 2,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "def encrypt(signStr):\n    hash_algorithm = hashlib.sha256()\n    hash_algorithm.update(signStr.encode('utf-8'))\n    return hash_algorithm.hexdigest()\ndef do_request(data):\n    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n    return requests.post(YOUDAO_URL, data=data, headers=headers)\ndef encrypt_tts(signStr):\n    hash_algorithm = hashlib.md5()\n    hash_algorithm.update(signStr.encode('utf-8'))",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "do_request",
        "kind": 2,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "def do_request(data):\n    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n    return requests.post(YOUDAO_URL, data=data, headers=headers)\ndef encrypt_tts(signStr):\n    hash_algorithm = hashlib.md5()\n    hash_algorithm.update(signStr.encode('utf-8'))\n    return hash_algorithm.hexdigest()\ndef do_request_tts(data):\n    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n    return requests.post(YOUDAO_URL_TTS, data=data, headers=headers)",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "encrypt_tts",
        "kind": 2,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "def encrypt_tts(signStr):\n    hash_algorithm = hashlib.md5()\n    hash_algorithm.update(signStr.encode('utf-8'))\n    return hash_algorithm.hexdigest()\ndef do_request_tts(data):\n    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n    return requests.post(YOUDAO_URL_TTS, data=data, headers=headers)\ndef wav_to_text(audio_file_path):\n    lang_type = 'zh-CHS'\n    extension = audio_file_path[audio_file_path.rindex('.')+1:]",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "do_request_tts",
        "kind": 2,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "def do_request_tts(data):\n    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n    return requests.post(YOUDAO_URL_TTS, data=data, headers=headers)\ndef wav_to_text(audio_file_path):\n    lang_type = 'zh-CHS'\n    extension = audio_file_path[audio_file_path.rindex('.')+1:]\n    if extension != 'wav':\n        print('')\n        sys.exit(1)\n    wav_info = wave.open(audio_file_path, 'rb')",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "wav_to_text",
        "kind": 2,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "def wav_to_text(audio_file_path):\n    lang_type = 'zh-CHS'\n    extension = audio_file_path[audio_file_path.rindex('.')+1:]\n    if extension != 'wav':\n        print('')\n        sys.exit(1)\n    wav_info = wave.open(audio_file_path, 'rb')\n    sample_rate = wav_info.getframerate()\n    nchannels = wav_info.getnchannels()\n    wav_info.close()",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "text_to_wav",
        "kind": 2,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "def text_to_wav(text):\n    data = {}\n    data['langType'] = 'zh-CHS'\n    salt = str(uuid.uuid1())\n    signStr = APP_KEY + text + salt + APP_SECRET\n    sign = encrypt_tts(signStr)\n    data['appKey'] = APP_KEY\n    data['q'] = text\n    data['salt'] = salt\n    data['sign'] = sign",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "trans_mp3_to_wav",
        "kind": 2,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "def trans_mp3_to_wav(filepath):\n    song = AudioSegment.from_mp3(filepath)\n    song.export(filepath.replace(\"mp3\",\"wav\"), format=\"wav\")\n    return filepath.replace(\"mp3\",\"wav\")\n# \nFORMAT = pyaudio.paInt16\nCHANNELS = 1\nRATE = 16000 #44100\nCHUNK = 1024\nRECORD_SECONDS = 5",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "client = ZhipuAI(api_key=\"ac92c959bb3b9092c6f2e0da45cb3acf.kSuemdoJcPgFxieY\") # APIKey\nreload(sys)\n#\nYOUDAO_URL = 'https://openapi.youdao.com/asrapi'\nYOUDAO_URL_TTS = 'https://openapi.youdao.com/ttsapi'\nAPP_KEY = '28d5d96858e3caee'\nAPP_SECRET = '3FXrws2dDX3Gq0p1GGc0Ek8bMoDGBl6d'\nWAVE_OUTPUT_FILENAME = \"D:/python_project/emotiVoice/output.wav\"\ndef truncate(q):\n    if q is None:",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "YOUDAO_URL",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "YOUDAO_URL = 'https://openapi.youdao.com/asrapi'\nYOUDAO_URL_TTS = 'https://openapi.youdao.com/ttsapi'\nAPP_KEY = '28d5d96858e3caee'\nAPP_SECRET = '3FXrws2dDX3Gq0p1GGc0Ek8bMoDGBl6d'\nWAVE_OUTPUT_FILENAME = \"D:/python_project/emotiVoice/output.wav\"\ndef truncate(q):\n    if q is None:\n        return None\n    size = len(q)\n    return q if size <= 20 else q[0:10] + str(size) + q[size-10:size]",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "YOUDAO_URL_TTS",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "YOUDAO_URL_TTS = 'https://openapi.youdao.com/ttsapi'\nAPP_KEY = '28d5d96858e3caee'\nAPP_SECRET = '3FXrws2dDX3Gq0p1GGc0Ek8bMoDGBl6d'\nWAVE_OUTPUT_FILENAME = \"D:/python_project/emotiVoice/output.wav\"\ndef truncate(q):\n    if q is None:\n        return None\n    size = len(q)\n    return q if size <= 20 else q[0:10] + str(size) + q[size-10:size]\ndef encrypt(signStr):",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "APP_KEY",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "APP_KEY = '28d5d96858e3caee'\nAPP_SECRET = '3FXrws2dDX3Gq0p1GGc0Ek8bMoDGBl6d'\nWAVE_OUTPUT_FILENAME = \"D:/python_project/emotiVoice/output.wav\"\ndef truncate(q):\n    if q is None:\n        return None\n    size = len(q)\n    return q if size <= 20 else q[0:10] + str(size) + q[size-10:size]\ndef encrypt(signStr):\n    hash_algorithm = hashlib.sha256()",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "APP_SECRET",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "APP_SECRET = '3FXrws2dDX3Gq0p1GGc0Ek8bMoDGBl6d'\nWAVE_OUTPUT_FILENAME = \"D:/python_project/emotiVoice/output.wav\"\ndef truncate(q):\n    if q is None:\n        return None\n    size = len(q)\n    return q if size <= 20 else q[0:10] + str(size) + q[size-10:size]\ndef encrypt(signStr):\n    hash_algorithm = hashlib.sha256()\n    hash_algorithm.update(signStr.encode('utf-8'))",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "WAVE_OUTPUT_FILENAME",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "WAVE_OUTPUT_FILENAME = \"D:/python_project/emotiVoice/output.wav\"\ndef truncate(q):\n    if q is None:\n        return None\n    size = len(q)\n    return q if size <= 20 else q[0:10] + str(size) + q[size-10:size]\ndef encrypt(signStr):\n    hash_algorithm = hashlib.sha256()\n    hash_algorithm.update(signStr.encode('utf-8'))\n    return hash_algorithm.hexdigest()",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "FORMAT",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "FORMAT = pyaudio.paInt16\nCHANNELS = 1\nRATE = 16000 #44100\nCHUNK = 1024\nRECORD_SECONDS = 5\nWAVE_OUTPUT_FILENAME = WAVE_OUTPUT_FILENAME #\"output.wav\"\n# PyAudio\naudio = pyaudio.PyAudio()\n# \nstream = audio.open(format=FORMAT,",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "CHANNELS",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "CHANNELS = 1\nRATE = 16000 #44100\nCHUNK = 1024\nRECORD_SECONDS = 5\nWAVE_OUTPUT_FILENAME = WAVE_OUTPUT_FILENAME #\"output.wav\"\n# PyAudio\naudio = pyaudio.PyAudio()\n# \nstream = audio.open(format=FORMAT,\n                    channels=CHANNELS,",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "RATE",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "RATE = 16000 #44100\nCHUNK = 1024\nRECORD_SECONDS = 5\nWAVE_OUTPUT_FILENAME = WAVE_OUTPUT_FILENAME #\"output.wav\"\n# PyAudio\naudio = pyaudio.PyAudio()\n# \nstream = audio.open(format=FORMAT,\n                    channels=CHANNELS,\n                    rate=RATE,",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "CHUNK",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "CHUNK = 1024\nRECORD_SECONDS = 5\nWAVE_OUTPUT_FILENAME = WAVE_OUTPUT_FILENAME #\"output.wav\"\n# PyAudio\naudio = pyaudio.PyAudio()\n# \nstream = audio.open(format=FORMAT,\n                    channels=CHANNELS,\n                    rate=RATE,\n                    input=True,",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "RECORD_SECONDS",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "RECORD_SECONDS = 5\nWAVE_OUTPUT_FILENAME = WAVE_OUTPUT_FILENAME #\"output.wav\"\n# PyAudio\naudio = pyaudio.PyAudio()\n# \nstream = audio.open(format=FORMAT,\n                    channels=CHANNELS,\n                    rate=RATE,\n                    input=True,\n                    frames_per_buffer=CHUNK)",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "WAVE_OUTPUT_FILENAME",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "WAVE_OUTPUT_FILENAME = WAVE_OUTPUT_FILENAME #\"output.wav\"\n# PyAudio\naudio = pyaudio.PyAudio()\n# \nstream = audio.open(format=FORMAT,\n                    channels=CHANNELS,\n                    rate=RATE,\n                    input=True,\n                    frames_per_buffer=CHUNK)\nprint(\"...\")",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "audio",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "audio = pyaudio.PyAudio()\n# \nstream = audio.open(format=FORMAT,\n                    channels=CHANNELS,\n                    rate=RATE,\n                    input=True,\n                    frames_per_buffer=CHUNK)\nprint(\"...\")\n# \nframes = []",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "stream",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "stream = audio.open(format=FORMAT,\n                    channels=CHANNELS,\n                    rate=RATE,\n                    input=True,\n                    frames_per_buffer=CHUNK)\nprint(\"...\")\n# \nframes = []\nfor i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n    data = stream.read(CHUNK)",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "frames",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "frames = []\nfor i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n    data = stream.read(CHUNK)\n    frames.append(data)\nprint(\"...\")\n# \nstream.stop_stream()\nstream.close()\naudio.terminate()\n# .wav",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "waveFile",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "waveFile = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\nwaveFile.setnchannels(CHANNELS)\nwaveFile.setsampwidth(audio.get_sample_size(FORMAT))\nwaveFile.setframerate(RATE)\nwaveFile.writeframes(b''.join(frames))\nwaveFile.close()\n# \ntext = wav_to_text(WAVE_OUTPUT_FILENAME)\n# \nresponse = client.chat.completions.create(",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "text = wav_to_text(WAVE_OUTPUT_FILENAME)\n# \nresponse = client.chat.completions.create(\n    model=\"glm-4\",  # \n    messages=[\n        {\"role\": \"user\", \"content\": f\"{text}\"}\n    ]\n)\nprint(response.choices[0].message.content)\nprint(response.usage.total_tokens)",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "response = client.chat.completions.create(\n    model=\"glm-4\",  # \n    messages=[\n        {\"role\": \"user\", \"content\": f\"{text}\"}\n    ]\n)\nprint(response.choices[0].message.content)\nprint(response.usage.total_tokens)\n# \noutput = text_to_wav(response.choices[0].message.content)",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "output = text_to_wav(response.choices[0].message.content)\nprint(output)\n# \n# output = trans_mp3_to_wav(output)\nfrom playsound import playsound\nplaysound(output)",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "ollama.test04",
        "description": "ollama.test04",
        "peekOfCode": "client = Groq(\n    # api_key=os.environ.get(\"GROQ_API_KEY\"),\n    api_key=\"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\",\n)\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Explain the importance of low latency LLMs\",\n        }",
        "detail": "ollama.test04",
        "documentation": {}
    },
    {
        "label": "chat_completion",
        "kind": 5,
        "importPath": "ollama.test04",
        "description": "ollama.test04",
        "peekOfCode": "chat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Explain the importance of low latency LLMs\",\n        }\n    ],\n    model=\"mixtral-8x7b-32768\",\n)\nprint(chat_completion.choices[0].message.content)",
        "detail": "ollama.test04",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "openai.openai_1_3_7_azure_Test",
        "description": "openai.openai_1_3_7_azure_Test",
        "peekOfCode": "client = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\",\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://glsk-openai-ce.openai.azure.com\",\n    api_key=\"8ba61036fe7847c7be4ebaaf58b85275\"\n)\ncompletion = client.chat.completions.create(\n    model=\"gpt-35-turbo-1106\",  # e.g. gpt-35-instant\n    messages=[",
        "detail": "openai.openai_1_3_7_azure_Test",
        "documentation": {}
    },
    {
        "label": "completion",
        "kind": 5,
        "importPath": "openai.openai_1_3_7_azure_Test",
        "description": "openai.openai_1_3_7_azure_Test",
        "peekOfCode": "completion = client.chat.completions.create(\n    model=\"gpt-35-turbo-1106\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"212json\",\n        },\n    ],\n)\nprint(completion.usage)",
        "detail": "openai.openai_1_3_7_azure_Test",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "openai.test01",
        "description": "openai.test01",
        "peekOfCode": "client = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\",\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://glsk-openai-ce.openai.azure.com\",\n    api_key=\"8ba61036fe7847c7be4ebaaf58b85275\",\n)\nprompt = \"opencv\"\ncompletion = client.chat.completions.create(\n    # model=\"gpt-35-turbo-1106\",  # e.g. gpt-35-instant",
        "detail": "openai.test01",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "openai.test01",
        "description": "openai.test01",
        "peekOfCode": "prompt = \"opencv\"\ncompletion = client.chat.completions.create(\n    # model=\"gpt-35-turbo-1106\",  # e.g. gpt-35-instant\n    model=\"gpt-4-1106\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": prompt,\n        },\n    ],",
        "detail": "openai.test01",
        "documentation": {}
    },
    {
        "label": "completion",
        "kind": 5,
        "importPath": "openai.test01",
        "description": "openai.test01",
        "peekOfCode": "completion = client.chat.completions.create(\n    # model=\"gpt-35-turbo-1106\",  # e.g. gpt-35-instant\n    model=\"gpt-4-1106\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": prompt,\n        },\n    ],\n)",
        "detail": "openai.test01",
        "documentation": {}
    },
    {
        "label": "openai_api_key",
        "kind": 5,
        "importPath": "openai.test02",
        "description": "openai.test02",
        "peekOfCode": "openai_api_key = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\nopenai_api_base = \"https://oneapi.365jpshop.com/v1\"\n# Set up request parameters\nmodel_id = \"code-davinci-002\"\nprompt = \"Write a Python function to calculate the factorial of a number.\"\nmax_tokens = 100\ntemperature = 0.7\n# Construct request body\nheaders = {\n    \"Content-Type\": \"application/json\",",
        "detail": "openai.test02",
        "documentation": {}
    },
    {
        "label": "openai_api_base",
        "kind": 5,
        "importPath": "openai.test02",
        "description": "openai.test02",
        "peekOfCode": "openai_api_base = \"https://oneapi.365jpshop.com/v1\"\n# Set up request parameters\nmodel_id = \"code-davinci-002\"\nprompt = \"Write a Python function to calculate the factorial of a number.\"\nmax_tokens = 100\ntemperature = 0.7\n# Construct request body\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {openai_api_key}\",",
        "detail": "openai.test02",
        "documentation": {}
    },
    {
        "label": "model_id",
        "kind": 5,
        "importPath": "openai.test02",
        "description": "openai.test02",
        "peekOfCode": "model_id = \"code-davinci-002\"\nprompt = \"Write a Python function to calculate the factorial of a number.\"\nmax_tokens = 100\ntemperature = 0.7\n# Construct request body\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {openai_api_key}\",\n}\ndata = {",
        "detail": "openai.test02",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "openai.test02",
        "description": "openai.test02",
        "peekOfCode": "prompt = \"Write a Python function to calculate the factorial of a number.\"\nmax_tokens = 100\ntemperature = 0.7\n# Construct request body\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {openai_api_key}\",\n}\ndata = {\n    \"model\": model_id,",
        "detail": "openai.test02",
        "documentation": {}
    },
    {
        "label": "max_tokens",
        "kind": 5,
        "importPath": "openai.test02",
        "description": "openai.test02",
        "peekOfCode": "max_tokens = 100\ntemperature = 0.7\n# Construct request body\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {openai_api_key}\",\n}\ndata = {\n    \"model\": model_id,\n    \"prompt\": prompt,",
        "detail": "openai.test02",
        "documentation": {}
    },
    {
        "label": "temperature",
        "kind": 5,
        "importPath": "openai.test02",
        "description": "openai.test02",
        "peekOfCode": "temperature = 0.7\n# Construct request body\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {openai_api_key}\",\n}\ndata = {\n    \"model\": model_id,\n    \"prompt\": prompt,\n    \"max_tokens\": max_tokens,",
        "detail": "openai.test02",
        "documentation": {}
    },
    {
        "label": "headers",
        "kind": 5,
        "importPath": "openai.test02",
        "description": "openai.test02",
        "peekOfCode": "headers = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {openai_api_key}\",\n}\ndata = {\n    \"model\": model_id,\n    \"prompt\": prompt,\n    \"max_tokens\": max_tokens,\n    \"temperature\": temperature,\n}",
        "detail": "openai.test02",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "openai.test02",
        "description": "openai.test02",
        "peekOfCode": "data = {\n    \"model\": model_id,\n    \"prompt\": prompt,\n    \"max_tokens\": max_tokens,\n    \"temperature\": temperature,\n}\n# Send request and handle response\nresponse = requests.post(openai_api_base + \"completions\", headers=headers, json=data)\nif response.status_code == 200:\n    result = response.json()",
        "detail": "openai.test02",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "openai.test02",
        "description": "openai.test02",
        "peekOfCode": "response = requests.post(openai_api_base + \"completions\", headers=headers, json=data)\nif response.status_code == 200:\n    result = response.json()\n    print(result[\"choices\"][0][\"text\"])\nelse:\n    print(f\"Request failed with status code {response.status_code}\")",
        "detail": "openai.test02",
        "documentation": {}
    },
    {
        "label": "tsc",
        "kind": 2,
        "importPath": "openai.test03",
        "description": "openai.test03",
        "peekOfCode": "def tsc(inp: str, nn: int) -> str:\n    # nn0my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1\n        text = (\n            \"10\"\n            + inp\n            + \"\"",
        "detail": "openai.test03",
        "documentation": {}
    },
    {
        "label": "tzh",
        "kind": 2,
        "importPath": "openai.test03",
        "description": "openai.test03",
        "peekOfCode": "def tzh():\n    text = \"20\"\n    response = llm.invoke(input=text)\n    array = np.array(response.content.split(\",\"))\n    print(array)\n    for key in array:\n        arr = tsc(key, 10)\n        arr = np.array(arr.split(\",\"))\n        global my_array\n        my_array = np.append(my_array, arr)",
        "detail": "openai.test03",
        "documentation": {}
    },
    {
        "label": "jiesh",
        "kind": 2,
        "importPath": "openai.test03",
        "description": "openai.test03",
        "peekOfCode": "def jiesh():\n    arr = tzh()\n    #     arr = np.array(arr.split(','))\n    for a in arr:\n        text = \"\" + a + \"200\"\n        # llm = OpenAIChat(temperature=0.9, model_name=model)\n        llm = ChatOpenAI(temperature=0.9, model_name=model)\n        response = llm.invoke(input=text)\n        response_content = a + \"\\n\" + response.content\n        print(response_content)",
        "detail": "openai.test03",
        "documentation": {}
    },
    {
        "label": "openai.api_base",
        "kind": 5,
        "importPath": "openai.test03",
        "description": "openai.test03",
        "peekOfCode": "openai.api_base = \"https://oneapi.365jpshop.com/v1\"\n# from config import *\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# os.environ[\"OPENAI_API_KEY\"] = \"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\"#key\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nimport time\n# model = \"chatglm3\"\n# model = \"gemini-pro\"\nfrom datetime import datetime",
        "detail": "openai.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "openai.test03",
        "description": "openai.test03",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# os.environ[\"OPENAI_API_KEY\"] = \"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\"#key\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nimport time\n# model = \"chatglm3\"\n# model = \"gemini-pro\"\nfrom datetime import datetime\nimport numpy as np\n# from langchain.llms import OpenAI\n# from langchain_community.llms import OpenAIChat",
        "detail": "openai.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "openai.test03",
        "description": "openai.test03",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nimport time\n# model = \"chatglm3\"\n# model = \"gemini-pro\"\nfrom datetime import datetime\nimport numpy as np\n# from langchain.llms import OpenAI\n# from langchain_community.llms import OpenAIChat\nfrom langchain_openai import ChatOpenAI\n# from langchain_community.chat_models import ChatOpenAI",
        "detail": "openai.test03",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "openai.test03",
        "description": "openai.test03",
        "peekOfCode": "model = \"gpt-4-1106-preview\"\n# llm = OpenAIChat(temperature=0.9, model_name=model)\nllm = ChatOpenAI(\n    temperature=0.9,\n    model_name=model,\n    api_key=\"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\",\n    base_url=\"https://oneapi.365jpshop.com/v1\",\n)\n# client = OpenAIChat()\n# llm = OpenAI(temperature=0.9,max_tokens=1000)",
        "detail": "openai.test03",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "openai.test03",
        "description": "openai.test03",
        "peekOfCode": "llm = ChatOpenAI(\n    temperature=0.9,\n    model_name=model,\n    api_key=\"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\",\n    base_url=\"https://oneapi.365jpshop.com/v1\",\n)\n# client = OpenAIChat()\n# llm = OpenAI(temperature=0.9,max_tokens=1000)\nnn = 5\nmy_array = np.array([])",
        "detail": "openai.test03",
        "documentation": {}
    },
    {
        "label": "nn",
        "kind": 5,
        "importPath": "openai.test03",
        "description": "openai.test03",
        "peekOfCode": "nn = 5\nmy_array = np.array([])\njiesh_array = np.array([])\n# nn\n# \ndef tsc(inp: str, nn: int) -> str:\n    # nn0my_array\n    if nn == 0:\n        return my_array\n    else:",
        "detail": "openai.test03",
        "documentation": {}
    },
    {
        "label": "my_array",
        "kind": 5,
        "importPath": "openai.test03",
        "description": "openai.test03",
        "peekOfCode": "my_array = np.array([])\njiesh_array = np.array([])\n# nn\n# \ndef tsc(inp: str, nn: int) -> str:\n    # nn0my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1",
        "detail": "openai.test03",
        "documentation": {}
    },
    {
        "label": "jiesh_array",
        "kind": 5,
        "importPath": "openai.test03",
        "description": "openai.test03",
        "peekOfCode": "jiesh_array = np.array([])\n# nn\n# \ndef tsc(inp: str, nn: int) -> str:\n    # nn0my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1\n        text = (",
        "detail": "openai.test03",
        "documentation": {}
    },
    {
        "label": "chat",
        "kind": 5,
        "importPath": "openai.test04",
        "description": "openai.test04",
        "peekOfCode": "chat = ChatOpenAI(\n    temperature=0.9,\n    model_name=\"gpt-4-1106-preview\",\n    api_key=\"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\",\n    base_url=\"https://oneapi.365jpshop.com/v1\",\n)\n# Use the chat object to generate a response to a prompt\n# response = chat.invoke(input=\"212json\")\nresponse = chat.invoke(\n    input=\",,,\"",
        "detail": "openai.test04",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "openai.test04",
        "description": "openai.test04",
        "peekOfCode": "response = chat.invoke(\n    input=\",,,\"\n    # input=\"\"\n    # input=\"212json\"\n)\nprint(response.content)",
        "detail": "openai.test04",
        "documentation": {}
    },
    {
        "label": "tsc",
        "kind": 2,
        "importPath": "openai.test05",
        "description": "openai.test05",
        "peekOfCode": "def tsc(inp: str, nn: int) -> str:\n    # nn0my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1\n        text = (\n            \"10\"\n            + inp\n            + \"\"",
        "detail": "openai.test05",
        "documentation": {}
    },
    {
        "label": "tzh",
        "kind": 2,
        "importPath": "openai.test05",
        "description": "openai.test05",
        "peekOfCode": "def tzh():\n    # text = \"20\"\n    text = \",,,\"\n    response = llm.invoke(input=text)\n    array = np.array(response.content.split(\",\"))\n    print(array)\n    for key in array:\n        arr = tsc(key, 10)\n        arr = np.array(arr.split(\",\"))\n        global my_array",
        "detail": "openai.test05",
        "documentation": {}
    },
    {
        "label": "jiesh",
        "kind": 2,
        "importPath": "openai.test05",
        "description": "openai.test05",
        "peekOfCode": "def jiesh():\n    arr = tzh()\n    #     arr = np.array(arr.split(','))\n    for a in arr:\n        text = \"\" + a + \"50\"\n        # llm = OpenAIChat(temperature=0.9, model_name=model)\n        llm = ChatOpenAI(temperature=0.9, model_name=model)\n        response = llm.invoke(input=text)\n        response_content = a + \"\\n\" + response.content\n        print(response_content)",
        "detail": "openai.test05",
        "documentation": {}
    },
    {
        "label": "openai.api_base",
        "kind": 5,
        "importPath": "openai.test05",
        "description": "openai.test05",
        "peekOfCode": "openai.api_base = \"https://oneapi.365jpshop.com/v1\"\n# from config import *\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# os.environ[\"OPENAI_API_KEY\"] = \"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\"#key\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nimport time\n# model = \"chatglm3\"\n# model = \"gemini-pro\"\nfrom datetime import datetime",
        "detail": "openai.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "openai.test05",
        "description": "openai.test05",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# os.environ[\"OPENAI_API_KEY\"] = \"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\"#key\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nimport time\n# model = \"chatglm3\"\n# model = \"gemini-pro\"\nfrom datetime import datetime\nimport numpy as np\n# from langchain.llms import OpenAI\n# from langchain_community.llms import OpenAIChat",
        "detail": "openai.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "openai.test05",
        "description": "openai.test05",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nimport time\n# model = \"chatglm3\"\n# model = \"gemini-pro\"\nfrom datetime import datetime\nimport numpy as np\n# from langchain.llms import OpenAI\n# from langchain_community.llms import OpenAIChat\nfrom langchain_openai import ChatOpenAI\n# from langchain_community.chat_models import ChatOpenAI",
        "detail": "openai.test05",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "openai.test05",
        "description": "openai.test05",
        "peekOfCode": "model = \"gpt-4-1106-preview\"\n# llm = OpenAIChat(temperature=0.9, model_name=model)\nllm = ChatOpenAI(\n    temperature=0.9,\n    model_name=model,\n    api_key=\"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\",\n    base_url=\"https://oneapi.365jpshop.com/v1\",\n)\n# client = OpenAIChat()\n# llm = OpenAI(temperature=0.9,max_tokens=1000)",
        "detail": "openai.test05",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "openai.test05",
        "description": "openai.test05",
        "peekOfCode": "llm = ChatOpenAI(\n    temperature=0.9,\n    model_name=model,\n    api_key=\"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\",\n    base_url=\"https://oneapi.365jpshop.com/v1\",\n)\n# client = OpenAIChat()\n# llm = OpenAI(temperature=0.9,max_tokens=1000)\nnn = 5\nmy_array = np.array([])",
        "detail": "openai.test05",
        "documentation": {}
    },
    {
        "label": "nn",
        "kind": 5,
        "importPath": "openai.test05",
        "description": "openai.test05",
        "peekOfCode": "nn = 5\nmy_array = np.array([])\njiesh_array = np.array([])\n# nn\n# \ndef tsc(inp: str, nn: int) -> str:\n    # nn0my_array\n    if nn == 0:\n        return my_array\n    else:",
        "detail": "openai.test05",
        "documentation": {}
    },
    {
        "label": "my_array",
        "kind": 5,
        "importPath": "openai.test05",
        "description": "openai.test05",
        "peekOfCode": "my_array = np.array([])\njiesh_array = np.array([])\n# nn\n# \ndef tsc(inp: str, nn: int) -> str:\n    # nn0my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1",
        "detail": "openai.test05",
        "documentation": {}
    },
    {
        "label": "jiesh_array",
        "kind": 5,
        "importPath": "openai.test05",
        "description": "openai.test05",
        "peekOfCode": "jiesh_array = np.array([])\n# nn\n# \ndef tsc(inp: str, nn: int) -> str:\n    # nn0my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1\n        text = (",
        "detail": "openai.test05",
        "documentation": {}
    },
    {
        "label": "tsc",
        "kind": 2,
        "importPath": "openai.zhishi",
        "description": "openai.zhishi",
        "peekOfCode": "def tsc(inp: str, nn: int) -> str:\n    # nn0my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1\n        text = (\n            \"10\"\n            + inp\n            + \"\"",
        "detail": "openai.zhishi",
        "documentation": {}
    },
    {
        "label": "tzh",
        "kind": 2,
        "importPath": "openai.zhishi",
        "description": "openai.zhishi",
        "peekOfCode": "def tzh():\n    text = \"20\"\n    response = llm(text)\n    array = np.array(response.split(\",\"))\n    print(array)\n    for key in array:\n        arr = tsc(key, 10)\n        arr = np.array(arr.split(\",\"))\n        global my_array\n        my_array = np.append(my_array, arr)",
        "detail": "openai.zhishi",
        "documentation": {}
    },
    {
        "label": "jiesh",
        "kind": 2,
        "importPath": "openai.zhishi",
        "description": "openai.zhishi",
        "peekOfCode": "def jiesh():\n    arr = tzh()\n    #     arr = np.array(arr.split(','))\n    for a in arr:\n        text = \"\" + a + \"200\"\n        llm = OpenAIChat(temperature=0.9, model_name=model)\n        response = llm(text)\n        #         time.sleep(5)\n        con = response\n        response = a + \"\\n\" + response",
        "detail": "openai.zhishi",
        "documentation": {}
    },
    {
        "label": "openai.api_base",
        "kind": 5,
        "importPath": "openai.zhishi",
        "description": "openai.zhishi",
        "peekOfCode": "openai.api_base = \"https://oneapi.365jpshop.com/v1\"\n# from config import *\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# os.environ[\"OPENAI_API_KEY\"] = \"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\"#key\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nimport time\n# model = \"chatglm3\"\n# model = \"gemini-pro\"\n# import mysql.connector",
        "detail": "openai.zhishi",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "openai.zhishi",
        "description": "openai.zhishi",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# os.environ[\"OPENAI_API_KEY\"] = \"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\"#key\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nimport time\n# model = \"chatglm3\"\n# model = \"gemini-pro\"\n# import mysql.connector\nfrom datetime import datetime\nimport numpy as np\n# from langchain.llms import OpenAI",
        "detail": "openai.zhishi",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "openai.zhishi",
        "description": "openai.zhishi",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nimport time\n# model = \"chatglm3\"\n# model = \"gemini-pro\"\n# import mysql.connector\nfrom datetime import datetime\nimport numpy as np\n# from langchain.llms import OpenAI\nfrom langchain_community.llms import OpenAIChat\n# from langchain_community.chat_models import ChatOpenAI",
        "detail": "openai.zhishi",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "openai.zhishi",
        "description": "openai.zhishi",
        "peekOfCode": "model = \"gpt-4-1106-preview\"\nllm = OpenAIChat(temperature=0.9, model_name=model)\n# client = OpenAIChat()\n# llm = OpenAI(temperature=0.9,max_tokens=1000)\nnn = 5\nmy_array = np.array([])\njiesh_array = np.array([])\n# nn\n# \ndef tsc(inp: str, nn: int) -> str:",
        "detail": "openai.zhishi",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "openai.zhishi",
        "description": "openai.zhishi",
        "peekOfCode": "llm = OpenAIChat(temperature=0.9, model_name=model)\n# client = OpenAIChat()\n# llm = OpenAI(temperature=0.9,max_tokens=1000)\nnn = 5\nmy_array = np.array([])\njiesh_array = np.array([])\n# nn\n# \ndef tsc(inp: str, nn: int) -> str:\n    # nn0my_array",
        "detail": "openai.zhishi",
        "documentation": {}
    },
    {
        "label": "nn",
        "kind": 5,
        "importPath": "openai.zhishi",
        "description": "openai.zhishi",
        "peekOfCode": "nn = 5\nmy_array = np.array([])\njiesh_array = np.array([])\n# nn\n# \ndef tsc(inp: str, nn: int) -> str:\n    # nn0my_array\n    if nn == 0:\n        return my_array\n    else:",
        "detail": "openai.zhishi",
        "documentation": {}
    },
    {
        "label": "my_array",
        "kind": 5,
        "importPath": "openai.zhishi",
        "description": "openai.zhishi",
        "peekOfCode": "my_array = np.array([])\njiesh_array = np.array([])\n# nn\n# \ndef tsc(inp: str, nn: int) -> str:\n    # nn0my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1",
        "detail": "openai.zhishi",
        "documentation": {}
    },
    {
        "label": "jiesh_array",
        "kind": 5,
        "importPath": "openai.zhishi",
        "description": "openai.zhishi",
        "peekOfCode": "jiesh_array = np.array([])\n# nn\n# \ndef tsc(inp: str, nn: int) -> str:\n    # nn0my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1\n        text = (",
        "detail": "openai.zhishi",
        "documentation": {}
    },
    {
        "label": "audio",
        "kind": 5,
        "importPath": "paddlespeech.test01",
        "description": "paddlespeech.test01",
        "peekOfCode": "audio = \"zh.wav\"\nasr = ASRExecutor()\nresult = asr(audio_file=audio, model=\"conformer_online_wenetspeech\")\nprint(result)",
        "detail": "paddlespeech.test01",
        "documentation": {}
    },
    {
        "label": "asr",
        "kind": 5,
        "importPath": "paddlespeech.test01",
        "description": "paddlespeech.test01",
        "peekOfCode": "asr = ASRExecutor()\nresult = asr(audio_file=audio, model=\"conformer_online_wenetspeech\")\nprint(result)",
        "detail": "paddlespeech.test01",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "paddlespeech.test01",
        "description": "paddlespeech.test01",
        "peekOfCode": "result = asr(audio_file=audio, model=\"conformer_online_wenetspeech\")\nprint(result)",
        "detail": "paddlespeech.test01",
        "documentation": {}
    },
    {
        "label": "tts_executor",
        "kind": 5,
        "importPath": "paddlespeech.test02",
        "description": "paddlespeech.test02",
        "peekOfCode": "tts_executor = TTSExecutor()\ntts_executor(text=\"\", output=\"output.wav\")\nfrom paddlespeech.cli.audio import AudioExecutor\naudio_executor = AudioExecutor()\naudio_executor(input=\"output.wav\", output=\"output01.wav\", effect=\"pitch\")",
        "detail": "paddlespeech.test02",
        "documentation": {}
    },
    {
        "label": "audio_executor",
        "kind": 5,
        "importPath": "paddlespeech.test02",
        "description": "paddlespeech.test02",
        "peekOfCode": "audio_executor = AudioExecutor()\naudio_executor(input=\"output.wav\", output=\"output01.wav\", effect=\"pitch\")",
        "detail": "paddlespeech.test02",
        "documentation": {}
    },
    {
        "label": "play_audio",
        "kind": 2,
        "importPath": "pyaudio.test01",
        "description": "pyaudio.test01",
        "peekOfCode": "def play_audio(wave_path):\n    CHUNK = 1024\n    wf = wave.open(wave_path, 'rb')\n    # instantiate PyAudio (1)\n    p = pyaudio.PyAudio()\n    # open stream (2)\n    stream = p.open(format=p.get_format_from_width(wf.getsampwidth()),\n                    channels=wf.getnchannels(),\n                    rate=wf.getframerate(),\n                    output=True)",
        "detail": "pyaudio.test01",
        "documentation": {}
    },
    {
        "label": "play",
        "kind": 2,
        "importPath": "pyaudio.test02",
        "description": "pyaudio.test02",
        "peekOfCode": "def play():\n    chunk = 1024\n    wf = wave.open(r\".wav\", 'rb')\n    p = pyaudio.PyAudio()\n    stream = p.open(format=p.get_format_from_width(wf.getsampwidth()), channels=wf.getnchannels(),\n                    rate=wf.getframerate(), output=True)\n    data = wf.readframes(chunk)  # \n    print(data)\n    while data != b'':  # \n        stream.write(data)",
        "detail": "pyaudio.test02",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 2,
        "importPath": "suanfa.main01",
        "description": "suanfa.main01",
        "peekOfCode": "def name(args):\n    print(\"Hello World\" + args)\ndef bubble_sort(arr):\n    n = len(arr)\n    for i in range(n - 1):\n        # \n        for j in range(n - 1 - i):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\ndef my_bubble_sort(arr):",
        "detail": "suanfa.main01",
        "documentation": {}
    },
    {
        "label": "bubble_sort",
        "kind": 2,
        "importPath": "suanfa.main01",
        "description": "suanfa.main01",
        "peekOfCode": "def bubble_sort(arr):\n    n = len(arr)\n    for i in range(n - 1):\n        # \n        for j in range(n - 1 - i):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\ndef my_bubble_sort(arr):\n    n = len(arr)\n    for i in range(n - 1):",
        "detail": "suanfa.main01",
        "documentation": {}
    },
    {
        "label": "my_bubble_sort",
        "kind": 2,
        "importPath": "suanfa.main01",
        "description": "suanfa.main01",
        "peekOfCode": "def my_bubble_sort(arr):\n    n = len(arr)\n    for i in range(n - 1):\n        for j in range(n - 1 - i):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\ndef insertion_sort(arr):\n    n = len(arr)\n    for i in range(1, n):\n        key = arr[i]",
        "detail": "suanfa.main01",
        "documentation": {}
    },
    {
        "label": "insertion_sort",
        "kind": 2,
        "importPath": "suanfa.main01",
        "description": "suanfa.main01",
        "peekOfCode": "def insertion_sort(arr):\n    n = len(arr)\n    for i in range(1, n):\n        key = arr[i]\n        j = i - 1\n        # \n        while j >= 0 and arr[j] > key:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key",
        "detail": "suanfa.main01",
        "documentation": {}
    },
    {
        "label": "reader",
        "kind": 5,
        "importPath": "suanfa.mian02",
        "description": "suanfa.mian02",
        "peekOfCode": "reader = easyocr.Reader(\n    [\"en\", \"ch_sim\"],\n    gpu=True,\n)  # this needs to run only once to load the model into memory\n# \nimage_path = \"./15889317528424945.png\"\nimage_path_out = \"./results/15889317528424945.png\"\nimage = Image.open(image_path)\n# image = image.resize((800, 600))\n# image = image.resize((800, 300))",
        "detail": "suanfa.mian02",
        "documentation": {}
    },
    {
        "label": "image_path",
        "kind": 5,
        "importPath": "suanfa.mian02",
        "description": "suanfa.mian02",
        "peekOfCode": "image_path = \"./15889317528424945.png\"\nimage_path_out = \"./results/15889317528424945.png\"\nimage = Image.open(image_path)\n# image = image.resize((800, 600))\n# image = image.resize((800, 300))\n# image.show()\nimage.save(image_path_out)\n# result = reader.readtext(\"./image/baidu_image/test5.jpg\", detail=0)\nresults = reader.readtext(image_path_out, detail=0, batch_size=20, paragraph=True)\nprint(\"results==\", results)",
        "detail": "suanfa.mian02",
        "documentation": {}
    },
    {
        "label": "image_path_out",
        "kind": 5,
        "importPath": "suanfa.mian02",
        "description": "suanfa.mian02",
        "peekOfCode": "image_path_out = \"./results/15889317528424945.png\"\nimage = Image.open(image_path)\n# image = image.resize((800, 600))\n# image = image.resize((800, 300))\n# image.show()\nimage.save(image_path_out)\n# result = reader.readtext(\"./image/baidu_image/test5.jpg\", detail=0)\nresults = reader.readtext(image_path_out, detail=0, batch_size=20, paragraph=True)\nprint(\"results==\", results)\nfor result in results:",
        "detail": "suanfa.mian02",
        "documentation": {}
    },
    {
        "label": "image",
        "kind": 5,
        "importPath": "suanfa.mian02",
        "description": "suanfa.mian02",
        "peekOfCode": "image = Image.open(image_path)\n# image = image.resize((800, 600))\n# image = image.resize((800, 300))\n# image.show()\nimage.save(image_path_out)\n# result = reader.readtext(\"./image/baidu_image/test5.jpg\", detail=0)\nresults = reader.readtext(image_path_out, detail=0, batch_size=20, paragraph=True)\nprint(\"results==\", results)\nfor result in results:\n    print(\"result\", result)",
        "detail": "suanfa.mian02",
        "documentation": {}
    },
    {
        "label": "results",
        "kind": 5,
        "importPath": "suanfa.mian02",
        "description": "suanfa.mian02",
        "peekOfCode": "results = reader.readtext(image_path_out, detail=0, batch_size=20, paragraph=True)\nprint(\"results==\", results)\nfor result in results:\n    print(\"result\", result)",
        "detail": "suanfa.mian02",
        "documentation": {}
    },
    {
        "label": "ocr",
        "kind": 5,
        "importPath": "suanfa.mian03",
        "description": "suanfa.mian03",
        "peekOfCode": "ocr = PaddleOCR(\n    lang=\"ch\",\n    # use_gpu=False,\n    use_gpu=True,\n    det_model_dir=\"../paddleORC_model/ch_ppocr_server_v2.0_det_infer/\",\n    cls_model_dir=\"ch_ppocr_mobile_v2.0_cls_infer/\",\n    rec_model_dir=\"ch_ppocr_server_v2.0_rec_infer/\",\n)\n# load dataset\n# img_path = \"./08631508_2.jpg\"",
        "detail": "suanfa.mian03",
        "documentation": {}
    },
    {
        "label": "img_path",
        "kind": 5,
        "importPath": "suanfa.mian03",
        "description": "suanfa.mian03",
        "peekOfCode": "img_path = \"./image/baidu_image/test7.jpg\"\nresult = ocr.ocr(img_path, cls=True)\nfor line in result:\n    print(line)\n    print(line[0][1][0])\n# \n# resultlistitem\n# line\n# [[[3.0, 149.0], [43.0, 149.0], [43.0, 163.0], [3.0, 163.0]], ('', 0.6762619018554688)]\n#  boxes = line[0](x,y)",
        "detail": "suanfa.mian03",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "suanfa.mian03",
        "description": "suanfa.mian03",
        "peekOfCode": "result = ocr.ocr(img_path, cls=True)\nfor line in result:\n    print(line)\n    print(line[0][1][0])\n# \n# resultlistitem\n# line\n# [[[3.0, 149.0], [43.0, 149.0], [43.0, 163.0], [3.0, 163.0]], ('', 0.6762619018554688)]\n#  boxes = line[0](x,y)\n#  txts = line[1][0]",
        "detail": "suanfa.mian03",
        "documentation": {}
    },
    {
        "label": "processor",
        "kind": 5,
        "importPath": "wav2vec.test01",
        "description": "wav2vec.test01",
        "peekOfCode": "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")   # ASR32\naudio_input, sample_rate = sf.read(path_audio)  # (31129,)\ninput_values = processor(audio_input, sampling_rate=sample_rate, return_tensors=\"pt\").input_values  # torch.Size([1, 31129])\nlogits = model(input_values).logits     # torch.Size([1, 97, 32])\npredicted_ids = torch.argmax(logits, dim=-1)    # torch.Size([1, 97])\ntranscription = processor.decode(predicted_ids[0])  # ASR\nfrom transformers import Wav2Vec2Model\nmodel = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")    # 768\nwav2vec2 = model(input_values)['last_hidden_state']     # torch.Size([1, 97, 768])BaseModelOutput",
        "detail": "wav2vec.test01",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "wav2vec.test01",
        "description": "wav2vec.test01",
        "peekOfCode": "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")   # ASR32\naudio_input, sample_rate = sf.read(path_audio)  # (31129,)\ninput_values = processor(audio_input, sampling_rate=sample_rate, return_tensors=\"pt\").input_values  # torch.Size([1, 31129])\nlogits = model(input_values).logits     # torch.Size([1, 97, 32])\npredicted_ids = torch.argmax(logits, dim=-1)    # torch.Size([1, 97])\ntranscription = processor.decode(predicted_ids[0])  # ASR\nfrom transformers import Wav2Vec2Model\nmodel = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")    # 768\nwav2vec2 = model(input_values)['last_hidden_state']     # torch.Size([1, 97, 768])BaseModelOutput",
        "detail": "wav2vec.test01",
        "documentation": {}
    },
    {
        "label": "input_values",
        "kind": 5,
        "importPath": "wav2vec.test01",
        "description": "wav2vec.test01",
        "peekOfCode": "input_values = processor(audio_input, sampling_rate=sample_rate, return_tensors=\"pt\").input_values  # torch.Size([1, 31129])\nlogits = model(input_values).logits     # torch.Size([1, 97, 32])\npredicted_ids = torch.argmax(logits, dim=-1)    # torch.Size([1, 97])\ntranscription = processor.decode(predicted_ids[0])  # ASR\nfrom transformers import Wav2Vec2Model\nmodel = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")    # 768\nwav2vec2 = model(input_values)['last_hidden_state']     # torch.Size([1, 97, 768])BaseModelOutput",
        "detail": "wav2vec.test01",
        "documentation": {}
    },
    {
        "label": "logits",
        "kind": 5,
        "importPath": "wav2vec.test01",
        "description": "wav2vec.test01",
        "peekOfCode": "logits = model(input_values).logits     # torch.Size([1, 97, 32])\npredicted_ids = torch.argmax(logits, dim=-1)    # torch.Size([1, 97])\ntranscription = processor.decode(predicted_ids[0])  # ASR\nfrom transformers import Wav2Vec2Model\nmodel = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")    # 768\nwav2vec2 = model(input_values)['last_hidden_state']     # torch.Size([1, 97, 768])BaseModelOutput",
        "detail": "wav2vec.test01",
        "documentation": {}
    },
    {
        "label": "predicted_ids",
        "kind": 5,
        "importPath": "wav2vec.test01",
        "description": "wav2vec.test01",
        "peekOfCode": "predicted_ids = torch.argmax(logits, dim=-1)    # torch.Size([1, 97])\ntranscription = processor.decode(predicted_ids[0])  # ASR\nfrom transformers import Wav2Vec2Model\nmodel = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")    # 768\nwav2vec2 = model(input_values)['last_hidden_state']     # torch.Size([1, 97, 768])BaseModelOutput",
        "detail": "wav2vec.test01",
        "documentation": {}
    },
    {
        "label": "transcription",
        "kind": 5,
        "importPath": "wav2vec.test01",
        "description": "wav2vec.test01",
        "peekOfCode": "transcription = processor.decode(predicted_ids[0])  # ASR\nfrom transformers import Wav2Vec2Model\nmodel = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")    # 768\nwav2vec2 = model(input_values)['last_hidden_state']     # torch.Size([1, 97, 768])BaseModelOutput",
        "detail": "wav2vec.test01",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "wav2vec.test01",
        "description": "wav2vec.test01",
        "peekOfCode": "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")    # 768\nwav2vec2 = model(input_values)['last_hidden_state']     # torch.Size([1, 97, 768])BaseModelOutput",
        "detail": "wav2vec.test01",
        "documentation": {}
    },
    {
        "label": "wav2vec2",
        "kind": 5,
        "importPath": "wav2vec.test01",
        "description": "wav2vec.test01",
        "peekOfCode": "wav2vec2 = model(input_values)['last_hidden_state']     # torch.Size([1, 97, 768])BaseModelOutput",
        "detail": "wav2vec.test01",
        "documentation": {}
    },
    {
        "label": "update",
        "kind": 2,
        "importPath": "wav2vec.test02",
        "description": "wav2vec.test02",
        "peekOfCode": "def update():\n    global X, Y, W, lr, n\n    n = n + 1\n    O = np.dot(X, W.T)\n    O2 = np.dot(X, W.T)\n    # Q=np.array([O1,O2]);\n    # Q=np.dot(Q,W.T);\n    # \n    W_Tmp = lr * ((Y - O.T).dot(X))\n    W = W + W_Tmp",
        "detail": "wav2vec.test02",
        "documentation": {}
    },
    {
        "label": "X",
        "kind": 5,
        "importPath": "wav2vec.test02",
        "description": "wav2vec.test02",
        "peekOfCode": "X = np.array([[8, 6, 8], [5, 8, 8], [1, 2, 2], [2, 2, 4], [6, 6, 8], [7, 6, 8]])\nY = np.array([15, 13, 3, 5, 13, 14])\n# W1 = (np.random.random(2)-0.5)*2;\nW = np.array([0, 0, 0])\nlr = 0.002\n# \nn = 0\n# #\nO1 = 0\nO2 = 0",
        "detail": "wav2vec.test02",
        "documentation": {}
    },
    {
        "label": "Y",
        "kind": 5,
        "importPath": "wav2vec.test02",
        "description": "wav2vec.test02",
        "peekOfCode": "Y = np.array([15, 13, 3, 5, 13, 14])\n# W1 = (np.random.random(2)-0.5)*2;\nW = np.array([0, 0, 0])\nlr = 0.002\n# \nn = 0\n# #\nO1 = 0\nO2 = 0\nQ = 0",
        "detail": "wav2vec.test02",
        "documentation": {}
    },
    {
        "label": "W",
        "kind": 5,
        "importPath": "wav2vec.test02",
        "description": "wav2vec.test02",
        "peekOfCode": "W = np.array([0, 0, 0])\nlr = 0.002\n# \nn = 0\n# #\nO1 = 0\nO2 = 0\nQ = 0\nn = 0\ndef update():",
        "detail": "wav2vec.test02",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "wav2vec.test02",
        "description": "wav2vec.test02",
        "peekOfCode": "lr = 0.002\n# \nn = 0\n# #\nO1 = 0\nO2 = 0\nQ = 0\nn = 0\ndef update():\n    global X, Y, W, lr, n",
        "detail": "wav2vec.test02",
        "documentation": {}
    },
    {
        "label": "n",
        "kind": 5,
        "importPath": "wav2vec.test02",
        "description": "wav2vec.test02",
        "peekOfCode": "n = 0\n# #\nO1 = 0\nO2 = 0\nQ = 0\nn = 0\ndef update():\n    global X, Y, W, lr, n\n    n = n + 1\n    O = np.dot(X, W.T)",
        "detail": "wav2vec.test02",
        "documentation": {}
    },
    {
        "label": "O1",
        "kind": 5,
        "importPath": "wav2vec.test02",
        "description": "wav2vec.test02",
        "peekOfCode": "O1 = 0\nO2 = 0\nQ = 0\nn = 0\ndef update():\n    global X, Y, W, lr, n\n    n = n + 1\n    O = np.dot(X, W.T)\n    O2 = np.dot(X, W.T)\n    # Q=np.array([O1,O2]);",
        "detail": "wav2vec.test02",
        "documentation": {}
    },
    {
        "label": "O2",
        "kind": 5,
        "importPath": "wav2vec.test02",
        "description": "wav2vec.test02",
        "peekOfCode": "O2 = 0\nQ = 0\nn = 0\ndef update():\n    global X, Y, W, lr, n\n    n = n + 1\n    O = np.dot(X, W.T)\n    O2 = np.dot(X, W.T)\n    # Q=np.array([O1,O2]);\n    # Q=np.dot(Q,W.T);",
        "detail": "wav2vec.test02",
        "documentation": {}
    },
    {
        "label": "Q",
        "kind": 5,
        "importPath": "wav2vec.test02",
        "description": "wav2vec.test02",
        "peekOfCode": "Q = 0\nn = 0\ndef update():\n    global X, Y, W, lr, n\n    n = n + 1\n    O = np.dot(X, W.T)\n    O2 = np.dot(X, W.T)\n    # Q=np.array([O1,O2]);\n    # Q=np.dot(Q,W.T);\n    # ",
        "detail": "wav2vec.test02",
        "documentation": {}
    },
    {
        "label": "n",
        "kind": 5,
        "importPath": "wav2vec.test02",
        "description": "wav2vec.test02",
        "peekOfCode": "n = 0\ndef update():\n    global X, Y, W, lr, n\n    n = n + 1\n    O = np.dot(X, W.T)\n    O2 = np.dot(X, W.T)\n    # Q=np.array([O1,O2]);\n    # Q=np.dot(Q,W.T);\n    # \n    W_Tmp = lr * ((Y - O.T).dot(X))",
        "detail": "wav2vec.test02",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "model = models.resnet18(pretrained=True)\n# \nnum_classes = 10  # \nnum_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features, num_classes)\n# \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n# \ntrain_loader = \"\"  # ",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "num_classes",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "num_classes = 10  # \nnum_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features, num_classes)\n# \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n# \ntrain_loader = \"\"  # \nnum_epochs = 10  # \nfor epoch in range(num_epochs):",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "num_features",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "num_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features, num_classes)\n# \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n# \ntrain_loader = \"\"  # \nnum_epochs = 10  # \nfor epoch in range(num_epochs):\n    for images, labels in train_loader:",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "model.fc",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "model.fc = nn.Linear(num_features, num_classes)\n# \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n# \ntrain_loader = \"\"  # \nnum_epochs = 10  # \nfor epoch in range(num_epochs):\n    for images, labels in train_loader:\n        optimizer.zero_grad()",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "criterion",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "criterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n# \ntrain_loader = \"\"  # \nnum_epochs = 10  # \nfor epoch in range(num_epochs):\n    for images, labels in train_loader:\n        optimizer.zero_grad()\n        # \n        outputs = model(images)",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "optimizer",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n# \ntrain_loader = \"\"  # \nnum_epochs = 10  # \nfor epoch in range(num_epochs):\n    for images, labels in train_loader:\n        optimizer.zero_grad()\n        # \n        outputs = model(images)\n        loss = criterion(outputs, labels)",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "train_loader",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "train_loader = \"\"  # \nnum_epochs = 10  # \nfor epoch in range(num_epochs):\n    for images, labels in train_loader:\n        optimizer.zero_grad()\n        # \n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        # \n        loss.backward()",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "num_epochs = 10  # \nfor epoch in range(num_epochs):\n    for images, labels in train_loader:\n        optimizer.zero_grad()\n        # \n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        # \n        loss.backward()\n        optimizer.step()",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "api_id",
        "kind": 5,
        "importPath": "bot",
        "description": "bot",
        "peekOfCode": "api_id = \"27175683\"\napi_hash = \"ff5bd7a1d985ad6e6db389e4ef104fe6\"\nbot_token = \"6778102881:AAEJ0PvBN8qDjR3MqNSGNFWUXaLacuKbQ7Y\"\napp = Client(\n    \"kjjjkljklbot\",\n    api_id=api_id, api_hash=api_hash,\n    bot_token=bot_token\n)\n@app.on_message()\nasync def echo(client: Client, message: Message):",
        "detail": "bot",
        "documentation": {}
    },
    {
        "label": "api_hash",
        "kind": 5,
        "importPath": "bot",
        "description": "bot",
        "peekOfCode": "api_hash = \"ff5bd7a1d985ad6e6db389e4ef104fe6\"\nbot_token = \"6778102881:AAEJ0PvBN8qDjR3MqNSGNFWUXaLacuKbQ7Y\"\napp = Client(\n    \"kjjjkljklbot\",\n    api_id=api_id, api_hash=api_hash,\n    bot_token=bot_token\n)\n@app.on_message()\nasync def echo(client: Client, message: Message):\n    await message.reply(message.text)",
        "detail": "bot",
        "documentation": {}
    },
    {
        "label": "bot_token",
        "kind": 5,
        "importPath": "bot",
        "description": "bot",
        "peekOfCode": "bot_token = \"6778102881:AAEJ0PvBN8qDjR3MqNSGNFWUXaLacuKbQ7Y\"\napp = Client(\n    \"kjjjkljklbot\",\n    api_id=api_id, api_hash=api_hash,\n    bot_token=bot_token\n)\n@app.on_message()\nasync def echo(client: Client, message: Message):\n    await message.reply(message.text)\napp.run()",
        "detail": "bot",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "bot",
        "description": "bot",
        "peekOfCode": "app = Client(\n    \"kjjjkljklbot\",\n    api_id=api_id, api_hash=api_hash,\n    bot_token=bot_token\n)\n@app.on_message()\nasync def echo(client: Client, message: Message):\n    await message.reply(message.text)\napp.run()",
        "detail": "bot",
        "documentation": {}
    },
    {
        "label": "paddstr",
        "kind": 2,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "def paddstr(strx):\n    return \"NULL\" if strx == None else \"\\'\" + escape_string(str(strx))+ \"\\'\"\ndef deal_sn_data():\n    bizProConn = MySQL(edun_pro_cfg)\n    sql = \"\"\"\n       SELECT deviceType,deviceNo,sn,imsi,iccid,simPhone,isActive,activeDate,checkCode,modelId,userId,createtime,updateTime,d_LoginUserId,issub,vehicleId,partner,validateStatus,validateBy,validateNums,soft_version,imei,deviceId,source,networkCardNubmer FROM d_track_info dti WHERE dti.sn IN (\t\n       SELECT dti.sn\n       FROM d_vehicle dv,\n       d_track_info dti,\n       d_class dc",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "deal_sn_data",
        "kind": 2,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "def deal_sn_data():\n    bizProConn = MySQL(edun_pro_cfg)\n    sql = \"\"\"\n       SELECT deviceType,deviceNo,sn,imsi,iccid,simPhone,isActive,activeDate,checkCode,modelId,userId,createtime,updateTime,d_LoginUserId,issub,vehicleId,partner,validateStatus,validateBy,validateNums,soft_version,imei,deviceId,source,networkCardNubmer FROM d_track_info dti WHERE dti.sn IN (\t\n       SELECT dti.sn\n       FROM d_vehicle dv,\n       d_track_info dti,\n       d_class dc\n       WHERE dti.vehicleId = dv.vehicleId\n       AND dc.classid = dv.classId",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "logger = logging.getLogger(\"deal_equity_data\")\nlogger.setLevel(logging.DEBUG)\nfileHandler = logging.FileHandler(\"import.log\")\nfileHandler.setLevel(logging.DEBUG)\nformatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\ncnslhandler = logging.StreamHandler()\ncnslhandler.setLevel(logging.DEBUG)\ncnslhandler.setFormatter(formatter)\nlogger.addHandler(cnslhandler)\nedun_pro_cfg = {",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "fileHandler",
        "kind": 5,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "fileHandler = logging.FileHandler(\"import.log\")\nfileHandler.setLevel(logging.DEBUG)\nformatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\ncnslhandler = logging.StreamHandler()\ncnslhandler.setLevel(logging.DEBUG)\ncnslhandler.setFormatter(formatter)\nlogger.addHandler(cnslhandler)\nedun_pro_cfg = {\n    \"host\": \"192.168.5.22\",\n    \"port\": 3307,",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "formatter",
        "kind": 5,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\ncnslhandler = logging.StreamHandler()\ncnslhandler.setLevel(logging.DEBUG)\ncnslhandler.setFormatter(formatter)\nlogger.addHandler(cnslhandler)\nedun_pro_cfg = {\n    \"host\": \"192.168.5.22\",\n    \"port\": 3307,\n    \"user\": \"user_yangll\",\n    \"passwd\": \"vGxw9jWg\",",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "cnslhandler",
        "kind": 5,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "cnslhandler = logging.StreamHandler()\ncnslhandler.setLevel(logging.DEBUG)\ncnslhandler.setFormatter(formatter)\nlogger.addHandler(cnslhandler)\nedun_pro_cfg = {\n    \"host\": \"192.168.5.22\",\n    \"port\": 3307,\n    \"user\": \"user_yangll\",\n    \"passwd\": \"vGxw9jWg\",\n    \"db\": \"gps\",",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "edun_pro_cfg",
        "kind": 5,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "edun_pro_cfg = {\n    \"host\": \"192.168.5.22\",\n    \"port\": 3307,\n    \"user\": \"user_yangll\",\n    \"passwd\": \"vGxw9jWg\",\n    \"db\": \"gps\",\n    \"charset\": \"utf8\",\n}\nedun_test_cfg = {\n    \"host\": \"192.168.3.222\",",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "edun_test_cfg",
        "kind": 5,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "edun_test_cfg = {\n    \"host\": \"192.168.3.222\",\n    \"port\": 3307,\n    \"user\": \"root\",\n    \"passwd\": \"Msd^*$@online\",\n    \"db\": \"newgps\",\n    \"charset\": \"utf8\",\n}\nfrom pymysql.converters import escape_string\n#",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "demoFindLogClass",
        "kind": 6,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "class demoFindLogClass(object):\n    def __init__(self):\n        pass\n    def demoFindLog(self):\n        deault_lng = 110.35\n        deault_lat = 34.62\n        with open(\"biz-gps-kafka-pazlpush.log.2023-07-28.99\", \"r\", encoding='UTF-8') as f:\n            content_list = f.readlines()\n        for content_line in content_list:\n            if \"===\" in content_line:",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "parse_json",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def parse_json():\n    path = \"H:\\PythonWorkSpaces\\deal_lnglat_data\"\n    files = os.listdir(path)\n    s = []\n    for file in files:\n        print(file)\n        # \n        # f = open(\"biz-gps-kafka-pazlpush.log.2023-07-28.99\", 'r', encoding='UTF-8')\n        # lines = f.readlines()\n        # for line in lines:",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "print_hi",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def print_hi(name):\n    print(f'Hi, {name}')\nclass demoFindLogClass(object):\n    def __init__(self):\n        pass\n    def demoFindLog(self):\n        deault_lng = 110.35\n        deault_lat = 34.62\n        with open(\"biz-gps-kafka-pazlpush.log.2023-07-28.99\", \"r\", encoding='UTF-8') as f:\n            content_list = f.readlines()",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "print_others",
        "kind": 2,
        "importPath": "main_bot",
        "description": "main_bot",
        "peekOfCode": "def print_others(msg):\n    print(msg)\n#  my_friend  (!)\n@bot.register(my_friend)\ndef reply_my_friend(msg):\n    return \"received: {} ({})\".format(msg.text, msg.type)\n# \n@bot.register(msg_types=FRIENDS)\ndef auto_accept_friends(msg):\n    # ",
        "detail": "main_bot",
        "documentation": {}
    },
    {
        "label": "reply_my_friend",
        "kind": 2,
        "importPath": "main_bot",
        "description": "main_bot",
        "peekOfCode": "def reply_my_friend(msg):\n    return \"received: {} ({})\".format(msg.text, msg.type)\n# \n@bot.register(msg_types=FRIENDS)\ndef auto_accept_friends(msg):\n    # \n    new_friend = msg.card.accept()\n    # \n    new_friend.send(\"\")",
        "detail": "main_bot",
        "documentation": {}
    },
    {
        "label": "auto_accept_friends",
        "kind": 2,
        "importPath": "main_bot",
        "description": "main_bot",
        "peekOfCode": "def auto_accept_friends(msg):\n    # \n    new_friend = msg.card.accept()\n    # \n    new_friend.send(\"\")",
        "detail": "main_bot",
        "documentation": {}
    },
    {
        "label": "bot",
        "kind": 5,
        "importPath": "main_bot",
        "description": "main_bot",
        "peekOfCode": "bot = Bot()\n# **:**\n#  \"\" \nmy_friend = bot.friends().search(\"\", sex=MALE, city=\"\")[0]\n# **:**\n# \nmy_friend.send(\"Hello WeChat!\")\n# \nmy_friend.send_image(\"my_picture.jpg\")\n# **:**",
        "detail": "main_bot",
        "documentation": {}
    },
    {
        "label": "my_friend",
        "kind": 5,
        "importPath": "main_bot",
        "description": "main_bot",
        "peekOfCode": "my_friend = bot.friends().search(\"\", sex=MALE, city=\"\")[0]\n# **:**\n# \nmy_friend.send(\"Hello WeChat!\")\n# \nmy_friend.send_image(\"my_picture.jpg\")\n# **:**\n# \n@bot.register()\ndef print_others(msg):",
        "detail": "main_bot",
        "documentation": {}
    },
    {
        "label": "reply_msg",
        "kind": 2,
        "importPath": "main_itchat",
        "description": "main_itchat",
        "peekOfCode": "def reply_msg(msg):\n    print(\"\", msg.text)\nif __name__ == \"__main__\":\n    itchat.auto_login()\n    time.sleep(5)\n    itchat.send(\"\", toUserName=\"filehelper\")\n    itchat.run()",
        "detail": "main_itchat",
        "documentation": {}
    },
    {
        "label": "friends",
        "kind": 5,
        "importPath": "main_itchat01",
        "description": "main_itchat01",
        "peekOfCode": "friends = itchat.get_friends()\n# jsonjson\nprint(json.dumps(friends))\n# \nitchat.run()",
        "detail": "main_itchat01",
        "documentation": {}
    },
    {
        "label": "my_method",
        "kind": 2,
        "importPath": "main_test01",
        "description": "main_test01",
        "peekOfCode": "def my_method():\n    print(\"\")\n    # requests.post()\n# end def\ndef bot_chat():\n    \"\"\"\n    Purpose:\n    \"\"\"\n    print(\"nihao\")\n# end def",
        "detail": "main_test01",
        "documentation": {}
    },
    {
        "label": "bot_chat",
        "kind": 2,
        "importPath": "main_test01",
        "description": "main_test01",
        "peekOfCode": "def bot_chat():\n    \"\"\"\n    Purpose:\n    \"\"\"\n    print(\"nihao\")\n# end def\nif __name__ == \"__main__\":\n    my_method()\n    bot_chat()\n    print(\"nihao\")",
        "detail": "main_test01",
        "documentation": {}
    }
]