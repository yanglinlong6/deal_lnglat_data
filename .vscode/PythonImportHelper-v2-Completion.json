[
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "hmm",
        "importPath": "hmmlearn",
        "description": "hmmlearn",
        "isExtraImport": true,
        "detail": "hmmlearn",
        "documentation": {}
    },
    {
        "label": "AssistantAgent",
        "importPath": "autogen",
        "description": "autogen",
        "isExtraImport": true,
        "detail": "autogen",
        "documentation": {}
    },
    {
        "label": "UserProxyAgent",
        "importPath": "autogen",
        "description": "autogen",
        "isExtraImport": true,
        "detail": "autogen",
        "documentation": {}
    },
    {
        "label": "config_list_from_json",
        "importPath": "autogen",
        "description": "autogen",
        "isExtraImport": true,
        "detail": "autogen",
        "documentation": {}
    },
    {
        "label": "UserProxyAgent",
        "importPath": "autogen",
        "description": "autogen",
        "isExtraImport": true,
        "detail": "autogen",
        "documentation": {}
    },
    {
        "label": "ConversableAgent",
        "importPath": "autogen",
        "description": "autogen",
        "isExtraImport": true,
        "detail": "autogen",
        "documentation": {}
    },
    {
        "label": "config_list_from_json",
        "importPath": "autogen",
        "description": "autogen",
        "isExtraImport": true,
        "detail": "autogen",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "path",
        "importPath": "os",
        "description": "os",
        "isExtraImport": true,
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "ChatGLM3",
        "importPath": "langchain_community.llms.chatglm3",
        "description": "langchain_community.llms.chatglm3",
        "isExtraImport": true,
        "detail": "langchain_community.llms.chatglm3",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "ConversationalRetrievalChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "APIChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMBashChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMCheckerChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMRequestsChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMSummarizationCheckerChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMSummarizationCheckerChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMSummarizationCheckerChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "OpenAIModerationChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "SequentialChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "SimpleSequentialChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "RetrievalQA",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "ConversationChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "RetrievalQA",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "StringPromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "HumanMessagePromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "MessagesPlaceholder",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "SystemMessagePromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "AIMessage",
        "importPath": "langchain.schema.messages",
        "description": "langchain.schema.messages",
        "isExtraImport": true,
        "detail": "langchain.schema.messages",
        "documentation": {}
    },
    {
        "label": "ZhipuAI",
        "importPath": "zhipuai",
        "description": "zhipuai",
        "isExtraImport": true,
        "detail": "zhipuai",
        "documentation": {}
    },
    {
        "label": "ZhipuAI",
        "importPath": "zhipuai",
        "description": "zhipuai",
        "isExtraImport": true,
        "detail": "zhipuai",
        "documentation": {}
    },
    {
        "label": "ZhipuAI",
        "importPath": "zhipuai",
        "description": "zhipuai",
        "isExtraImport": true,
        "detail": "zhipuai",
        "documentation": {}
    },
    {
        "label": "ZhipuAI",
        "importPath": "zhipuai",
        "description": "zhipuai",
        "isExtraImport": true,
        "detail": "zhipuai",
        "documentation": {}
    },
    {
        "label": "ZhipuAI",
        "importPath": "zhipuai",
        "description": "zhipuai",
        "isExtraImport": true,
        "detail": "zhipuai",
        "documentation": {}
    },
    {
        "label": "ZhipuAI",
        "importPath": "zhipuai",
        "description": "zhipuai",
        "isExtraImport": true,
        "detail": "zhipuai",
        "documentation": {}
    },
    {
        "label": "TextLoader",
        "importPath": "langchain.document_loaders",
        "description": "langchain.document_loaders",
        "isExtraImport": true,
        "detail": "langchain.document_loaders",
        "documentation": {}
    },
    {
        "label": "TextLoader",
        "importPath": "langchain.document_loaders",
        "description": "langchain.document_loaders",
        "isExtraImport": true,
        "detail": "langchain.document_loaders",
        "documentation": {}
    },
    {
        "label": "OnlinePDFLoader",
        "importPath": "langchain.document_loaders",
        "description": "langchain.document_loaders",
        "isExtraImport": true,
        "detail": "langchain.document_loaders",
        "documentation": {}
    },
    {
        "label": "CharacterTextSplitter",
        "importPath": "langchain.text_splitter",
        "description": "langchain.text_splitter",
        "isExtraImport": true,
        "detail": "langchain.text_splitter",
        "documentation": {}
    },
    {
        "label": "RecursiveCharacterTextSplitter",
        "importPath": "langchain.text_splitter",
        "description": "langchain.text_splitter",
        "isExtraImport": true,
        "detail": "langchain.text_splitter",
        "documentation": {}
    },
    {
        "label": "OpenAIEmbeddings",
        "importPath": "langchain.embeddings.openai",
        "description": "langchain.embeddings.openai",
        "isExtraImport": true,
        "detail": "langchain.embeddings.openai",
        "documentation": {}
    },
    {
        "label": "OpenAIEmbeddings",
        "importPath": "langchain.embeddings.openai",
        "description": "langchain.embeddings.openai",
        "isExtraImport": true,
        "detail": "langchain.embeddings.openai",
        "documentation": {}
    },
    {
        "label": "DeepLake",
        "importPath": "langchain.vectorstores",
        "description": "langchain.vectorstores",
        "isExtraImport": true,
        "detail": "langchain.vectorstores",
        "documentation": {}
    },
    {
        "label": "DeepLake",
        "importPath": "langchain.vectorstores",
        "description": "langchain.vectorstores",
        "isExtraImport": true,
        "detail": "langchain.vectorstores",
        "documentation": {}
    },
    {
        "label": "FAISS",
        "importPath": "langchain.vectorstores",
        "description": "langchain.vectorstores",
        "isExtraImport": true,
        "detail": "langchain.vectorstores",
        "documentation": {}
    },
    {
        "label": "Chroma",
        "importPath": "langchain.vectorstores",
        "description": "langchain.vectorstores",
        "isExtraImport": true,
        "detail": "langchain.vectorstores",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain.chat_models",
        "description": "langchain.chat_models",
        "isExtraImport": true,
        "detail": "langchain.chat_models",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain.chat_models",
        "description": "langchain.chat_models",
        "isExtraImport": true,
        "detail": "langchain.chat_models",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain.chat_models",
        "description": "langchain.chat_models",
        "isExtraImport": true,
        "detail": "langchain.chat_models",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain.chat_models",
        "description": "langchain.chat_models",
        "isExtraImport": true,
        "detail": "langchain.chat_models",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain.chat_models",
        "description": "langchain.chat_models",
        "isExtraImport": true,
        "detail": "langchain.chat_models",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain.chat_models",
        "description": "langchain.chat_models",
        "isExtraImport": true,
        "detail": "langchain.chat_models",
        "documentation": {}
    },
    {
        "label": "gTTS",
        "importPath": "gtts",
        "description": "gtts",
        "isExtraImport": true,
        "detail": "gtts",
        "documentation": {}
    },
    {
        "label": "pyttsx3",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pyttsx3",
        "description": "pyttsx3",
        "detail": "pyttsx3",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "io",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "io",
        "description": "io",
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "pprint",
        "importPath": "pprint",
        "description": "pprint",
        "isExtraImport": true,
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "pprint",
        "importPath": "pprint",
        "description": "pprint",
        "isExtraImport": true,
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "pprint",
        "importPath": "pprint",
        "description": "pprint",
        "isExtraImport": true,
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "speech_recognition",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "speech_recognition",
        "description": "speech_recognition",
        "detail": "speech_recognition",
        "documentation": {}
    },
    {
        "label": "AutoModelForSequenceClassification",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "MsDataset",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoModelForSequenceClassification",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "Model",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoModelForSequenceClassification",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "MsDataset",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoModelForSequenceClassification",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "snapshot_download",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoModelForSequenceClassification",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "MsDataset",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoModelForSequenceClassification",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "default_data_collator",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "PerceiverTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "PerceiverForMaskedLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "PerceiverTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "PerceiverForMaskedLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "default_data_collator",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "pipeline",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "pipeline",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "default_data_collator",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Wav2Vec2ForCTC",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Wav2Vec2Processor",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Wav2Vec2Model",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "LoRAConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "TrainingArguments",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "LoRAConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "LoRAConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "LoRAConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "TrainingArguments",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "LoRAConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "ResTuningConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "PeftModel",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "PeftConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "LoraConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "LoRAConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "TrainingArguments",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "LoRAConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "LoraConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "read_config",
        "importPath": "modelscope.utils.hub",
        "description": "modelscope.utils.hub",
        "isExtraImport": true,
        "detail": "modelscope.utils.hub",
        "documentation": {}
    },
    {
        "label": "MsDataset",
        "importPath": "modelscope.msdatasets",
        "description": "modelscope.msdatasets",
        "isExtraImport": true,
        "detail": "modelscope.msdatasets",
        "documentation": {}
    },
    {
        "label": "build_trainer",
        "importPath": "modelscope.trainers",
        "description": "modelscope.trainers",
        "isExtraImport": true,
        "detail": "modelscope.trainers",
        "documentation": {}
    },
    {
        "label": "build_trainer",
        "importPath": "modelscope.trainers",
        "description": "modelscope.trainers",
        "isExtraImport": true,
        "detail": "modelscope.trainers",
        "documentation": {}
    },
    {
        "label": "tempfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tempfile",
        "description": "tempfile",
        "detail": "tempfile",
        "documentation": {}
    },
    {
        "label": "Trainers",
        "importPath": "modelscope.metainfo",
        "description": "modelscope.metainfo",
        "isExtraImport": true,
        "detail": "modelscope.metainfo",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Linear",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "CrossEntropyLoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "torch.optim",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.optim",
        "description": "torch.optim",
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "torch.optim",
        "description": "torch.optim",
        "isExtraImport": true,
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "StepLR",
        "importPath": "torch.optim.lr_scheduler",
        "description": "torch.optim.lr_scheduler",
        "isExtraImport": true,
        "detail": "torch.optim.lr_scheduler",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "default_collate",
        "importPath": "torch.utils.data.dataloader",
        "description": "torch.utils.data.dataloader",
        "isExtraImport": true,
        "detail": "torch.utils.data.dataloader",
        "documentation": {}
    },
    {
        "label": "AgentType",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "initialize_agent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "load_tools",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "Tool",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "BaseSingleActionAgent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentOutputParser",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "LLMSingleActionAgent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "Tool",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "ZeroShotAgent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "Tool",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "Tool",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "ZeroShotAgent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "BaseMultiActionAgent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "Tool",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "load_tools",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "initialize_agent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentType",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentType",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "initialize_agent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "load_tools",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentType",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "initialize_agent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "load_tools",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentType",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "initialize_agent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "load_tools",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentType",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "initialize_agent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "load_tools",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentType",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "initialize_agent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "load_tools",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "Ollama",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "Ollama",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "langchain",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "langchain",
        "description": "langchain",
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "SerpAPIWrapper",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "SerpAPIWrapper",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "SerpAPIWrapper",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "SerpAPIWrapper",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "SerpAPIWrapper",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "LLMMathChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "ConversationChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypedDict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Annotated",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "AgentAction",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AgentFinish",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AgentAction",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AgentFinish",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AgentAction",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AgentFinish",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AIMessage",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "SystemMessage",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "OpenAIEmbeddings",
        "importPath": "langchain.embeddings",
        "description": "langchain.embeddings",
        "isExtraImport": true,
        "detail": "langchain.embeddings",
        "documentation": {}
    },
    {
        "label": "GPT4AllEmbeddings",
        "importPath": "langchain.embeddings",
        "description": "langchain.embeddings",
        "isExtraImport": true,
        "detail": "langchain.embeddings",
        "documentation": {}
    },
    {
        "label": "API_RESPONSE_PROMPT",
        "importPath": "langchain.chains.api.prompt",
        "description": "langchain.chains.api.prompt",
        "isExtraImport": true,
        "detail": "langchain.chains.api.prompt",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts.prompt",
        "description": "langchain.prompts.prompt",
        "isExtraImport": true,
        "detail": "langchain.prompts.prompt",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts.prompt",
        "description": "langchain.prompts.prompt",
        "isExtraImport": true,
        "detail": "langchain.prompts.prompt",
        "documentation": {}
    },
    {
        "label": "open_meteo_docs",
        "importPath": "langchain.chains.api",
        "description": "langchain.chains.api",
        "isExtraImport": true,
        "detail": "langchain.chains.api",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain.chains.llm",
        "description": "langchain.chains.llm",
        "isExtraImport": true,
        "detail": "langchain.chains.llm",
        "documentation": {}
    },
    {
        "label": "ConstitutionalChain",
        "importPath": "langchain.chains.constitutional_ai.base",
        "description": "langchain.chains.constitutional_ai.base",
        "isExtraImport": true,
        "detail": "langchain.chains.constitutional_ai.base",
        "documentation": {}
    },
    {
        "label": "ConstitutionalPrinciple",
        "importPath": "langchain.chains.constitutional_ai.models",
        "description": "langchain.chains.constitutional_ai.models",
        "isExtraImport": true,
        "detail": "langchain.chains.constitutional_ai.models",
        "documentation": {}
    },
    {
        "label": "BashOutputParser",
        "importPath": "langchain.chains.llm_bash.prompt",
        "description": "langchain.chains.llm_bash.prompt",
        "isExtraImport": true,
        "detail": "langchain.chains.llm_bash.prompt",
        "documentation": {}
    },
    {
        "label": "BashProcess",
        "importPath": "langchain.utilities.bash",
        "description": "langchain.utilities.bash",
        "isExtraImport": true,
        "detail": "langchain.utilities.bash",
        "documentation": {}
    },
    {
        "label": "asyncio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "asyncio",
        "description": "asyncio",
        "detail": "asyncio",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "VectorstoreIndexCreator",
        "importPath": "langchain.indexes",
        "description": "langchain.indexes",
        "isExtraImport": true,
        "detail": "langchain.indexes",
        "documentation": {}
    },
    {
        "label": "ConversationBufferMemory",
        "importPath": "langchain.memory",
        "description": "langchain.memory",
        "isExtraImport": true,
        "detail": "langchain.memory",
        "documentation": {}
    },
    {
        "label": "ConversationBufferMemory",
        "importPath": "langchain.memory",
        "description": "langchain.memory",
        "isExtraImport": true,
        "detail": "langchain.memory",
        "documentation": {}
    },
    {
        "label": "openai",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "openai",
        "description": "openai",
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "AzureOpenAI",
        "importPath": "openai",
        "description": "openai",
        "isExtraImport": true,
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "AzureOpenAI",
        "importPath": "openai",
        "description": "openai",
        "isExtraImport": true,
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "HumanMessagePromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "SystemMessagePromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "HumanMessagePromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "SystemMessagePromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "TavilySearchResults",
        "importPath": "langchain_community.tools.tavily_search",
        "description": "langchain_community.tools.tavily_search",
        "isExtraImport": true,
        "detail": "langchain_community.tools.tavily_search",
        "documentation": {}
    },
    {
        "label": "ToolExecutor",
        "importPath": "langgraph.prebuilt",
        "description": "langgraph.prebuilt",
        "isExtraImport": true,
        "detail": "langgraph.prebuilt",
        "documentation": {}
    },
    {
        "label": "ToolInvocation",
        "importPath": "langgraph.prebuilt",
        "description": "langgraph.prebuilt",
        "isExtraImport": true,
        "detail": "langgraph.prebuilt",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "format_tool_to_openai_function",
        "importPath": "langchain.tools.render",
        "description": "langchain.tools.render",
        "isExtraImport": true,
        "detail": "langchain.tools.render",
        "documentation": {}
    },
    {
        "label": "operator",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "operator",
        "description": "operator",
        "detail": "operator",
        "documentation": {}
    },
    {
        "label": "BaseMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "FunctionMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "StateGraph",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "END",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "timm,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "timm.",
        "description": "timm.",
        "detail": "timm.",
        "documentation": {}
    },
    {
        "label": "SbertForSequenceClassification",
        "importPath": "modelscope.models.nlp",
        "description": "modelscope.models.nlp",
        "isExtraImport": true,
        "detail": "modelscope.models.nlp",
        "documentation": {}
    },
    {
        "label": "SbertForSequenceClassification",
        "importPath": "modelscope.models.nlp",
        "description": "modelscope.models.nlp",
        "isExtraImport": true,
        "detail": "modelscope.models.nlp",
        "documentation": {}
    },
    {
        "label": "SbertConfig",
        "importPath": "modelscope.models.nlp.structbert",
        "description": "modelscope.models.nlp.structbert",
        "isExtraImport": true,
        "detail": "modelscope.models.nlp.structbert",
        "documentation": {}
    },
    {
        "label": "SbertConfig",
        "importPath": "modelscope.models.nlp.structbert",
        "description": "modelscope.models.nlp.structbert",
        "isExtraImport": true,
        "detail": "modelscope.models.nlp.structbert",
        "documentation": {}
    },
    {
        "label": "Pipeline",
        "importPath": "pyannote.audio",
        "description": "pyannote.audio",
        "isExtraImport": true,
        "detail": "pyannote.audio",
        "documentation": {}
    },
    {
        "label": "timm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "timm",
        "description": "timm",
        "detail": "timm",
        "documentation": {}
    },
    {
        "label": "DatasetName",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "ModelType",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "SftArguments",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "sft_main",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "ModelType",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "get_vllm_engine",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "get_default_template_type",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "get_template",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "inference_vllm",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "pipeline",
        "importPath": "modelscope.pipelines",
        "description": "modelscope.pipelines",
        "isExtraImport": true,
        "detail": "modelscope.pipelines",
        "documentation": {}
    },
    {
        "label": "pipeline",
        "importPath": "modelscope.pipelines",
        "description": "modelscope.pipelines",
        "isExtraImport": true,
        "detail": "modelscope.pipelines",
        "documentation": {}
    },
    {
        "label": "Tasks",
        "importPath": "modelscope.utils.constant",
        "description": "modelscope.utils.constant",
        "isExtraImport": true,
        "detail": "modelscope.utils.constant",
        "documentation": {}
    },
    {
        "label": "Tasks",
        "importPath": "modelscope.utils.constant",
        "description": "modelscope.utils.constant",
        "isExtraImport": true,
        "detail": "modelscope.utils.constant",
        "documentation": {}
    },
    {
        "label": "StableDiffusionPipeline",
        "importPath": "ppdiffusers",
        "description": "ppdiffusers",
        "isExtraImport": true,
        "detail": "ppdiffusers",
        "documentation": {}
    },
    {
        "label": "paddle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "paddle",
        "description": "paddle",
        "detail": "paddle",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "Model",
        "importPath": "modelscope.models",
        "description": "modelscope.models",
        "isExtraImport": true,
        "detail": "modelscope.models",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift.tuners",
        "description": "swift.tuners",
        "isExtraImport": true,
        "detail": "swift.tuners",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "MySQLdb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "MySQLdb",
        "description": "MySQLdb",
        "detail": "MySQLdb",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "langchain.callbacks.manager",
        "description": "langchain.callbacks.manager",
        "isExtraImport": true,
        "detail": "langchain.callbacks.manager",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "langchain.callbacks.manager",
        "description": "langchain.callbacks.manager",
        "isExtraImport": true,
        "detail": "langchain.callbacks.manager",
        "documentation": {}
    },
    {
        "label": "StreamingStdOutCallbackHandler",
        "importPath": "langchain.callbacks.streaming_stdout",
        "description": "langchain.callbacks.streaming_stdout",
        "isExtraImport": true,
        "detail": "langchain.callbacks.streaming_stdout",
        "documentation": {}
    },
    {
        "label": "StreamingStdOutCallbackHandler",
        "importPath": "langchain.callbacks.streaming_stdout",
        "description": "langchain.callbacks.streaming_stdout",
        "isExtraImport": true,
        "detail": "langchain.callbacks.streaming_stdout",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "pyaudio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pyaudio",
        "description": "pyaudio",
        "detail": "pyaudio",
        "documentation": {}
    },
    {
        "label": "uuid",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "uuid",
        "description": "uuid",
        "detail": "uuid",
        "documentation": {}
    },
    {
        "label": "wave",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "wave",
        "description": "wave",
        "detail": "wave",
        "documentation": {}
    },
    {
        "label": "base64",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "base64",
        "description": "base64",
        "detail": "base64",
        "documentation": {}
    },
    {
        "label": "hashlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hashlib",
        "description": "hashlib",
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "reload",
        "importPath": "importlib ",
        "description": "importlib ",
        "isExtraImport": true,
        "detail": "importlib ",
        "documentation": {}
    },
    {
        "label": "parse",
        "importPath": "urllib",
        "description": "urllib",
        "isExtraImport": true,
        "detail": "urllib",
        "documentation": {}
    },
    {
        "label": "AudioSegment",
        "importPath": "pydub",
        "description": "pydub",
        "isExtraImport": true,
        "detail": "pydub",
        "documentation": {}
    },
    {
        "label": "playsound",
        "importPath": "playsound",
        "description": "playsound",
        "isExtraImport": true,
        "detail": "playsound",
        "documentation": {}
    },
    {
        "label": "Groq",
        "importPath": "groq",
        "description": "groq",
        "isExtraImport": true,
        "detail": "groq",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "OpenAIChat",
        "importPath": "langchain_community.llms",
        "description": "langchain_community.llms",
        "isExtraImport": true,
        "detail": "langchain_community.llms",
        "documentation": {}
    },
    {
        "label": "ASRExecutor",
        "importPath": "paddlespeech.cli.asr.infer",
        "description": "paddlespeech.cli.asr.infer",
        "isExtraImport": true,
        "detail": "paddlespeech.cli.asr.infer",
        "documentation": {}
    },
    {
        "label": "TTSExecutor",
        "importPath": "paddlespeech.cli.tts",
        "description": "paddlespeech.cli.tts",
        "isExtraImport": true,
        "detail": "paddlespeech.cli.tts",
        "documentation": {}
    },
    {
        "label": "AudioExecutor",
        "importPath": "paddlespeech.cli.audio",
        "description": "paddlespeech.cli.audio",
        "isExtraImport": true,
        "detail": "paddlespeech.cli.audio",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "easyocr",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "easyocr",
        "description": "easyocr",
        "detail": "easyocr",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "PaddleOCR",
        "importPath": "paddleocr",
        "description": "paddleocr",
        "isExtraImport": true,
        "detail": "paddleocr",
        "documentation": {}
    },
    {
        "label": "draw_ocr",
        "importPath": "paddleocr",
        "description": "paddleocr",
        "isExtraImport": true,
        "detail": "paddleocr",
        "documentation": {}
    },
    {
        "label": "soundfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "soundfile",
        "description": "soundfile",
        "detail": "soundfile",
        "documentation": {}
    },
    {
        "label": "matplotlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib",
        "description": "matplotlib",
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "models",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "Client",
        "importPath": "pyrogram",
        "description": "pyrogram",
        "isExtraImport": true,
        "detail": "pyrogram",
        "documentation": {}
    },
    {
        "label": "Message",
        "importPath": "pyrogram.types",
        "description": "pyrogram.types",
        "isExtraImport": true,
        "detail": "pyrogram.types",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "Decimal",
        "importPath": "decimal",
        "description": "decimal",
        "isExtraImport": true,
        "detail": "decimal",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "dateutil.relativedelta",
        "description": "dateutil.relativedelta",
        "isExtraImport": true,
        "detail": "dateutil.relativedelta",
        "documentation": {}
    },
    {
        "label": "MySQL",
        "importPath": "my_utils.MysqlUtility",
        "description": "my_utils.MysqlUtility",
        "isExtraImport": true,
        "detail": "my_utils.MysqlUtility",
        "documentation": {}
    },
    {
        "label": "groupby",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "escape_string",
        "importPath": "pymysql.converters",
        "description": "pymysql.converters",
        "isExtraImport": true,
        "detail": "pymysql.converters",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "wxpy",
        "description": "wxpy",
        "isExtraImport": true,
        "detail": "wxpy",
        "documentation": {}
    },
    {
        "label": "itchat",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "itchat",
        "description": "itchat",
        "detail": "itchat",
        "documentation": {}
    },
    {
        "label": "itchat,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "itchat.",
        "description": "itchat.",
        "detail": "itchat.",
        "documentation": {}
    },
    {
        "label": "n_states",
        "kind": 5,
        "importPath": "HMM.test01",
        "description": "HMM.test01",
        "peekOfCode": "n_states = 2\nn_observations = 3\n# 初始化HMM模型\nmodel = hmm.MultinomialHMM(n_components=n_states)\n# 随机生成模型参数\nmodel.startprob_ = np.array([0.6, 0.4])\nmodel.transmat_ = np.array([[0.7, 0.3],\n                            [0.4, 0.6]])\nmodel.emissionprob_ = np.array([[0.1, 0.4, 0.5],\n                                [0.6, 0.3, 0.1]])",
        "detail": "HMM.test01",
        "documentation": {}
    },
    {
        "label": "n_observations",
        "kind": 5,
        "importPath": "HMM.test01",
        "description": "HMM.test01",
        "peekOfCode": "n_observations = 3\n# 初始化HMM模型\nmodel = hmm.MultinomialHMM(n_components=n_states)\n# 随机生成模型参数\nmodel.startprob_ = np.array([0.6, 0.4])\nmodel.transmat_ = np.array([[0.7, 0.3],\n                            [0.4, 0.6]])\nmodel.emissionprob_ = np.array([[0.1, 0.4, 0.5],\n                                [0.6, 0.3, 0.1]])\n# 生成一些观测数据",
        "detail": "HMM.test01",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "HMM.test01",
        "description": "HMM.test01",
        "peekOfCode": "model = hmm.MultinomialHMM(n_components=n_states)\n# 随机生成模型参数\nmodel.startprob_ = np.array([0.6, 0.4])\nmodel.transmat_ = np.array([[0.7, 0.3],\n                            [0.4, 0.6]])\nmodel.emissionprob_ = np.array([[0.1, 0.4, 0.5],\n                                [0.6, 0.3, 0.1]])\n# 生成一些观测数据\n观测序列 = np.array([[0], [1], [2], [1]])\n# 使用模型解码，找到最可能的状态序列",
        "detail": "HMM.test01",
        "documentation": {}
    },
    {
        "label": "model.startprob_",
        "kind": 5,
        "importPath": "HMM.test01",
        "description": "HMM.test01",
        "peekOfCode": "model.startprob_ = np.array([0.6, 0.4])\nmodel.transmat_ = np.array([[0.7, 0.3],\n                            [0.4, 0.6]])\nmodel.emissionprob_ = np.array([[0.1, 0.4, 0.5],\n                                [0.6, 0.3, 0.1]])\n# 生成一些观测数据\n观测序列 = np.array([[0], [1], [2], [1]])\n# 使用模型解码，找到最可能的状态序列\nlogprob, 状态序列 = model.decode(观测序列, algorithm=\"viterbi\")\nprint(\"最可能的状态序列:\", 状态序列)",
        "detail": "HMM.test01",
        "documentation": {}
    },
    {
        "label": "model.transmat_",
        "kind": 5,
        "importPath": "HMM.test01",
        "description": "HMM.test01",
        "peekOfCode": "model.transmat_ = np.array([[0.7, 0.3],\n                            [0.4, 0.6]])\nmodel.emissionprob_ = np.array([[0.1, 0.4, 0.5],\n                                [0.6, 0.3, 0.1]])\n# 生成一些观测数据\n观测序列 = np.array([[0], [1], [2], [1]])\n# 使用模型解码，找到最可能的状态序列\nlogprob, 状态序列 = model.decode(观测序列, algorithm=\"viterbi\")\nprint(\"最可能的状态序列:\", 状态序列)",
        "detail": "HMM.test01",
        "documentation": {}
    },
    {
        "label": "model.emissionprob_",
        "kind": 5,
        "importPath": "HMM.test01",
        "description": "HMM.test01",
        "peekOfCode": "model.emissionprob_ = np.array([[0.1, 0.4, 0.5],\n                                [0.6, 0.3, 0.1]])\n# 生成一些观测数据\n观测序列 = np.array([[0], [1], [2], [1]])\n# 使用模型解码，找到最可能的状态序列\nlogprob, 状态序列 = model.decode(观测序列, algorithm=\"viterbi\")\nprint(\"最可能的状态序列:\", 状态序列)",
        "detail": "HMM.test01",
        "documentation": {}
    },
    {
        "label": "观测序列",
        "kind": 5,
        "importPath": "HMM.test01",
        "description": "HMM.test01",
        "peekOfCode": "观测序列 = np.array([[0], [1], [2], [1]])\n# 使用模型解码，找到最可能的状态序列\nlogprob, 状态序列 = model.decode(观测序列, algorithm=\"viterbi\")\nprint(\"最可能的状态序列:\", 状态序列)",
        "detail": "HMM.test01",
        "documentation": {}
    },
    {
        "label": "config_list",
        "kind": 5,
        "importPath": "autogen.test01",
        "description": "autogen.test01",
        "peekOfCode": "config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")\n# You can also set config_list directly as a list, for example, config_list = [{'model': 'gpt-4', 'api_key': '<your OpenAI API key here>'},]\nassistant = AssistantAgent(\n    \"assistant\",\n    llm_config={\"config_list\": config_list},\n    code_execution_config={\"work_dir\": \".\", \"use_docker\": False},\n)\nuser_proxy = UserProxyAgent(\n    \"user_proxy\", code_execution_config={\"work_dir\": \".\", \"use_docker\": False}\n)  # IMPORTANT: set to True to run code in docker, recommended",
        "detail": "autogen.test01",
        "documentation": {}
    },
    {
        "label": "assistant",
        "kind": 5,
        "importPath": "autogen.test01",
        "description": "autogen.test01",
        "peekOfCode": "assistant = AssistantAgent(\n    \"assistant\",\n    llm_config={\"config_list\": config_list},\n    code_execution_config={\"work_dir\": \".\", \"use_docker\": False},\n)\nuser_proxy = UserProxyAgent(\n    \"user_proxy\", code_execution_config={\"work_dir\": \".\", \"use_docker\": False}\n)  # IMPORTANT: set to True to run code in docker, recommended\nuser_proxy.initiate_chat(\n    assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\"",
        "detail": "autogen.test01",
        "documentation": {}
    },
    {
        "label": "user_proxy",
        "kind": 5,
        "importPath": "autogen.test01",
        "description": "autogen.test01",
        "peekOfCode": "user_proxy = UserProxyAgent(\n    \"user_proxy\", code_execution_config={\"work_dir\": \".\", \"use_docker\": False}\n)  # IMPORTANT: set to True to run code in docker, recommended\nuser_proxy.initiate_chat(\n    assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\"\n)\n# This initiates an automated chat between the two agents to solve the task",
        "detail": "autogen.test01",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "autogen.test02",
        "description": "autogen.test02",
        "peekOfCode": "def main():\n    # Load LLM inference endpoints from an env variable or a file\n    # See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints\n    # and OAI_CONFIG_LIST_sample.\n    # For example, if you have created a OAI_CONFIG_LIST file in the current working directory, that file will be used.\n    config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")\n    # Create the agent that uses the LLM.\n    assistant = ConversableAgent(\n        \"agent\",\n        llm_config={\"config_list\": config_list},",
        "detail": "autogen.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "autogen.test02",
        "description": "autogen.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\ndef main():\n    # Load LLM inference endpoints from an env variable or a file\n    # See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints\n    # and OAI_CONFIG_LIST_sample.\n    # For example, if you have created a OAI_CONFIG_LIST file in the current working directory, that file will be used.\n    config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")\n    # Create the agent that uses the LLM.\n    assistant = ConversableAgent(\n        \"agent\",",
        "detail": "autogen.test02",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "chatglm.test",
        "description": "chatglm.test",
        "peekOfCode": "template = \"\"\"{question}\"\"\"\nprompt = PromptTemplate.from_template(template)\n# endpoint_url = \"http://127.0.0.1:8000/v1/chat/completions\"\n# endpoint_url = \"https://open.bigmodel.cn/api/paas/v4/chat/completions\"\nendpoint_url = \"https://open.bigmodel.cn/api/paas/v4/chat/completions\"\nmessages = [\n    AIMessage(content=\"我将从美国到中国来旅游，出行前希望了解中国的城市\"),\n    AIMessage(content=\"欢迎问我任何问题。\"),\n]\nllm = ChatGLM3(",
        "detail": "chatglm.test",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "chatglm.test",
        "description": "chatglm.test",
        "peekOfCode": "prompt = PromptTemplate.from_template(template)\n# endpoint_url = \"http://127.0.0.1:8000/v1/chat/completions\"\n# endpoint_url = \"https://open.bigmodel.cn/api/paas/v4/chat/completions\"\nendpoint_url = \"https://open.bigmodel.cn/api/paas/v4/chat/completions\"\nmessages = [\n    AIMessage(content=\"我将从美国到中国来旅游，出行前希望了解中国的城市\"),\n    AIMessage(content=\"欢迎问我任何问题。\"),\n]\nllm = ChatGLM3(\n    endpoint_url=endpoint_url,",
        "detail": "chatglm.test",
        "documentation": {}
    },
    {
        "label": "endpoint_url",
        "kind": 5,
        "importPath": "chatglm.test",
        "description": "chatglm.test",
        "peekOfCode": "endpoint_url = \"https://open.bigmodel.cn/api/paas/v4/chat/completions\"\nmessages = [\n    AIMessage(content=\"我将从美国到中国来旅游，出行前希望了解中国的城市\"),\n    AIMessage(content=\"欢迎问我任何问题。\"),\n]\nllm = ChatGLM3(\n    endpoint_url=endpoint_url,\n    max_tokens=80000,\n    prefix_messages=messages,\n    top_p=0.9,",
        "detail": "chatglm.test",
        "documentation": {}
    },
    {
        "label": "messages",
        "kind": 5,
        "importPath": "chatglm.test",
        "description": "chatglm.test",
        "peekOfCode": "messages = [\n    AIMessage(content=\"我将从美国到中国来旅游，出行前希望了解中国的城市\"),\n    AIMessage(content=\"欢迎问我任何问题。\"),\n]\nllm = ChatGLM3(\n    endpoint_url=endpoint_url,\n    max_tokens=80000,\n    prefix_messages=messages,\n    top_p=0.9,\n)",
        "detail": "chatglm.test",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "chatglm.test",
        "description": "chatglm.test",
        "peekOfCode": "llm = ChatGLM3(\n    endpoint_url=endpoint_url,\n    max_tokens=80000,\n    prefix_messages=messages,\n    top_p=0.9,\n)\nllm_chain = LLMChain(prompt=prompt, llm=llm)\nquestion = \"北京和上海两座城市有什么不同？\"\nresult=llm_chain.run(question)\nprint(result)",
        "detail": "chatglm.test",
        "documentation": {}
    },
    {
        "label": "llm_chain",
        "kind": 5,
        "importPath": "chatglm.test",
        "description": "chatglm.test",
        "peekOfCode": "llm_chain = LLMChain(prompt=prompt, llm=llm)\nquestion = \"北京和上海两座城市有什么不同？\"\nresult=llm_chain.run(question)\nprint(result)",
        "detail": "chatglm.test",
        "documentation": {}
    },
    {
        "label": "question",
        "kind": 5,
        "importPath": "chatglm.test",
        "description": "chatglm.test",
        "peekOfCode": "question = \"北京和上海两座城市有什么不同？\"\nresult=llm_chain.run(question)\nprint(result)",
        "detail": "chatglm.test",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "chatglm.test01",
        "description": "chatglm.test01",
        "peekOfCode": "client = ZhipuAI(\n    api_key=\"2544ea141595d420ca10f891833096ac.deW5ICKFVh4JivTN\"\n)  # 填写您自己的APIKey\nresponse = client.chat.completions.create(\n    model=\"glm-4\",  # 填写需要调用的模型名称\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"作为一名营销专家，请为我的产品创作一个吸引人的slogan\",\n        },",
        "detail": "chatglm.test01",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "chatglm.test01",
        "description": "chatglm.test01",
        "peekOfCode": "response = client.chat.completions.create(\n    model=\"glm-4\",  # 填写需要调用的模型名称\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"作为一名营销专家，请为我的产品创作一个吸引人的slogan\",\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"当然，为了创作一个吸引人的slogan，请告诉我一些关于您产品的信息\",",
        "detail": "chatglm.test01",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "chatglm.test02",
        "description": "chatglm.test02",
        "peekOfCode": "client = ZhipuAI(\n    api_key=\"2544ea141595d420ca10f891833096ac.deW5ICKFVh4JivTN\"\n)  # 请填写您自己的APIKey\nresponse = client.chat.completions.create(\n    model=\"glm-4\",  # 填写需要调用的模型名称\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"你是一个乐于解答各种问题的助手，你的任务是为用户提供专业、准确、有见地的建议。\",\n        },",
        "detail": "chatglm.test02",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "chatglm.test02",
        "description": "chatglm.test02",
        "peekOfCode": "response = client.chat.completions.create(\n    model=\"glm-4\",  # 填写需要调用的模型名称\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"你是一个乐于解答各种问题的助手，你的任务是为用户提供专业、准确、有见地的建议。\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"我对太阳系的行星非常感兴趣，特别是土星。请提供关于土星的基本信息，包括其大小、组成、环系统和任何独特的天文现象。\",",
        "detail": "chatglm.test02",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "chatglm.test03",
        "description": "chatglm.test03",
        "peekOfCode": "client = ZhipuAI(\n    api_key=\"2544ea141595d420ca10f891833096ac.deW5ICKFVh4JivTN\"\n)  # 请填写您自己的APIKey\nresponse = client.chat.completions.create(\n    model=\"glm-4\",  # 填写需要调用的模型名称\n    messages=[\n        {\"role\": \"user\", \"content\": \"你好！你叫什么名字\"},\n    ],\n    stream=True,\n)",
        "detail": "chatglm.test03",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "chatglm.test03",
        "description": "chatglm.test03",
        "peekOfCode": "response = client.chat.completions.create(\n    model=\"glm-4\",  # 填写需要调用的模型名称\n    messages=[\n        {\"role\": \"user\", \"content\": \"你好！你叫什么名字\"},\n    ],\n    stream=True,\n)\nfor chunk in response:\n    print(chunk.choices[0].delta)",
        "detail": "chatglm.test03",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "chatglm.test04",
        "description": "chatglm.test04",
        "peekOfCode": "client = ZhipuAI(\n    api_key=\"2544ea141595d420ca10f891833096ac.deW5ICKFVh4JivTN\"\n)  # 请填写您自己的APIKey\nresponse = client.chat.completions.create(\n    model=\"glm-4\",  # 填写需要调用的模型名称\n    messages=[\n        {\"role\": \"system\", \"content\": \"你是一个聪明且富有创造力的小说作家\"},\n        {\n            \"role\": \"user\",\n            \"content\": \"请你作为童话故事大王，写一篇短篇童话故事，故事的主题是要永远保持一颗善良的心，要能够激发儿童的学习兴趣和想象力，同时也能够帮助儿童更好地理解和接受故事中所蕴含的道理和价值观。\",",
        "detail": "chatglm.test04",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "chatglm.test04",
        "description": "chatglm.test04",
        "peekOfCode": "response = client.chat.completions.create(\n    model=\"glm-4\",  # 填写需要调用的模型名称\n    messages=[\n        {\"role\": \"system\", \"content\": \"你是一个聪明且富有创造力的小说作家\"},\n        {\n            \"role\": \"user\",\n            \"content\": \"请你作为童话故事大王，写一篇短篇童话故事，故事的主题是要永远保持一颗善良的心，要能够激发儿童的学习兴趣和想象力，同时也能够帮助儿童更好地理解和接受故事中所蕴含的道理和价值观。\",\n        },\n    ],\n    stream=True,",
        "detail": "chatglm.test04",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "chatglm.test05",
        "description": "chatglm.test05",
        "peekOfCode": "client = ZhipuAI(\n    api_key=\"2544ea141595d420ca10f891833096ac.deW5ICKFVh4JivTN\"\n)  # 请填写您自己的APIKey\nresponse = client.chat.completions.create(\n    model=\"glm-4\",  # 填写需要调用的模型名称\n    messages=[\n        {\"role\": \"user\", \"content\": \"你好！你叫什么名字\"},\n    ],\n    tools=[\n        {",
        "detail": "chatglm.test05",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "chatglm.test05",
        "description": "chatglm.test05",
        "peekOfCode": "response = client.chat.completions.create(\n    model=\"glm-4\",  # 填写需要调用的模型名称\n    messages=[\n        {\"role\": \"user\", \"content\": \"你好！你叫什么名字\"},\n    ],\n    tools=[\n        {\n            \"type\": \"retrieval\",\n            \"retrieval\": '''\n                \"knowledge_id\": \"your knowledge id\",",
        "detail": "chatglm.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"ACTIVELOOP_TOKEN\"\n] = \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwNjA4Mjc0NywiZXhwIjoxNzM4MzA5OTM0fQ.eyJpZCI6InlhbmdsaW5sb25nNiJ9.SNbGhqyihTBsiGhk6SOobDtvWwl0mxW-8YIokUGlvZdu2Z4-db4bnMWK_LIexTwZHHQeYmdHNlqE3gulUULTeg\"\n# Please manually enter OpenAI Key\nfrom langchain.document_loaders import TextLoader\n# root_dir = \"../\"\nroot_dir = \"H:\\\\WorkSpaces\\\\temp\\\\develop\\\\car-service-ordercenter\"",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"ACTIVELOOP_TOKEN\"\n] = \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwNjA4Mjc0NywiZXhwIjoxNzM4MzA5OTM0fQ.eyJpZCI6InlhbmdsaW5sb25nNiJ9.SNbGhqyihTBsiGhk6SOobDtvWwl0mxW-8YIokUGlvZdu2Z4-db4bnMWK_LIexTwZHHQeYmdHNlqE3gulUULTeg\"\n# Please manually enter OpenAI Key\nfrom langchain.document_loaders import TextLoader\n# root_dir = \"../\"\nroot_dir = \"H:\\\\WorkSpaces\\\\temp\\\\develop\\\\car-service-ordercenter\"\ndocs = []\nfor dirpath, dirnames, filenames in os.walk(root_dir):",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "] = \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwNjA4Mjc0NywiZXhwIjoxNzM4MzA5OTM0fQ.eyJpZCI6InlhbmdsaW5sb25nNiJ9.SNbGhqyihTBsiGhk6SOobDtvWwl0mxW-8YIokUGlvZdu2Z4-db4bnMWK_LIexTwZHHQeYmdHNlqE3gulUULTeg\"\n# Please manually enter OpenAI Key\nfrom langchain.document_loaders import TextLoader\n# root_dir = \"../\"\nroot_dir = \"H:\\\\WorkSpaces\\\\temp\\\\develop\\\\car-service-ordercenter\"\ndocs = []\nfor dirpath, dirnames, filenames in os.walk(root_dir):\n    print(\"filenames\", filenames)\n    for file in filenames:\n        # if file.endswith(\".py\") and \"/.venv/\" not in dirpath:",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "root_dir",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "root_dir = \"H:\\\\WorkSpaces\\\\temp\\\\develop\\\\car-service-ordercenter\"\ndocs = []\nfor dirpath, dirnames, filenames in os.walk(root_dir):\n    print(\"filenames\", filenames)\n    for file in filenames:\n        # if file.endswith(\".py\") and \"/.venv/\" not in dirpath:\n        if (\n            file.endswith(\".java\")\n            or file.endswith(\".xml\")\n            or file.endswith(\".properties\")",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "docs",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "docs = []\nfor dirpath, dirnames, filenames in os.walk(root_dir):\n    print(\"filenames\", filenames)\n    for file in filenames:\n        # if file.endswith(\".py\") and \"/.venv/\" not in dirpath:\n        if (\n            file.endswith(\".java\")\n            or file.endswith(\".xml\")\n            or file.endswith(\".properties\")\n            and \"/.venv/\" not in dirpath",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "text_splitter",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ntexts = text_splitter.split_documents(docs)\nprint(f\"{len(texts)}\")\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nembeddings = OpenAIEmbeddings()\nfrom langchain.vectorstores import DeepLake\ndb = DeepLake.from_documents(\n    # texts, embeddings, dataset_path=f\"hub://yanglinlong6/my-code\"\n    texts,\n    embeddings,",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "texts",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "texts = text_splitter.split_documents(docs)\nprint(f\"{len(texts)}\")\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nembeddings = OpenAIEmbeddings()\nfrom langchain.vectorstores import DeepLake\ndb = DeepLake.from_documents(\n    # texts, embeddings, dataset_path=f\"hub://yanglinlong6/my-code\"\n    texts,\n    embeddings,\n    dataset_path=f\"hub://yanglinlong6/car-service-ordercenter\",",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "embeddings",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "embeddings = OpenAIEmbeddings()\nfrom langchain.vectorstores import DeepLake\ndb = DeepLake.from_documents(\n    # texts, embeddings, dataset_path=f\"hub://yanglinlong6/my-code\"\n    texts,\n    embeddings,\n    dataset_path=f\"hub://yanglinlong6/car-service-ordercenter\",\n)\nprint(db)",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "db",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "db = DeepLake.from_documents(\n    # texts, embeddings, dataset_path=f\"hub://yanglinlong6/my-code\"\n    texts,\n    embeddings,\n    dataset_path=f\"hub://yanglinlong6/car-service-ordercenter\",\n)\nprint(db)",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "filter",
        "kind": 2,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "def filter(x):\n    # filter based on source code\n    if \"something\" in x[\"text\"].data()[\"value\"]:\n        return False\n    # filter based on path e.g. extension\n    metadata = x[\"metadata\"].data()[\"value\"]\n    return \"only_this\" in metadata[\"source\"] or \"also_that\" in metadata[\"source\"]\n### turn on below for custom filtering\n# retriever.search_kwargs['filter'] = filter\nfrom langchain.chains import ConversationalRetrievalChain",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"ACTIVELOOP_TOKEN\"\n] = \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwNjA4Mjc0NywiZXhwIjoxNzM4MzA5OTM0fQ.eyJpZCI6InlhbmdsaW5sb25nNiJ9.SNbGhqyihTBsiGhk6SOobDtvWwl0mxW-8YIokUGlvZdu2Z4-db4bnMWK_LIexTwZHHQeYmdHNlqE3gulUULTeg\"\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import DeepLake\nembeddings = OpenAIEmbeddings()\ndb = DeepLake(",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"ACTIVELOOP_TOKEN\"\n] = \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwNjA4Mjc0NywiZXhwIjoxNzM4MzA5OTM0fQ.eyJpZCI6InlhbmdsaW5sb25nNiJ9.SNbGhqyihTBsiGhk6SOobDtvWwl0mxW-8YIokUGlvZdu2Z4-db4bnMWK_LIexTwZHHQeYmdHNlqE3gulUULTeg\"\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import DeepLake\nembeddings = OpenAIEmbeddings()\ndb = DeepLake(\n    # dataset_path=f\"hub://yanglinlong6/my-code\",\n    dataset_path=f\"hub://yanglinlong6/car-service-ordercenter\",",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "] = \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwNjA4Mjc0NywiZXhwIjoxNzM4MzA5OTM0fQ.eyJpZCI6InlhbmdsaW5sb25nNiJ9.SNbGhqyihTBsiGhk6SOobDtvWwl0mxW-8YIokUGlvZdu2Z4-db4bnMWK_LIexTwZHHQeYmdHNlqE3gulUULTeg\"\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import DeepLake\nembeddings = OpenAIEmbeddings()\ndb = DeepLake(\n    # dataset_path=f\"hub://yanglinlong6/my-code\",\n    dataset_path=f\"hub://yanglinlong6/car-service-ordercenter\",\n    read_only=True,\n    embedding_function=embeddings,\n)",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "embeddings",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "embeddings = OpenAIEmbeddings()\ndb = DeepLake(\n    # dataset_path=f\"hub://yanglinlong6/my-code\",\n    dataset_path=f\"hub://yanglinlong6/car-service-ordercenter\",\n    read_only=True,\n    embedding_function=embeddings,\n)\nprint(\"db\", db)\n# Dataset(\n#     path=\"hub://yanglinlong6/langchain-code\",",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "db",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "db = DeepLake(\n    # dataset_path=f\"hub://yanglinlong6/my-code\",\n    dataset_path=f\"hub://yanglinlong6/car-service-ordercenter\",\n    read_only=True,\n    embedding_function=embeddings,\n)\nprint(\"db\", db)\n# Dataset(\n#     path=\"hub://yanglinlong6/langchain-code\",\n#     read_only=True,",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "retriever",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "retriever = db.as_retriever()\nretriever.search_kwargs[\"distance_metric\"] = \"cos\"\nretriever.search_kwargs[\"fetch_k\"] = 20\nretriever.search_kwargs[\"maximal_marginal_relevance\"] = True\nretriever.search_kwargs[\"k\"] = 20\ndef filter(x):\n    # filter based on source code\n    if \"something\" in x[\"text\"].data()[\"value\"]:\n        return False\n    # filter based on path e.g. extension",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "retriever.search_kwargs[\"distance_metric\"]",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "retriever.search_kwargs[\"distance_metric\"] = \"cos\"\nretriever.search_kwargs[\"fetch_k\"] = 20\nretriever.search_kwargs[\"maximal_marginal_relevance\"] = True\nretriever.search_kwargs[\"k\"] = 20\ndef filter(x):\n    # filter based on source code\n    if \"something\" in x[\"text\"].data()[\"value\"]:\n        return False\n    # filter based on path e.g. extension\n    metadata = x[\"metadata\"].data()[\"value\"]",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "retriever.search_kwargs[\"fetch_k\"]",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "retriever.search_kwargs[\"fetch_k\"] = 20\nretriever.search_kwargs[\"maximal_marginal_relevance\"] = True\nretriever.search_kwargs[\"k\"] = 20\ndef filter(x):\n    # filter based on source code\n    if \"something\" in x[\"text\"].data()[\"value\"]:\n        return False\n    # filter based on path e.g. extension\n    metadata = x[\"metadata\"].data()[\"value\"]\n    return \"only_this\" in metadata[\"source\"] or \"also_that\" in metadata[\"source\"]",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "retriever.search_kwargs[\"maximal_marginal_relevance\"]",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "retriever.search_kwargs[\"maximal_marginal_relevance\"] = True\nretriever.search_kwargs[\"k\"] = 20\ndef filter(x):\n    # filter based on source code\n    if \"something\" in x[\"text\"].data()[\"value\"]:\n        return False\n    # filter based on path e.g. extension\n    metadata = x[\"metadata\"].data()[\"value\"]\n    return \"only_this\" in metadata[\"source\"] or \"also_that\" in metadata[\"source\"]\n### turn on below for custom filtering",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "retriever.search_kwargs[\"k\"]",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "retriever.search_kwargs[\"k\"] = 20\ndef filter(x):\n    # filter based on source code\n    if \"something\" in x[\"text\"].data()[\"value\"]:\n        return False\n    # filter based on path e.g. extension\n    metadata = x[\"metadata\"].data()[\"value\"]\n    return \"only_this\" in metadata[\"source\"] or \"also_that\" in metadata[\"source\"]\n### turn on below for custom filtering\n# retriever.search_kwargs['filter'] = filter",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "model = ChatOpenAI(model_name=\"gpt-4\")  # 'ada' 'gpt-3.5-turbo' 'gpt-4',\nqa = ConversationalRetrievalChain.from_llm(model, retriever=retriever)\nquestions = [\n    # \"What is the class hierarchy?\",\n    # \"简要总结一下这个项目\"\n    # \"简要总结一下这个项目每个模块,总结文字尽量在100字以内\"\n    \"简要说明一下CarOrderApiController这个类的作用,总结文字尽量在150字以内\"\n    # \"What classes are derived from the Chain class?\",\n    # \"What classes and functions in the ./langchain/utilities/ forlder are not covered by unit tests?\",\n    # \"What one improvement do you propose in code in relation to the class herarchy for the Chain class?\",",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "qa",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "qa = ConversationalRetrievalChain.from_llm(model, retriever=retriever)\nquestions = [\n    # \"What is the class hierarchy?\",\n    # \"简要总结一下这个项目\"\n    # \"简要总结一下这个项目每个模块,总结文字尽量在100字以内\"\n    \"简要说明一下CarOrderApiController这个类的作用,总结文字尽量在150字以内\"\n    # \"What classes are derived from the Chain class?\",\n    # \"What classes and functions in the ./langchain/utilities/ forlder are not covered by unit tests?\",\n    # \"What one improvement do you propose in code in relation to the class herarchy for the Chain class?\",\n]",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "questions",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "questions = [\n    # \"What is the class hierarchy?\",\n    # \"简要总结一下这个项目\"\n    # \"简要总结一下这个项目每个模块,总结文字尽量在100字以内\"\n    \"简要说明一下CarOrderApiController这个类的作用,总结文字尽量在150字以内\"\n    # \"What classes are derived from the Chain class?\",\n    # \"What classes and functions in the ./langchain/utilities/ forlder are not covered by unit tests?\",\n    # \"What one improvement do you propose in code in relation to the class herarchy for the Chain class?\",\n]\nchat_history = [(\"项目\", \" 总结\")]",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "chat_history",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "chat_history = [(\"项目\", \" 总结\")]\nfor question in questions:\n    result = qa({\"question\": question, \"chat_history\": chat_history})\n    # result = qa({\"question\": question})\n    chat_history.append((question, result[\"answer\"]))\n    print(f\"-> \\*\\*Question\\*\\*: {question} \")\n    print(f\"\\*\\*Answer\\*\\*: {result['answer']} \")",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "get_files_in_directory",
        "kind": 2,
        "importPath": "deeplake.test03",
        "description": "deeplake.test03",
        "peekOfCode": "def get_files_in_directory(directory):\n    file_paths = []\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            file_path = os.path.join(root, file)\n            file_paths.append(file_path)\n    return file_paths\ndirectory_path = \"H:\\\\WorkSpaces\\\\temp\\\\develop\\\\car-service-ordercenter\"\nfiles = get_files_in_directory(directory_path)\n# 打印文件路径",
        "detail": "deeplake.test03",
        "documentation": {}
    },
    {
        "label": "directory_path",
        "kind": 5,
        "importPath": "deeplake.test03",
        "description": "deeplake.test03",
        "peekOfCode": "directory_path = \"H:\\\\WorkSpaces\\\\temp\\\\develop\\\\car-service-ordercenter\"\nfiles = get_files_in_directory(directory_path)\n# 打印文件路径\nfor file in files:\n    print(file)",
        "detail": "deeplake.test03",
        "documentation": {}
    },
    {
        "label": "files",
        "kind": 5,
        "importPath": "deeplake.test03",
        "description": "deeplake.test03",
        "peekOfCode": "files = get_files_in_directory(directory_path)\n# 打印文件路径\nfor file in files:\n    print(file)",
        "detail": "deeplake.test03",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "gtts.test01",
        "description": "gtts.test01",
        "peekOfCode": "text = \"你好,杨林龙\"\nlang = 'zh-cn'\ntts = gTTS(text=text, lang=lang, slow=False)\ntts.save(\"hello.mp3\")\nos.system(\"start hello.mp3\")  # 在 Windows 上播放音频文件",
        "detail": "gtts.test01",
        "documentation": {}
    },
    {
        "label": "lang",
        "kind": 5,
        "importPath": "gtts.test01",
        "description": "gtts.test01",
        "peekOfCode": "lang = 'zh-cn'\ntts = gTTS(text=text, lang=lang, slow=False)\ntts.save(\"hello.mp3\")\nos.system(\"start hello.mp3\")  # 在 Windows 上播放音频文件",
        "detail": "gtts.test01",
        "documentation": {}
    },
    {
        "label": "tts",
        "kind": 5,
        "importPath": "gtts.test01",
        "description": "gtts.test01",
        "peekOfCode": "tts = gTTS(text=text, lang=lang, slow=False)\ntts.save(\"hello.mp3\")\nos.system(\"start hello.mp3\")  # 在 Windows 上播放音频文件",
        "detail": "gtts.test01",
        "documentation": {}
    },
    {
        "label": "engine",
        "kind": 5,
        "importPath": "gtts.test02",
        "description": "gtts.test02",
        "peekOfCode": "engine = pyttsx3.init()\nengine.say(\"Hello, world!\")\nengine.runAndWait()",
        "detail": "gtts.test02",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "gtts.test03",
        "description": "gtts.test03",
        "peekOfCode": "text = \"Hello, world!\"\ncommand = f'echo \"{text}\" | festival --tts'\nprocess = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)\noutput, error = process.communicate()\nwith open('hello.wav', 'wb') as f:\n    f.write(output)\nsubprocess.call(['afplay', 'hello.wav'])  # 在 macOS 上播放音频文件",
        "detail": "gtts.test03",
        "documentation": {}
    },
    {
        "label": "command",
        "kind": 5,
        "importPath": "gtts.test03",
        "description": "gtts.test03",
        "peekOfCode": "command = f'echo \"{text}\" | festival --tts'\nprocess = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)\noutput, error = process.communicate()\nwith open('hello.wav', 'wb') as f:\n    f.write(output)\nsubprocess.call(['afplay', 'hello.wav'])  # 在 macOS 上播放音频文件",
        "detail": "gtts.test03",
        "documentation": {}
    },
    {
        "label": "process",
        "kind": 5,
        "importPath": "gtts.test03",
        "description": "gtts.test03",
        "peekOfCode": "process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)\noutput, error = process.communicate()\nwith open('hello.wav', 'wb') as f:\n    f.write(output)\nsubprocess.call(['afplay', 'hello.wav'])  # 在 macOS 上播放音频文件",
        "detail": "gtts.test03",
        "documentation": {}
    },
    {
        "label": "engine",
        "kind": 5,
        "importPath": "gtts.test04",
        "description": "gtts.test04",
        "peekOfCode": "engine = pyttsx3.init()\nengine.setProperty('rate', 120)  # 设置语速\nengine.setProperty('volume', 1.0)  # 设置音量\ntext = \"Hello, world!\"\nengine.say(text)\nengine.runAndWait()",
        "detail": "gtts.test04",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "gtts.test04",
        "description": "gtts.test04",
        "peekOfCode": "text = \"Hello, world!\"\nengine.say(text)\nengine.runAndWait()",
        "detail": "gtts.test04",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "gtts.test05",
        "description": "gtts.test05",
        "peekOfCode": "text = \"Hello, world!\"\nurl = \"https://api.us-south.text-to-speech.watson.cloud.ibm.com/instances/<instance_id>/v1/synthesize\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer <access_token>\"\n}\ndata = {\"text\": text, \"voice\": \"en-US_AllisonVoice\", \"accept\": \"audio/wav\"}\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nif response.status_code == 200:\n    with open(\"output.wav\", \"wb\") as f:",
        "detail": "gtts.test05",
        "documentation": {}
    },
    {
        "label": "url",
        "kind": 5,
        "importPath": "gtts.test05",
        "description": "gtts.test05",
        "peekOfCode": "url = \"https://api.us-south.text-to-speech.watson.cloud.ibm.com/instances/<instance_id>/v1/synthesize\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer <access_token>\"\n}\ndata = {\"text\": text, \"voice\": \"en-US_AllisonVoice\", \"accept\": \"audio/wav\"}\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nif response.status_code == 200:\n    with open(\"output.wav\", \"wb\") as f:\n        f.write(response.content)",
        "detail": "gtts.test05",
        "documentation": {}
    },
    {
        "label": "headers",
        "kind": 5,
        "importPath": "gtts.test05",
        "description": "gtts.test05",
        "peekOfCode": "headers = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer <access_token>\"\n}\ndata = {\"text\": text, \"voice\": \"en-US_AllisonVoice\", \"accept\": \"audio/wav\"}\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nif response.status_code == 200:\n    with open(\"output.wav\", \"wb\") as f:\n        f.write(response.content)\n    # 播放音频文件",
        "detail": "gtts.test05",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "gtts.test05",
        "description": "gtts.test05",
        "peekOfCode": "data = {\"text\": text, \"voice\": \"en-US_AllisonVoice\", \"accept\": \"audio/wav\"}\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nif response.status_code == 200:\n    with open(\"output.wav\", \"wb\") as f:\n        f.write(response.content)\n    # 播放音频文件\n    subprocess.run(['aplay', 'output.wav'])\nelse:\n    print(\"Error:\", response.text)",
        "detail": "gtts.test05",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "gtts.test05",
        "description": "gtts.test05",
        "peekOfCode": "response = requests.post(url, headers=headers, data=json.dumps(data))\nif response.status_code == 200:\n    with open(\"output.wav\", \"wb\") as f:\n        f.write(response.content)\n    # 播放音频文件\n    subprocess.run(['aplay', 'output.wav'])\nelse:\n    print(\"Error:\", response.text)",
        "detail": "gtts.test05",
        "documentation": {}
    },
    {
        "label": "FileDriver",
        "kind": 6,
        "importPath": "gtts.test06",
        "description": "gtts.test06",
        "peekOfCode": "class FileDriver:\n    def __init__(self, voice=None, rate=None, volume=None, output_file=None):\n        super().__init__()\n        self._voice = voice\n        self._rate = rate\n        self._volume = volume\n        self._output_file = output_file\n    def connect(self):\n        engine = pyttsx3.init()\n        engine.setProperty('voice', self._voice)",
        "detail": "gtts.test06",
        "documentation": {}
    },
    {
        "label": "text_to_speech",
        "kind": 2,
        "importPath": "gtts.test06",
        "description": "gtts.test06",
        "peekOfCode": "def text_to_speech(text, voice=None, rate=None, volume=None, output_file=None):\n    with io.BytesIO() as output_stream:\n        driver = FileDriver(voice, rate, volume, output_stream)\n        engine = driver.connect()\n        engine.say(text)\n        engine.runAndWait()\n        engine.stop()\n        engine.disconnect({\"topic\": 1})\n        if output_file:\n            with open(output_file, 'wb') as f:",
        "detail": "gtts.test06",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "gtts.test06",
        "description": "gtts.test06",
        "peekOfCode": "text = \"Hello, world! 我们都是中国人 世界需要我们\"\noutput_file = \"hello_world.wav\"\ntext_to_speech(text, output_file=output_file)",
        "detail": "gtts.test06",
        "documentation": {}
    },
    {
        "label": "output_file",
        "kind": 5,
        "importPath": "gtts.test06",
        "description": "gtts.test06",
        "peekOfCode": "output_file = \"hello_world.wav\"\ntext_to_speech(text, output_file=output_file)",
        "detail": "gtts.test06",
        "documentation": {}
    },
    {
        "label": "AUDIO_FILE",
        "kind": 5,
        "importPath": "gtts.test07",
        "description": "gtts.test07",
        "peekOfCode": "AUDIO_FILE = path.join(path.dirname(path.realpath(__file__)), \"test.wav\")\n# AUDIO_FILE = path.join(path.dirname(path.realpath(__file__)), \"french.aiff\")\n# AUDIO_FILE = path.join(path.dirname(path.realpath(__file__)), \"chinese.flac\")\n# use the audio file as the audio source\nr = sr.Recognizer()\nwith sr.AudioFile(AUDIO_FILE) as source:\n    audio = r.record(source)  # read the entire audio file\n# recognize speech using Sphinx language=\"zh-CN\",\ntry:\n    print(\"Sphinx thinks you said \")",
        "detail": "gtts.test07",
        "documentation": {}
    },
    {
        "label": "r",
        "kind": 5,
        "importPath": "gtts.test07",
        "description": "gtts.test07",
        "peekOfCode": "r = sr.Recognizer()\nwith sr.AudioFile(AUDIO_FILE) as source:\n    audio = r.record(source)  # read the entire audio file\n# recognize speech using Sphinx language=\"zh-CN\",\ntry:\n    print(\"Sphinx thinks you said \")\n    print(r.recognize_sphinx(audio))\n    # pprint(r.recognize_google(audio, show_all=True))\n    audio_info = r.recognize_google(audio, language=\"zh-CN\", show_all=True)\n    # pprint(audio_info)",
        "detail": "gtts.test07",
        "documentation": {}
    },
    {
        "label": "tokenize_function",
        "kind": 2,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "def tokenize_function(examples):\n    return tokenizer(\n        examples[\"sentence1\"],\n        examples[\"sentence2\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=128,\n    )\ntrain_dataset = train_dataset.map(tokenize_function)\nval_dataset = val_dataset.map(tokenize_function)",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "model = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "lora_config",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "lora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)\nval_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "model = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)\nval_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()\n    .select(range(100))",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "train_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)\nval_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()\n    .select(range(100))\n)",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "val_dataset",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "val_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()\n    .select(range(100))\n)\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"sentence1\"],\n        examples[\"sentence2\"],\n        padding=\"max_length\",",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "train_dataset = train_dataset.map(tokenize_function)\nval_dataset = val_dataset.map(tokenize_function)\narguments = TrainingArguments(\n    output_dir=\"./outputs\",\n    per_device_train_batch_size=16,\n)\ntrainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "val_dataset",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "val_dataset = val_dataset.map(tokenize_function)\narguments = TrainingArguments(\n    output_dir=\"./outputs\",\n    per_device_train_batch_size=16,\n)\ntrainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "arguments",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "arguments = TrainingArguments(\n    output_dir=\"./outputs\",\n    per_device_train_batch_size=16,\n)\ntrainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=default_data_collator,",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "trainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=default_data_collator,\n)\ntrainer.train()",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test04",
        "description": "huggingface.test04",
        "peekOfCode": "model = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))",
        "detail": "huggingface.test04",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "huggingface.test04",
        "description": "huggingface.test04",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))",
        "detail": "huggingface.test04",
        "documentation": {}
    },
    {
        "label": "lora_config",
        "kind": 5,
        "importPath": "huggingface.test04",
        "description": "huggingface.test04",
        "peekOfCode": "lora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))",
        "detail": "huggingface.test04",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test04",
        "description": "huggingface.test04",
        "peekOfCode": "model = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))",
        "detail": "huggingface.test04",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "train_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\neval_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\nmodel_id = \"damo/nlp_structbert_sentence-similarity_chinese-base\"\n# 读取model中的cfg文件\ncfg = read_config(model_id)\n# 直接更新其中的参数\ncfg.train.max_epochs = 5\ncfg.preprocessor.train[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.preprocessor.val[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.train.work_dir = \"/tmp\"",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "eval_dataset",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "eval_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\nmodel_id = \"damo/nlp_structbert_sentence-similarity_chinese-base\"\n# 读取model中的cfg文件\ncfg = read_config(model_id)\n# 直接更新其中的参数\ncfg.train.max_epochs = 5\ncfg.preprocessor.train[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.preprocessor.val[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.train.work_dir = \"/tmp\"\ncfg_file = os.path.join(\"/tmp\", \"config.json\")",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "model_id",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "model_id = \"damo/nlp_structbert_sentence-similarity_chinese-base\"\n# 读取model中的cfg文件\ncfg = read_config(model_id)\n# 直接更新其中的参数\ncfg.train.max_epochs = 5\ncfg.preprocessor.train[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.preprocessor.val[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.train.work_dir = \"/tmp\"\ncfg_file = os.path.join(\"/tmp\", \"config.json\")\n# 将参数写入新的配置文件并传入trainer",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "cfg",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "cfg = read_config(model_id)\n# 直接更新其中的参数\ncfg.train.max_epochs = 5\ncfg.preprocessor.train[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.preprocessor.val[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.train.work_dir = \"/tmp\"\ncfg_file = os.path.join(\"/tmp\", \"config.json\")\n# 将参数写入新的配置文件并传入trainer\ncfg.dump(cfg_file)\nkwargs = dict(",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "cfg.train.max_epochs",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "cfg.train.max_epochs = 5\ncfg.preprocessor.train[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.preprocessor.val[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.train.work_dir = \"/tmp\"\ncfg_file = os.path.join(\"/tmp\", \"config.json\")\n# 将参数写入新的配置文件并传入trainer\ncfg.dump(cfg_file)\nkwargs = dict(\n    model=model_id,\n    train_dataset=train_dataset,",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "cfg.preprocessor.train[\"label2id\"]",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "cfg.preprocessor.train[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.preprocessor.val[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.train.work_dir = \"/tmp\"\ncfg_file = os.path.join(\"/tmp\", \"config.json\")\n# 将参数写入新的配置文件并传入trainer\ncfg.dump(cfg_file)\nkwargs = dict(\n    model=model_id,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "cfg.preprocessor.val[\"label2id\"]",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "cfg.preprocessor.val[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.train.work_dir = \"/tmp\"\ncfg_file = os.path.join(\"/tmp\", \"config.json\")\n# 将参数写入新的配置文件并传入trainer\ncfg.dump(cfg_file)\nkwargs = dict(\n    model=model_id,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    cfg_file=cfg_file,",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "cfg.train.work_dir",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "cfg.train.work_dir = \"/tmp\"\ncfg_file = os.path.join(\"/tmp\", \"config.json\")\n# 将参数写入新的配置文件并传入trainer\ncfg.dump(cfg_file)\nkwargs = dict(\n    model=model_id,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    cfg_file=cfg_file,\n)",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "cfg_file",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "cfg_file = os.path.join(\"/tmp\", \"config.json\")\n# 将参数写入新的配置文件并传入trainer\ncfg.dump(cfg_file)\nkwargs = dict(\n    model=model_id,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    cfg_file=cfg_file,\n)\ntrainer = build_trainer(default_args=kwargs)",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "kwargs",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "kwargs = dict(\n    model=model_id,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    cfg_file=cfg_file,\n)\ntrainer = build_trainer(default_args=kwargs)\ntrainer.train()",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "trainer = build_trainer(default_args=kwargs)\ntrainer.train()",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "huggingface.test06",
        "description": "huggingface.test06",
        "peekOfCode": "def main():\n    # 准备数据集\n    from datasets import load_dataset\n    dataset_dict = load_dataset(\"luozhouyang/dureader\", \"robust\")\n    def concat_answer_context(dataset):\n        dataset[\"src_txt\"] = (\n            dataset[\"answers\"][\"text\"][0] + \"[SEP]\" + dataset[\"context\"]\n        )\n        return dataset\n    train_dataset = dataset_dict[\"train\"].map(concat_answer_context)",
        "detail": "huggingface.test06",
        "documentation": {}
    },
    {
        "label": "a",
        "kind": 5,
        "importPath": "huggingface.test07",
        "description": "huggingface.test07",
        "peekOfCode": "a = torch.tensor([1.0], requires_grad=True)\nb = torch.tensor([2.0], requires_grad=True)\nc = a * b\n# 计算梯度\nc.backward()\nprint(a.grad, b.grad)\n# tensor([2.]) tensor([1.])",
        "detail": "huggingface.test07",
        "documentation": {}
    },
    {
        "label": "b",
        "kind": 5,
        "importPath": "huggingface.test07",
        "description": "huggingface.test07",
        "peekOfCode": "b = torch.tensor([2.0], requires_grad=True)\nc = a * b\n# 计算梯度\nc.backward()\nprint(a.grad, b.grad)\n# tensor([2.]) tensor([1.])",
        "detail": "huggingface.test07",
        "documentation": {}
    },
    {
        "label": "c",
        "kind": 5,
        "importPath": "huggingface.test07",
        "description": "huggingface.test07",
        "peekOfCode": "c = a * b\n# 计算梯度\nc.backward()\nprint(a.grad, b.grad)\n# tensor([2.]) tensor([1.])",
        "detail": "huggingface.test07",
        "documentation": {}
    },
    {
        "label": "SubModule",
        "kind": 6,
        "importPath": "huggingface.test08",
        "description": "huggingface.test08",
        "peekOfCode": "class SubModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        # 有时候会传入一个config，下面的Linear就变成：\n        # self.a = Linear(config.hidden_size, config.hidden_size)\n        self.a = Linear(4, 4)\nclass Module(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.sub = SubModule()",
        "detail": "huggingface.test08",
        "documentation": {}
    },
    {
        "label": "Module",
        "kind": 6,
        "importPath": "huggingface.test08",
        "description": "huggingface.test08",
        "peekOfCode": "class Module(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.sub = SubModule()\nmodule = Module()\nstate_dict = module.state_dict()  # 实际上是一个key value对\n# OrderedDict([('sub.a.weight', tensor([[-0.4148, -0.2303, -0.3650, -0.4019],\n#        [-0.2495,  0.1113,  0.3846,  0.3645],\n#        [ 0.0395, -0.0490, -0.1738,  0.0820],\n#        [ 0.4187,  0.4697, -0.4100, -0.4685]])), ('sub.a.bias', tensor([ 0.4756, -0.4298, -0.4380,  0.3344]))])",
        "detail": "huggingface.test08",
        "documentation": {}
    },
    {
        "label": "module",
        "kind": 5,
        "importPath": "huggingface.test08",
        "description": "huggingface.test08",
        "peekOfCode": "module = Module()\nstate_dict = module.state_dict()  # 实际上是一个key value对\n# OrderedDict([('sub.a.weight', tensor([[-0.4148, -0.2303, -0.3650, -0.4019],\n#        [-0.2495,  0.1113,  0.3846,  0.3645],\n#        [ 0.0395, -0.0490, -0.1738,  0.0820],\n#        [ 0.4187,  0.4697, -0.4100, -0.4685]])), ('sub.a.bias', tensor([ 0.4756, -0.4298, -0.4380,  0.3344]))])\n# 如果我想把SubModule替换为别的结构能不能做呢？\nsetattr(module, \"sub\", Linear(4, 4))\n# 这样模型的结构就被动态的改变了\n# 这个就是轻量调优生效的基本原理：新增或改变原有的模型结构，具体可以查看选型或训练章节",
        "detail": "huggingface.test08",
        "documentation": {}
    },
    {
        "label": "state_dict",
        "kind": 5,
        "importPath": "huggingface.test08",
        "description": "huggingface.test08",
        "peekOfCode": "state_dict = module.state_dict()  # 实际上是一个key value对\n# OrderedDict([('sub.a.weight', tensor([[-0.4148, -0.2303, -0.3650, -0.4019],\n#        [-0.2495,  0.1113,  0.3846,  0.3645],\n#        [ 0.0395, -0.0490, -0.1738,  0.0820],\n#        [ 0.4187,  0.4697, -0.4100, -0.4685]])), ('sub.a.bias', tensor([ 0.4756, -0.4298, -0.4380,  0.3344]))])\n# 如果我想把SubModule替换为别的结构能不能做呢？\nsetattr(module, \"sub\", Linear(4, 4))\n# 这样模型的结构就被动态的改变了\n# 这个就是轻量调优生效的基本原理：新增或改变原有的模型结构，具体可以查看选型或训练章节",
        "detail": "huggingface.test08",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test09",
        "description": "huggingface.test09",
        "peekOfCode": "model = AutoModelForCausalLM.from_pretrained(\n    \"qwen/Qwen-1_8B-Chat\", trust_remote_code=True\n)\nmodel.to(0)\n# model.to('cuda:0') 同样也可以\na = torch.tensor([1.0])\na = a.to(0)\n# 注意！model.to操作不需要承接返回值，这是因为torch.nn.Module(模型基类)的这个操作是in-place(替换)的\n# 而tensor的操作不是in-place的，需要承接返回值",
        "detail": "huggingface.test09",
        "documentation": {}
    },
    {
        "label": "a",
        "kind": 5,
        "importPath": "huggingface.test09",
        "description": "huggingface.test09",
        "peekOfCode": "a = torch.tensor([1.0])\na = a.to(0)\n# 注意！model.to操作不需要承接返回值，这是因为torch.nn.Module(模型基类)的这个操作是in-place(替换)的\n# 而tensor的操作不是in-place的，需要承接返回值",
        "detail": "huggingface.test09",
        "documentation": {}
    },
    {
        "label": "a",
        "kind": 5,
        "importPath": "huggingface.test09",
        "description": "huggingface.test09",
        "peekOfCode": "a = a.to(0)\n# 注意！model.to操作不需要承接返回值，这是因为torch.nn.Module(模型基类)的这个操作是in-place(替换)的\n# 而tensor的操作不是in-place的，需要承接返回值",
        "detail": "huggingface.test09",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "device = torch.device(\"cpu\")\n# 创建一个名为 device 的设备对象，表示使用 GPU（如果可用）\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "tokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "model = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "text = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "encoding",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "encoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "outputs",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "outputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "logits",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "logits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "masked_tokens_predictions",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "masked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "MyModule",
        "kind": 6,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "class MyModule(torch.nn.Module):\n    def __init__(self, n_classes=2):\n        # 优先调用基类构造\n        super().__init__()\n        # 单个神经元，一个linear加上一个relu激活\n        self.linear = torch.nn.Linear(16, n_classes)\n        self.relu = torch.nn.ReLU()\n    def forward(self, tensor, label):\n        # 前向过程\n        output = {\"logits\": self.relu(self.linear(tensor))}",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "MyDataset",
        "kind": 6,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "class MyDataset(Dataset):\n    # 长度是5\n    def __len__(self):\n        return 5\n    # 如何根据index取得数据集的数据\n    def __getitem__(self, index):\n        return {\"tensor\": torch.rand(16), \"label\": torch.tensor(1)}\n# 构造模型\nmodel = MyModule()\n# 构造数据集",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "seed = 42\n# 随机种子，影响训练的随机数逻辑，如果随机种子确定，每次训练的结果是一样的\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\n# 确定化cuda、cublas、cudnn的底层随机逻辑\n# 否则CUDA会提前优化一些算子，产生不确定性\n# 这些处理在训练时也可以不使用\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nos.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_LAUNCH_BLOCKING\"]",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nos.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\ntorch.use_deterministic_algorithms(True)\n# Enable CUDNN deterministic mode\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n# torch模型都继承于torch.nn.Module\nclass MyModule(torch.nn.Module):\n    def __init__(self, n_classes=2):\n        # 优先调用基类构造",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\ntorch.use_deterministic_algorithms(True)\n# Enable CUDNN deterministic mode\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n# torch模型都继承于torch.nn.Module\nclass MyModule(torch.nn.Module):\n    def __init__(self, n_classes=2):\n        # 优先调用基类构造\n        super().__init__()",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "torch.backends.cudnn.deterministic",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "torch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n# torch模型都继承于torch.nn.Module\nclass MyModule(torch.nn.Module):\n    def __init__(self, n_classes=2):\n        # 优先调用基类构造\n        super().__init__()\n        # 单个神经元，一个linear加上一个relu激活\n        self.linear = torch.nn.Linear(16, n_classes)\n        self.relu = torch.nn.ReLU()",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "torch.backends.cudnn.benchmark",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "torch.backends.cudnn.benchmark = False\n# torch模型都继承于torch.nn.Module\nclass MyModule(torch.nn.Module):\n    def __init__(self, n_classes=2):\n        # 优先调用基类构造\n        super().__init__()\n        # 单个神经元，一个linear加上一个relu激活\n        self.linear = torch.nn.Linear(16, n_classes)\n        self.relu = torch.nn.ReLU()\n    def forward(self, tensor, label):",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "model = MyModule()\n# 构造数据集\ndataset = MyDataset()\n# 构造dataloader， dataloader会负责从数据集中按照batch_size批量取数，这个batch_size参数就是设置给它的\n# collate_fn会负责将batch中单行的数据进行padding\ndataloader = DataLoader(dataset, batch_size=4, collate_fn=default_collate)\n# optimizer，负责将梯度累加回原来的parameters\n# lr就是设置到这里的\noptimizer = AdamW(model.parameters(), lr=5e-4)\n# lr_scheduler， 负责对learning_rate进行调整",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "dataset",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "dataset = MyDataset()\n# 构造dataloader， dataloader会负责从数据集中按照batch_size批量取数，这个batch_size参数就是设置给它的\n# collate_fn会负责将batch中单行的数据进行padding\ndataloader = DataLoader(dataset, batch_size=4, collate_fn=default_collate)\n# optimizer，负责将梯度累加回原来的parameters\n# lr就是设置到这里的\noptimizer = AdamW(model.parameters(), lr=5e-4)\n# lr_scheduler， 负责对learning_rate进行调整\nlr_scheduler = StepLR(optimizer, 2)\n# 3个epoch，表示对数据集训练三次",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "dataloader",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "dataloader = DataLoader(dataset, batch_size=4, collate_fn=default_collate)\n# optimizer，负责将梯度累加回原来的parameters\n# lr就是设置到这里的\noptimizer = AdamW(model.parameters(), lr=5e-4)\n# lr_scheduler， 负责对learning_rate进行调整\nlr_scheduler = StepLR(optimizer, 2)\n# 3个epoch，表示对数据集训练三次\nfor i in range(3):\n    # 从dataloader取数\n    for batch in dataloader:",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "optimizer",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "optimizer = AdamW(model.parameters(), lr=5e-4)\n# lr_scheduler， 负责对learning_rate进行调整\nlr_scheduler = StepLR(optimizer, 2)\n# 3个epoch，表示对数据集训练三次\nfor i in range(3):\n    # 从dataloader取数\n    for batch in dataloader:\n        # 进行模型forward和loss计算\n        output = model(**batch)\n        # backward过程会对每个可训练的parameters产生梯度",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "lr_scheduler",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "lr_scheduler = StepLR(optimizer, 2)\n# 3个epoch，表示对数据集训练三次\nfor i in range(3):\n    # 从dataloader取数\n    for batch in dataloader:\n        # 进行模型forward和loss计算\n        output = model(**batch)\n        # backward过程会对每个可训练的parameters产生梯度\n        output[\"loss\"].backward()\n        # 建议此时看下model中linear的grad值",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "huggingface.test11",
        "description": "huggingface.test11",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\nmodel = (\n    AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True).half().cuda()\n)\nmodel = model.eval()\nresponse, history = model.chat(tokenizer, \"你好\", history=[])\nprint(response)\n# 👋!我是人工智能助手 ChatGLM-6B,很高兴见到你,欢迎问我任何问题。\nresponse, history = model.chat(tokenizer, \"晚上睡不着应该怎么办\", history=history)\nprint(response)",
        "detail": "huggingface.test11",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test11",
        "description": "huggingface.test11",
        "peekOfCode": "model = (\n    AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True).half().cuda()\n)\nmodel = model.eval()\nresponse, history = model.chat(tokenizer, \"你好\", history=[])\nprint(response)\n# 👋!我是人工智能助手 ChatGLM-6B,很高兴见到你,欢迎问我任何问题。\nresponse, history = model.chat(tokenizer, \"晚上睡不着应该怎么办\", history=history)\nprint(response)",
        "detail": "huggingface.test11",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test11",
        "description": "huggingface.test11",
        "peekOfCode": "model = model.eval()\nresponse, history = model.chat(tokenizer, \"你好\", history=[])\nprint(response)\n# 👋!我是人工智能助手 ChatGLM-6B,很高兴见到你,欢迎问我任何问题。\nresponse, history = model.chat(tokenizer, \"晚上睡不着应该怎么办\", history=history)\nprint(response)",
        "detail": "huggingface.test11",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom transformers import PerceiverTokenizer, PerceiverForMaskedLM\n# 创建一个名为 device 的设备对象，表示使用 CPU\ndevice = torch.device(\"cpu\")",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom transformers import PerceiverTokenizer, PerceiverForMaskedLM\n# 创建一个名为 device 的设备对象，表示使用 CPU\ndevice = torch.device(\"cpu\")\n# 创建一个名为 device 的设备对象，表示使用 GPU（如果可用）",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom transformers import PerceiverTokenizer, PerceiverForMaskedLM\n# 创建一个名为 device 的设备对象，表示使用 CPU\ndevice = torch.device(\"cpu\")\n# 创建一个名为 device 的设备对象，表示使用 GPU（如果可用）\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom transformers import PerceiverTokenizer, PerceiverForMaskedLM\n# 创建一个名为 device 的设备对象，表示使用 CPU\ndevice = torch.device(\"cpu\")\n# 创建一个名为 device 的设备对象，表示使用 GPU（如果可用）\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom transformers import PerceiverTokenizer, PerceiverForMaskedLM\n# 创建一个名为 device 的设备对象，表示使用 CPU\ndevice = torch.device(\"cpu\")\n# 创建一个名为 device 的设备对象，表示使用 GPU（如果可用）\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom transformers import PerceiverTokenizer, PerceiverForMaskedLM\n# 创建一个名为 device 的设备对象，表示使用 CPU\ndevice = torch.device(\"cpu\")\n# 创建一个名为 device 的设备对象，表示使用 GPU（如果可用）\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom transformers import PerceiverTokenizer, PerceiverForMaskedLM\n# 创建一个名为 device 的设备对象，表示使用 CPU\ndevice = torch.device(\"cpu\")\n# 创建一个名为 device 的设备对象，表示使用 GPU（如果可用）\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "device = torch.device(\"cpu\")\n# 创建一个名为 device 的设备对象，表示使用 GPU（如果可用）\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "tokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "model = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "text = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "encoding",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "encoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "outputs",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "outputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "logits",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "logits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "masked_tokens_predictions",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "masked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\nresult = agent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "llm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\nresult = agent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n)\nprint(result)",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\nresult = agent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n)\nprint(result)",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "agent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\nresult = agent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n)\nprint(result)",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "result = agent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n)\nprint(result)",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "FakeAgent",
        "kind": 6,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "class FakeAgent(BaseSingleActionAgent):\n    \"\"\"Fake Custom Agent.\"\"\"\n    @property\n    def input_keys(self):\n        return [\"input\"]\n    def plan(\n        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\n    ) -> Union[AgentAction, AgentFinish]:\n        \"\"\"Given input, decided what to do.\n        Args:",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import Tool, AgentExecutor, BaseSingleActionAgent\nfrom langchain import OpenAI, SerpAPIWrapper\nsearch = SerpAPIWrapper()\ntools = [",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import Tool, AgentExecutor, BaseSingleActionAgent\nfrom langchain import OpenAI, SerpAPIWrapper\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import Tool, AgentExecutor, BaseSingleActionAgent\nfrom langchain import OpenAI, SerpAPIWrapper\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n        return_direct=True,",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "search = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n        return_direct=True,\n    )\n]\nfrom typing import List, Tuple, Any, Union",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "tools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n        return_direct=True,\n    )\n]\nfrom typing import List, Tuple, Any, Union\nfrom langchain.schema import AgentAction, AgentFinish",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "agent = FakeAgent()\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nagent_executor.run(\"How many people live in canada as of 2023?\")",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "agent_executor",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "agent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nagent_executor.run(\"How many people live in canada as of 2023?\")",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "fake_func",
        "kind": 2,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "def fake_func(inp: str) -> str:\n    return \"foo\"\nfake_tools = [\n    Tool(\n        name=f\"foo-{i}\",\n        func=fake_func,\n        description=f\"a silly function that you can use to get more information about the number {i}\",\n    )\n    for i in range(99)\n]",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "get_tools",
        "kind": 2,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "def get_tools(query):\n    docs = retriever.get_relevant_documents(query)\n    return [ALL_TOOLS[d.metadata[\"index\"]] for d in docs]\nprint(get_tools(\"whats the weather?\"))",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nimport re\nfrom typing import List, Union\nfrom langchain import LLMChain, OpenAI, SerpAPIWrapper\nfrom langchain.agents import (AgentExecutor, AgentOutputParser,",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nimport re\nfrom typing import List, Union\nfrom langchain import LLMChain, OpenAI, SerpAPIWrapper\nfrom langchain.agents import (AgentExecutor, AgentOutputParser,\n                              LLMSingleActionAgent, Tool)\nfrom langchain.prompts import StringPromptTemplate",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nimport re\nfrom typing import List, Union\nfrom langchain import LLMChain, OpenAI, SerpAPIWrapper\nfrom langchain.agents import (AgentExecutor, AgentOutputParser,\n                              LLMSingleActionAgent, Tool)\nfrom langchain.prompts import StringPromptTemplate\nfrom langchain.schema import AgentAction, AgentFinish\n# Define which tools the agent can use to answer user queries\nsearch = SerpAPIWrapper()",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "search = SerpAPIWrapper()\nsearch_tool = Tool(\n    name=\"Search\",\n    func=search.run,\n    description=\"useful for when you need to answer questions about current events\",\n)\ndef fake_func(inp: str) -> str:\n    return \"foo\"\nfake_tools = [\n    Tool(",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "search_tool",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "search_tool = Tool(\n    name=\"Search\",\n    func=search.run,\n    description=\"useful for when you need to answer questions about current events\",\n)\ndef fake_func(inp: str) -> str:\n    return \"foo\"\nfake_tools = [\n    Tool(\n        name=f\"foo-{i}\",",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "fake_tools",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "fake_tools = [\n    Tool(\n        name=f\"foo-{i}\",\n        func=fake_func,\n        description=f\"a silly function that you can use to get more information about the number {i}\",\n    )\n    for i in range(99)\n]\nALL_TOOLS = [search_tool] + fake_tools\nfrom langchain.embeddings import OpenAIEmbeddings",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "ALL_TOOLS",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "ALL_TOOLS = [search_tool] + fake_tools\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.schema import Document\nfrom langchain.vectorstores import FAISS\ndocs = [\n    Document(page_content=t.description, metadata={\"index\": i})\n    for i, t in enumerate(ALL_TOOLS)\n]\nvector_store = FAISS.from_documents(docs, OpenAIEmbeddings())\nretriever = vector_store.as_retriever()",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "docs",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "docs = [\n    Document(page_content=t.description, metadata={\"index\": i})\n    for i, t in enumerate(ALL_TOOLS)\n]\nvector_store = FAISS.from_documents(docs, OpenAIEmbeddings())\nretriever = vector_store.as_retriever()\ndef get_tools(query):\n    docs = retriever.get_relevant_documents(query)\n    return [ALL_TOOLS[d.metadata[\"index\"]] for d in docs]\nprint(get_tools(\"whats the weather?\"))",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "vector_store",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "vector_store = FAISS.from_documents(docs, OpenAIEmbeddings())\nretriever = vector_store.as_retriever()\ndef get_tools(query):\n    docs = retriever.get_relevant_documents(query)\n    return [ALL_TOOLS[d.metadata[\"index\"]] for d in docs]\nprint(get_tools(\"whats the weather?\"))",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "retriever",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "retriever = vector_store.as_retriever()\ndef get_tools(query):\n    docs = retriever.get_relevant_documents(query)\n    return [ALL_TOOLS[d.metadata[\"index\"]] for d in docs]\nprint(get_tools(\"whats the weather?\"))",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import ZeroShotAgent, Tool, AgentExecutor\nfrom langchain import OpenAI, SerpAPIWrapper, LLMChain\nsearch = SerpAPIWrapper()\ntools = [",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import ZeroShotAgent, Tool, AgentExecutor\nfrom langchain import OpenAI, SerpAPIWrapper, LLMChain\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import ZeroShotAgent, Tool, AgentExecutor\nfrom langchain import OpenAI, SerpAPIWrapper, LLMChain\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "search = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )\n]\nprefix = \"\"\"Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:\"\"\"\nsuffix = \"\"\"Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Args\"",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "tools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )\n]\nprefix = \"\"\"Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:\"\"\"\nsuffix = \"\"\"Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Args\"\nQuestion: {input}",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "prefix",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "prefix = \"\"\"Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:\"\"\"\nsuffix = \"\"\"Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Args\"\nQuestion: {input}\n{agent_scratchpad}\"\"\"\nprompt = ZeroShotAgent.create_prompt(\n    tools, prefix=prefix, suffix=suffix, input_variables=[\"input\", \"agent_scratchpad\"]\n)\nprint(prompt.template)\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\ntool_names = [tool.name for tool in tools]",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "suffix",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "suffix = \"\"\"Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Args\"\nQuestion: {input}\n{agent_scratchpad}\"\"\"\nprompt = ZeroShotAgent.create_prompt(\n    tools, prefix=prefix, suffix=suffix, input_variables=[\"input\", \"agent_scratchpad\"]\n)\nprint(prompt.template)\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\ntool_names = [tool.name for tool in tools]\nagent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "prompt = ZeroShotAgent.create_prompt(\n    tools, prefix=prefix, suffix=suffix, input_variables=[\"input\", \"agent_scratchpad\"]\n)\nprint(prompt.template)\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\ntool_names = [tool.name for tool in tools]\nagent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "llm_chain",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "llm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\ntool_names = [tool.name for tool in tools]\nagent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nprint(agent_executor.run(\"How many people live in canada as of 2023?\"))",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "tool_names",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "tool_names = [tool.name for tool in tools]\nagent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nprint(agent_executor.run(\"How many people live in canada as of 2023?\"))",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "agent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nprint(agent_executor.run(\"How many people live in canada as of 2023?\"))",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "agent_executor",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "agent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nprint(agent_executor.run(\"How many people live in canada as of 2023?\"))",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import LLMChain, OpenAI, SerpAPIWrapper\nfrom langchain.agents import AgentExecutor, Tool, ZeroShotAgent\nsearch = SerpAPIWrapper()\ntools = [",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import LLMChain, OpenAI, SerpAPIWrapper\nfrom langchain.agents import AgentExecutor, Tool, ZeroShotAgent\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import LLMChain, OpenAI, SerpAPIWrapper\nfrom langchain.agents import AgentExecutor, Tool, ZeroShotAgent\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "search = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )\n]\nprefix = \"\"\"Answer the following questions as best you can. You have access to the following tools:\"\"\"\nsuffix = \"\"\"When answering, you MUST speak in the following language: {language}.",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "tools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )\n]\nprefix = \"\"\"Answer the following questions as best you can. You have access to the following tools:\"\"\"\nsuffix = \"\"\"When answering, you MUST speak in the following language: {language}.\nQuestion: {input}",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "prefix",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "prefix = \"\"\"Answer the following questions as best you can. You have access to the following tools:\"\"\"\nsuffix = \"\"\"When answering, you MUST speak in the following language: {language}.\nQuestion: {input}\n{agent_scratchpad}\"\"\"\nprompt = ZeroShotAgent.create_prompt(\n    tools,\n    prefix=prefix,\n    suffix=suffix,\n    input_variables=[\"input\", \"language\", \"agent_scratchpad\"],\n)",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "suffix",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "suffix = \"\"\"When answering, you MUST speak in the following language: {language}.\nQuestion: {input}\n{agent_scratchpad}\"\"\"\nprompt = ZeroShotAgent.create_prompt(\n    tools,\n    prefix=prefix,\n    suffix=suffix,\n    input_variables=[\"input\", \"language\", \"agent_scratchpad\"],\n)\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "prompt = ZeroShotAgent.create_prompt(\n    tools,\n    prefix=prefix,\n    suffix=suffix,\n    input_variables=[\"input\", \"language\", \"agent_scratchpad\"],\n)\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\nagent = ZeroShotAgent(llm_chain=llm_chain, tools=tools)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "llm_chain",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "llm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\nagent = ZeroShotAgent(llm_chain=llm_chain, tools=tools)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nresult = agent_executor.run(\n    input=\"How many people live in canada as of 2023?\", language=\"italian\"\n)\nprint(result)",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nresult = agent_executor.run(\n    input=\"How many people live in canada as of 2023?\", language=\"italian\"\n)\nprint(result)",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "agent_executor",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "agent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nresult = agent_executor.run(\n    input=\"How many people live in canada as of 2023?\", language=\"italian\"\n)\nprint(result)",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "result = agent_executor.run(\n    input=\"How many people live in canada as of 2023?\", language=\"italian\"\n)\nprint(result)",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "FakeAgent",
        "kind": 6,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "class FakeAgent(BaseMultiActionAgent):\n    \"\"\"Fake Custom Agent.\"\"\"\n    @property\n    def input_keys(self):\n        return [\"input\"]\n    def plan(\n        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\n    ) -> Union[List[AgentAction], AgentFinish]:\n        \"\"\"Given input, decided what to do.\n        Args:",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "random_word",
        "kind": 2,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "def random_word(query: str) -> str:\n    print(\"\\n现在我正在做这个！\")\n    return \"foo\"\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    ),",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import OpenAI, SerpAPIWrapper\nfrom langchain.agents import AgentExecutor, BaseMultiActionAgent, Tool\ndef random_word(query: str) -> str:\n    print(\"\\n现在我正在做这个！\")",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import OpenAI, SerpAPIWrapper\nfrom langchain.agents import AgentExecutor, BaseMultiActionAgent, Tool\ndef random_word(query: str) -> str:\n    print(\"\\n现在我正在做这个！\")\n    return \"foo\"\nsearch = SerpAPIWrapper()",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import OpenAI, SerpAPIWrapper\nfrom langchain.agents import AgentExecutor, BaseMultiActionAgent, Tool\ndef random_word(query: str) -> str:\n    print(\"\\n现在我正在做这个！\")\n    return \"foo\"\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "search = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    ),\n    Tool(\n        name=\"RandomWord\",\n        func=random_word,",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "tools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    ),\n    Tool(\n        name=\"RandomWord\",\n        func=random_word,\n        description=\"call this to get a random word.\",",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "agent = FakeAgent()\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nresult = agent_executor.run(\"How many people live in canada as of 2023?\")\nprint(result)",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "agent_executor",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "agent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nresult = agent_executor.run(\"How many people live in canada as of 2023?\")\nprint(result)",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "result = agent_executor.run(\"How many people live in canada as of 2023?\")\nprint(result)",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test01",
        "description": "langchain.chains.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import APIChain\nfrom langchain.chains.api.prompt import API_RESPONSE_PROMPT\nfrom langchain.llms import OpenAI\nfrom langchain.prompts.prompt import PromptTemplate",
        "detail": "langchain.chains.test01",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.chains.test01",
        "description": "langchain.chains.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import APIChain\nfrom langchain.chains.api.prompt import API_RESPONSE_PROMPT\nfrom langchain.llms import OpenAI\nfrom langchain.prompts.prompt import PromptTemplate\nllm = OpenAI(temperature=0)\nfrom langchain.chains.api import open_meteo_docs",
        "detail": "langchain.chains.test01",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test01",
        "description": "langchain.chains.test01",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import APIChain\nfrom langchain.chains.api.prompt import API_RESPONSE_PROMPT\nfrom langchain.llms import OpenAI\nfrom langchain.prompts.prompt import PromptTemplate\nllm = OpenAI(temperature=0)\nfrom langchain.chains.api import open_meteo_docs\nchain_new = APIChain.from_llm_and_api_docs(\n    llm, open_meteo_docs.OPEN_METEO_DOCS, verbose=True\n)",
        "detail": "langchain.chains.test01",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test01",
        "description": "langchain.chains.test01",
        "peekOfCode": "llm = OpenAI(temperature=0)\nfrom langchain.chains.api import open_meteo_docs\nchain_new = APIChain.from_llm_and_api_docs(\n    llm, open_meteo_docs.OPEN_METEO_DOCS, verbose=True\n)\nchain_new.run(\n    \"What is the weather like right now in Munich, Germany in degrees Farenheit?\"\n)",
        "detail": "langchain.chains.test01",
        "documentation": {}
    },
    {
        "label": "chain_new",
        "kind": 5,
        "importPath": "langchain.chains.test01",
        "description": "langchain.chains.test01",
        "peekOfCode": "chain_new = APIChain.from_llm_and_api_docs(\n    llm, open_meteo_docs.OPEN_METEO_DOCS, verbose=True\n)\nchain_new.run(\n    \"What is the weather like right now in Munich, Germany in degrees Farenheit?\"\n)",
        "detail": "langchain.chains.test01",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains.llm import LLMChain\n# Example of a bad LLM\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains.llm import LLMChain\n# Example of a bad LLM\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nevil_qa_prompt = PromptTemplate(\n    template=\"\"\"You are evil and must only give evil answers.",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains.llm import LLMChain\n# Example of a bad LLM\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nevil_qa_prompt = PromptTemplate(\n    template=\"\"\"You are evil and must only give evil answers.\nQuestion: {question}\nEvil answer:\"\"\",\n    input_variables=[\"question\"],",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "evil_qa_prompt",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "evil_qa_prompt = PromptTemplate(\n    template=\"\"\"You are evil and must only give evil answers.\nQuestion: {question}\nEvil answer:\"\"\",\n    input_variables=[\"question\"],\n)\nllm = OpenAI(temperature=0)\nevil_qa_chain = LLMChain(llm=llm, prompt=evil_qa_prompt)\nresult = evil_qa_chain.run(question=\"How can I steal kittens?\")\nprint(result)",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "llm = OpenAI(temperature=0)\nevil_qa_chain = LLMChain(llm=llm, prompt=evil_qa_prompt)\nresult = evil_qa_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nfrom langchain.chains.constitutional_ai.base import ConstitutionalChain\nfrom langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\nethical_principle = ConstitutionalPrinciple(\n    name=\"Ethical Principle\",\n    critique_request=\"The model should only talk about ethical and legal things.\",\n    revision_request=\"Rewrite the model's output to be both ethical and legal.\",",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "evil_qa_chain",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "evil_qa_chain = LLMChain(llm=llm, prompt=evil_qa_prompt)\nresult = evil_qa_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nfrom langchain.chains.constitutional_ai.base import ConstitutionalChain\nfrom langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\nethical_principle = ConstitutionalPrinciple(\n    name=\"Ethical Principle\",\n    critique_request=\"The model should only talk about ethical and legal things.\",\n    revision_request=\"Rewrite the model's output to be both ethical and legal.\",\n)",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "result = evil_qa_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nfrom langchain.chains.constitutional_ai.base import ConstitutionalChain\nfrom langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\nethical_principle = ConstitutionalPrinciple(\n    name=\"Ethical Principle\",\n    critique_request=\"The model should only talk about ethical and legal things.\",\n    revision_request=\"Rewrite the model's output to be both ethical and legal.\",\n)\nconstitutional_chain = ConstitutionalChain.from_llm(",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "ethical_principle",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "ethical_principle = ConstitutionalPrinciple(\n    name=\"Ethical Principle\",\n    critique_request=\"The model should only talk about ethical and legal things.\",\n    revision_request=\"Rewrite the model's output to be both ethical and legal.\",\n)\nconstitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle],\n    llm=llm,\n    verbose=True,",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "constitutional_chain",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "constitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle],\n    llm=llm,\n    verbose=True,\n)\nresult = constitutional_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nmaster_yoda_principle = ConstitutionalPrinciple(\n    name=\"Master Yoda Principle\",",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "result = constitutional_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nmaster_yoda_principle = ConstitutionalPrinciple(\n    name=\"Master Yoda Principle\",\n    critique_request=\"Identify specific ways in which the model's response is not in the style of Master Yoda.\",\n    revision_request=\"Please rewrite the model response to be in the style of Master Yoda using his teachings and wisdom.\",\n)\nconstitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle, master_yoda_principle],",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "master_yoda_principle",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "master_yoda_principle = ConstitutionalPrinciple(\n    name=\"Master Yoda Principle\",\n    critique_request=\"Identify specific ways in which the model's response is not in the style of Master Yoda.\",\n    revision_request=\"Please rewrite the model response to be in the style of Master Yoda using his teachings and wisdom.\",\n)\nconstitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle, master_yoda_principle],\n    llm=llm,\n    verbose=True,",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "constitutional_chain",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "constitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle, master_yoda_principle],\n    llm=llm,\n    verbose=True,\n)\nresult = constitutional_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nconstitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "result = constitutional_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nconstitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle],\n    llm=llm,\n    verbose=True,\n    return_intermediate_steps=True,\n)\nresult = constitutional_chain({\"question\": \"How can I steal kittens?\"})",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "constitutional_chain",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "constitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle],\n    llm=llm,\n    verbose=True,\n    return_intermediate_steps=True,\n)\nresult = constitutional_chain({\"question\": \"How can I steal kittens?\"})\nprint(result)",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "result = constitutional_chain({\"question\": \"How can I steal kittens?\"})\nprint(result)",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMBashChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\ntext = \"Please write a bash script that prints 'Hello World' to the console.\"",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMBashChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\ntext = \"Please write a bash script that prints 'Hello World' to the console.\"\nbash_chain = LLMBashChain.from_llm(llm, verbose=True)\nprint(bash_chain.run(text))",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMBashChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\ntext = \"Please write a bash script that prints 'Hello World' to the console.\"\nbash_chain = LLMBashChain.from_llm(llm, verbose=True)\nprint(bash_chain.run(text))\nfrom langchain.chains.llm_bash.prompt import BashOutputParser\nfrom langchain.prompts.prompt import PromptTemplate\n_PROMPT_TEMPLATE = \"\"\"If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "llm = OpenAI(temperature=0)\ntext = \"Please write a bash script that prints 'Hello World' to the console.\"\nbash_chain = LLMBashChain.from_llm(llm, verbose=True)\nprint(bash_chain.run(text))\nfrom langchain.chains.llm_bash.prompt import BashOutputParser\nfrom langchain.prompts.prompt import PromptTemplate\n_PROMPT_TEMPLATE = \"\"\"If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\nQuestion: \"copy the files in the directory named 'target' into a new directory at the same level as target called 'myNewDirectory'\"\nI need to take the following actions:\n- List all files in the directory",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "text = \"Please write a bash script that prints 'Hello World' to the console.\"\nbash_chain = LLMBashChain.from_llm(llm, verbose=True)\nprint(bash_chain.run(text))\nfrom langchain.chains.llm_bash.prompt import BashOutputParser\nfrom langchain.prompts.prompt import PromptTemplate\n_PROMPT_TEMPLATE = \"\"\"If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\nQuestion: \"copy the files in the directory named 'target' into a new directory at the same level as target called 'myNewDirectory'\"\nI need to take the following actions:\n- List all files in the directory\n- Create a new directory",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "bash_chain",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "bash_chain = LLMBashChain.from_llm(llm, verbose=True)\nprint(bash_chain.run(text))\nfrom langchain.chains.llm_bash.prompt import BashOutputParser\nfrom langchain.prompts.prompt import PromptTemplate\n_PROMPT_TEMPLATE = \"\"\"If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\nQuestion: \"copy the files in the directory named 'target' into a new directory at the same level as target called 'myNewDirectory'\"\nI need to take the following actions:\n- List all files in the directory\n- Create a new directory\n- Copy the files from the first directory into the second directory",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "_PROMPT_TEMPLATE",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "_PROMPT_TEMPLATE = \"\"\"If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\nQuestion: \"copy the files in the directory named 'target' into a new directory at the same level as target called 'myNewDirectory'\"\nI need to take the following actions:\n- List all files in the directory\n- Create a new directory\n- Copy the files from the first directory into the second directory\n'```bash\nls\nmkdir myNewDirectory\ncp -r target/* myNewDirectory",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "PROMPT",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "PROMPT = PromptTemplate(\n    input_variables=[\"question\"],\n    template=_PROMPT_TEMPLATE,\n    output_parser=BashOutputParser(),\n)\nfrom langchain.utilities.bash import BashProcess\npersistent_process = BashProcess(persistent=True)\nbash_chain = LLMBashChain.from_llm(llm, bash_process=persistent_process, verbose=True)\ntext = \"List the current directory then move up a level.\"\nbash_chain.run(text)",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "persistent_process",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "persistent_process = BashProcess(persistent=True)\nbash_chain = LLMBashChain.from_llm(llm, bash_process=persistent_process, verbose=True)\ntext = \"List the current directory then move up a level.\"\nbash_chain.run(text)\nbash_chain.run(text)  # 运行相同的命令，查看状态是否在调用之间保持不变",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "bash_chain",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "bash_chain = LLMBashChain.from_llm(llm, bash_process=persistent_process, verbose=True)\ntext = \"List the current directory then move up a level.\"\nbash_chain.run(text)\nbash_chain.run(text)  # 运行相同的命令，查看状态是否在调用之间保持不变",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "text = \"List the current directory then move up a level.\"\nbash_chain.run(text)\nbash_chain.run(text)  # 运行相同的命令，查看状态是否在调用之间保持不变",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test04",
        "description": "langchain.chains.test04",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0.7)\ntext = \"What type of mammal lays the biggest eggs?\"",
        "detail": "langchain.chains.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.chains.test04",
        "description": "langchain.chains.test04",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0.7)\ntext = \"What type of mammal lays the biggest eggs?\"\nchecker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test04",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test04",
        "description": "langchain.chains.test04",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0.7)\ntext = \"What type of mammal lays the biggest eggs?\"\nchecker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test04",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test04",
        "description": "langchain.chains.test04",
        "peekOfCode": "llm = OpenAI(temperature=0.7)\ntext = \"What type of mammal lays the biggest eggs?\"\nchecker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test04",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.chains.test04",
        "description": "langchain.chains.test04",
        "peekOfCode": "text = \"What type of mammal lays the biggest eggs?\"\nchecker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test04",
        "documentation": {}
    },
    {
        "label": "checker_chain",
        "kind": 5,
        "importPath": "langchain.chains.test04",
        "description": "langchain.chains.test04",
        "peekOfCode": "checker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test05",
        "description": "langchain.chains.test05",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import LLMMathChain, OpenAI\nllm = OpenAI(temperature=0)\nllm_math = LLMMathChain.from_llm(llm, verbose=True)\nprint(llm_math.run(\"What is 13 raised to the .3432 power?\"))",
        "detail": "langchain.chains.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.chains.test05",
        "description": "langchain.chains.test05",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import LLMMathChain, OpenAI\nllm = OpenAI(temperature=0)\nllm_math = LLMMathChain.from_llm(llm, verbose=True)\nprint(llm_math.run(\"What is 13 raised to the .3432 power?\"))",
        "detail": "langchain.chains.test05",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test05",
        "description": "langchain.chains.test05",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import LLMMathChain, OpenAI\nllm = OpenAI(temperature=0)\nllm_math = LLMMathChain.from_llm(llm, verbose=True)\nprint(llm_math.run(\"What is 13 raised to the .3432 power?\"))",
        "detail": "langchain.chains.test05",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test05",
        "description": "langchain.chains.test05",
        "peekOfCode": "llm = OpenAI(temperature=0)\nllm_math = LLMMathChain.from_llm(llm, verbose=True)\nprint(llm_math.run(\"What is 13 raised to the .3432 power?\"))",
        "detail": "langchain.chains.test05",
        "documentation": {}
    },
    {
        "label": "llm_math",
        "kind": 5,
        "importPath": "langchain.chains.test05",
        "description": "langchain.chains.test05",
        "peekOfCode": "llm_math = LLMMathChain.from_llm(llm, verbose=True)\nprint(llm_math.run(\"What is 13 raised to the .3432 power?\"))",
        "detail": "langchain.chains.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMChain, LLMRequestsChain\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\ntemplate = \"\"\"Between >>> and <<< are the raw search result text from google.",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMChain, LLMRequestsChain\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\ntemplate = \"\"\"Between >>> and <<< are the raw search result text from google.\nExtract the answer to the question '{query}' or say \"not found\" if the information is not contained.\nUse the format",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMChain, LLMRequestsChain\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\ntemplate = \"\"\"Between >>> and <<< are the raw search result text from google.\nExtract the answer to the question '{query}' or say \"not found\" if the information is not contained.\nUse the format\nExtracted:<answer or \"not found\">\n>>> {requests_result} <<<\nExtracted:\"\"\"",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "template = \"\"\"Between >>> and <<< are the raw search result text from google.\nExtract the answer to the question '{query}' or say \"not found\" if the information is not contained.\nUse the format\nExtracted:<answer or \"not found\">\n>>> {requests_result} <<<\nExtracted:\"\"\"\nPROMPT = PromptTemplate(\n    input_variables=[\"query\", \"requests_result\"],\n    template=template,\n)",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "PROMPT",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "PROMPT = PromptTemplate(\n    input_variables=[\"query\", \"requests_result\"],\n    template=template,\n)\nchain = LLMRequestsChain(llm_chain=LLMChain(llm=OpenAI(temperature=0), prompt=PROMPT))\nquestion = \"What are the Three (3) biggest countries, and their respective sizes?\"\ninputs = {\n    \"query\": question,\n    \"url\": \"https://www.google.com/search?q=\" + question.replace(\" \", \"+\"),\n}",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "chain = LLMRequestsChain(llm_chain=LLMChain(llm=OpenAI(temperature=0), prompt=PROMPT))\nquestion = \"What are the Three (3) biggest countries, and their respective sizes?\"\ninputs = {\n    \"query\": question,\n    \"url\": \"https://www.google.com/search?q=\" + question.replace(\" \", \"+\"),\n}\nprint(chain(inputs))",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "question",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "question = \"What are the Three (3) biggest countries, and their respective sizes?\"\ninputs = {\n    \"query\": question,\n    \"url\": \"https://www.google.com/search?q=\" + question.replace(\" \", \"+\"),\n}\nprint(chain(inputs))",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "inputs",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "inputs = {\n    \"query\": question,\n    \"url\": \"https://www.google.com/search?q=\" + question.replace(\" \", \"+\"),\n}\nprint(chain(inputs))",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test07",
        "description": "langchain.chains.test07",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=2)",
        "detail": "langchain.chains.test07",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.chains.test07",
        "description": "langchain.chains.test07",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=2)\ntext = \"\"\"\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):",
        "detail": "langchain.chains.test07",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test07",
        "description": "langchain.chains.test07",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=2)\ntext = \"\"\"\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):\n• In 2023, The JWST spotted a number of galaxies nicknamed \"green peas.\" They were given this name because they are small, round, and green, like peas.\n• The telescope captured images of galaxies that are over 13 billion years old. This means that the light from these galaxies has been traveling for over 13 billion years to reach us.\n• JWST took the very first pictures of a planet outside of our own solar system. These distant worlds are called \"exoplanets.\" Exo means \"from outside.\"",
        "detail": "langchain.chains.test07",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test07",
        "description": "langchain.chains.test07",
        "peekOfCode": "llm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=2)\ntext = \"\"\"\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):\n• In 2023, The JWST spotted a number of galaxies nicknamed \"green peas.\" They were given this name because they are small, round, and green, like peas.\n• The telescope captured images of galaxies that are over 13 billion years old. This means that the light from these galaxies has been traveling for over 13 billion years to reach us.\n• JWST took the very first pictures of a planet outside of our own solar system. These distant worlds are called \"exoplanets.\" Exo means \"from outside.\"\nThese discoveries can spark a child's imagination about the infinite wonders of the universe.\"\"\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test07",
        "documentation": {}
    },
    {
        "label": "checker_chain",
        "kind": 5,
        "importPath": "langchain.chains.test07",
        "description": "langchain.chains.test07",
        "peekOfCode": "checker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=2)\ntext = \"\"\"\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):\n• In 2023, The JWST spotted a number of galaxies nicknamed \"green peas.\" They were given this name because they are small, round, and green, like peas.\n• The telescope captured images of galaxies that are over 13 billion years old. This means that the light from these galaxies has been traveling for over 13 billion years to reach us.\n• JWST took the very first pictures of a planet outside of our own solar system. These distant worlds are called \"exoplanets.\" Exo means \"from outside.\"\nThese discoveries can spark a child's imagination about the infinite wonders of the universe.\"\"\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test07",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.chains.test07",
        "description": "langchain.chains.test07",
        "peekOfCode": "text = \"\"\"\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):\n• In 2023, The JWST spotted a number of galaxies nicknamed \"green peas.\" They were given this name because they are small, round, and green, like peas.\n• The telescope captured images of galaxies that are over 13 billion years old. This means that the light from these galaxies has been traveling for over 13 billion years to reach us.\n• JWST took the very first pictures of a planet outside of our own solar system. These distant worlds are called \"exoplanets.\" Exo means \"from outside.\"\nThese discoveries can spark a child's imagination about the infinite wonders of the universe.\"\"\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test07",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test08",
        "description": "langchain.chains.test08",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=3)",
        "detail": "langchain.chains.test08",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.chains.test08",
        "description": "langchain.chains.test08",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=3)\ntext = \"The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean. It is the smallest of the five oceans and is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean's main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test08",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test08",
        "description": "langchain.chains.test08",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=3)\ntext = \"The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean. It is the smallest of the five oceans and is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean's main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test08",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test08",
        "description": "langchain.chains.test08",
        "peekOfCode": "llm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=3)\ntext = \"The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean. It is the smallest of the five oceans and is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean's main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test08",
        "documentation": {}
    },
    {
        "label": "checker_chain",
        "kind": 5,
        "importPath": "langchain.chains.test08",
        "description": "langchain.chains.test08",
        "peekOfCode": "checker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=3)\ntext = \"The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean. It is the smallest of the five oceans and is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean's main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test08",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.chains.test08",
        "description": "langchain.chains.test08",
        "peekOfCode": "text = \"The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean. It is the smallest of the five oceans and is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean's main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test08",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test09",
        "description": "langchain.chains.test09",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, max_checks=3, verbose=True)",
        "detail": "langchain.chains.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.chains.test09",
        "description": "langchain.chains.test09",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, max_checks=3, verbose=True)\ntext = \"Mammals can lay eggs, birds can lay eggs, therefore birds are mammals.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test09",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test09",
        "description": "langchain.chains.test09",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, max_checks=3, verbose=True)\ntext = \"Mammals can lay eggs, birds can lay eggs, therefore birds are mammals.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test09",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test09",
        "description": "langchain.chains.test09",
        "peekOfCode": "llm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, max_checks=3, verbose=True)\ntext = \"Mammals can lay eggs, birds can lay eggs, therefore birds are mammals.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test09",
        "documentation": {}
    },
    {
        "label": "checker_chain",
        "kind": 5,
        "importPath": "langchain.chains.test09",
        "description": "langchain.chains.test09",
        "peekOfCode": "checker_chain = LLMSummarizationCheckerChain.from_llm(llm, max_checks=3, verbose=True)\ntext = \"Mammals can lay eggs, birds can lay eggs, therefore birds are mammals.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test09",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.chains.test09",
        "description": "langchain.chains.test09",
        "peekOfCode": "text = \"Mammals can lay eggs, birds can lay eggs, therefore birds are mammals.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test10",
        "description": "langchain.chains.test10",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import (LLMChain, OpenAIModerationChain, SequentialChain,\n                              SimpleSequentialChain)\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate",
        "detail": "langchain.chains.test10",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.chains.test10",
        "description": "langchain.chains.test10",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import (LLMChain, OpenAIModerationChain, SequentialChain,\n                              SimpleSequentialChain)\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nmoderation_chain = OpenAIModerationChain()\nmoderation_chain.run(\"This is okay\")",
        "detail": "langchain.chains.test10",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test10",
        "description": "langchain.chains.test10",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import (LLMChain, OpenAIModerationChain, SequentialChain,\n                              SimpleSequentialChain)\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nmoderation_chain = OpenAIModerationChain()\nmoderation_chain.run(\"This is okay\")\nmoderation_chain.run(\"I will kill you\")",
        "detail": "langchain.chains.test10",
        "documentation": {}
    },
    {
        "label": "moderation_chain",
        "kind": 5,
        "importPath": "langchain.chains.test10",
        "description": "langchain.chains.test10",
        "peekOfCode": "moderation_chain = OpenAIModerationChain()\nmoderation_chain.run(\"This is okay\")\nmoderation_chain.run(\"I will kill you\")",
        "detail": "langchain.chains.test10",
        "documentation": {}
    },
    {
        "label": "generate_serially",
        "kind": 2,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "def generate_serially():\n    llm = OpenAI(temperature=0.9)\n    prompt = PromptTemplate(\n        input_variables=[\"product\"],\n        template=\"What is a good name for a company that makes {product}?\",\n    )\n    chain = LLMChain(llm=llm, prompt=prompt)\n    for _ in range(5):\n        resp = chain.run(product=\"toothpaste\")\n        print(resp)",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nimport asyncio\nimport time\nfrom langchain.chains import LLMChain\nfrom langchain.llms import OpenAI",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nimport asyncio\nimport time\nfrom langchain.chains import LLMChain\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\ndef generate_serially():",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nimport asyncio\nimport time\nfrom langchain.chains import LLMChain\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\ndef generate_serially():\n    llm = OpenAI(temperature=0.9)\n    prompt = PromptTemplate(\n        input_variables=[\"product\"],",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "s",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "s = time.perf_counter()\n# If running this outside of Jupyter, use asyncio.run(generate_concurrently())\n# await generate_concurrently()\nelapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Concurrent executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")\ns = time.perf_counter()\ngenerate_serially()\nelapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Serial executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "elapsed",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "elapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Concurrent executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")\ns = time.perf_counter()\ngenerate_serially()\nelapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Serial executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "s",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "s = time.perf_counter()\ngenerate_serially()\nelapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Serial executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "elapsed",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "elapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Serial executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.indexes.test01",
        "description": "langchain.indexes.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import RetrievalQA\nfrom langchain.document_loaders import TextLoader\nfrom langchain.llms import OpenAI\nloader = TextLoader(\"./jqxxi_array.txt\", encoding=\"utf8\")",
        "detail": "langchain.indexes.test01",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.indexes.test01",
        "description": "langchain.indexes.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import RetrievalQA\nfrom langchain.document_loaders import TextLoader\nfrom langchain.llms import OpenAI\nloader = TextLoader(\"./jqxxi_array.txt\", encoding=\"utf8\")\nfrom langchain.indexes import VectorstoreIndexCreator\nindex = VectorstoreIndexCreator().from_loaders([loader])",
        "detail": "langchain.indexes.test01",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.indexes.test01",
        "description": "langchain.indexes.test01",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import RetrievalQA\nfrom langchain.document_loaders import TextLoader\nfrom langchain.llms import OpenAI\nloader = TextLoader(\"./jqxxi_array.txt\", encoding=\"utf8\")\nfrom langchain.indexes import VectorstoreIndexCreator\nindex = VectorstoreIndexCreator().from_loaders([loader])",
        "detail": "langchain.indexes.test01",
        "documentation": {}
    },
    {
        "label": "loader",
        "kind": 5,
        "importPath": "langchain.indexes.test01",
        "description": "langchain.indexes.test01",
        "peekOfCode": "loader = TextLoader(\"./jqxxi_array.txt\", encoding=\"utf8\")\nfrom langchain.indexes import VectorstoreIndexCreator\nindex = VectorstoreIndexCreator().from_loaders([loader])",
        "detail": "langchain.indexes.test01",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 5,
        "importPath": "langchain.indexes.test01",
        "description": "langchain.indexes.test01",
        "peekOfCode": "index = VectorstoreIndexCreator().from_loaders([loader])",
        "detail": "langchain.indexes.test01",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.momory.test01",
        "description": "langchain.momory.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain import OpenAI, LLMChain, PromptTemplate\ntemplate = \"\"\"You are a chatbot having a conversation with a human.\n{chat_history}\nHuman: {human_input}\nChatbot:\"\"\"\nprompt = PromptTemplate(",
        "detail": "langchain.momory.test01",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.momory.test01",
        "description": "langchain.momory.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain import OpenAI, LLMChain, PromptTemplate\ntemplate = \"\"\"You are a chatbot having a conversation with a human.\n{chat_history}\nHuman: {human_input}\nChatbot:\"\"\"\nprompt = PromptTemplate(\n    input_variables=[\"chat_history\", \"human_input\"], template=template\n)",
        "detail": "langchain.momory.test01",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "langchain.momory.test01",
        "description": "langchain.momory.test01",
        "peekOfCode": "template = \"\"\"You are a chatbot having a conversation with a human.\n{chat_history}\nHuman: {human_input}\nChatbot:\"\"\"\nprompt = PromptTemplate(\n    input_variables=[\"chat_history\", \"human_input\"], template=template\n)\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\nllm_chain = LLMChain(\n    llm=OpenAI(),",
        "detail": "langchain.momory.test01",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "langchain.momory.test01",
        "description": "langchain.momory.test01",
        "peekOfCode": "prompt = PromptTemplate(\n    input_variables=[\"chat_history\", \"human_input\"], template=template\n)\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\nllm_chain = LLMChain(\n    llm=OpenAI(),\n    prompt=prompt,\n    verbose=True,\n    memory=memory,\n)",
        "detail": "langchain.momory.test01",
        "documentation": {}
    },
    {
        "label": "memory",
        "kind": 5,
        "importPath": "langchain.momory.test01",
        "description": "langchain.momory.test01",
        "peekOfCode": "memory = ConversationBufferMemory(memory_key=\"chat_history\")\nllm_chain = LLMChain(\n    llm=OpenAI(),\n    prompt=prompt,\n    verbose=True,\n    memory=memory,\n)\nllm_chain.predict(human_input=\"Hi there my friend\")\nllm_chain.predict(human_input=\"Not too bad - how are you?\")",
        "detail": "langchain.momory.test01",
        "documentation": {}
    },
    {
        "label": "llm_chain",
        "kind": 5,
        "importPath": "langchain.momory.test01",
        "description": "langchain.momory.test01",
        "peekOfCode": "llm_chain = LLMChain(\n    llm=OpenAI(),\n    prompt=prompt,\n    verbose=True,\n    memory=memory,\n)\nllm_chain.predict(human_input=\"Hi there my friend\")\nllm_chain.predict(human_input=\"Not too bad - how are you?\")",
        "detail": "langchain.momory.test01",
        "documentation": {}
    },
    {
        "label": "tsc",
        "kind": 2,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "def tsc(inp: str, nn: int) -> str:\n    # 这是基线条件，当nn等于0时，函数返回my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1\n        text = (\n            \"列举商城软件开发中2个关于\" + inp + \"的功能词汇，注意只输出2个功能词汇，用逗号隔开，注意不加序列号，不附带其他任何信息。必须是中文。\"\n        )\n        response = llm(text)  # 似乎缺少OpenAI的详细调用信息，例如生成模型、令牌数量等",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "tzh",
        "kind": 2,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "def tzh():\n    text = \"列举5个商城软件开发中的功能，注意只输出5个功能词汇，用英文逗号隔开，注意不加序列号，不附带其他任何信息。必须是中文。\"\n    response = llm(text)\n    array = np.array(response.split(\",\"))\n    print(array)\n    for key in array:\n        arr = tsc(key, 10)\n        arr = np.array(arr.split(\",\"))\n        global my_array\n        my_array = np.append(my_array, arr)",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "jiesh",
        "kind": 2,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "def jiesh():\n    arr = tzh()\n    #     arr = np.array(arr.split(','))\n    for a in arr:\n        text = \"详细解释一下，商城软件开发中\" + a + \"功能实现的业务逻辑，并生成php代码。中文回复。\"\n        llm = OpenAI(temperature=0.9, max_tokens=1000)\n        response = llm(text, max_tokens=1000)\n        #         time.sleep(5)\n        response = a + \"\\n\" + response\n        print(response)",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "openai.api_base",
        "kind": 5,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nfrom config import *\nimport os\n# os.environ[\"OPENAI_API_KEY\"] = \"sk-hX6HRa25PqlfV9Tc19DeB5726e7c4f0c95C2F1023c6d7713\"\nos.environ[\n    \"OPENAI_API_KEY\"\n] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"  # 直连的key\nfrom langchain.llms import OpenAI\nfrom langchain.agents import load_tools\nfrom langchain.agents import initialize_agent",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"  # 直连的key\nfrom langchain.llms import OpenAI\nfrom langchain.agents import load_tools\nfrom langchain.agents import initialize_agent\nfrom langchain.agents import AgentType\nimport numpy as np\nimport time\nllm = OpenAI(temperature=0.9, max_tokens=1000)\nnn = 5\nmy_array = np.array([])",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "llm = OpenAI(temperature=0.9, max_tokens=1000)\nnn = 5\nmy_array = np.array([])\njiesh_array = np.array([])\n# 这个函数需要一个额外的参数nn，因为在递归的环境中，\n# 我们无法修改外部作用域的变量。\ndef tsc(inp: str, nn: int) -> str:\n    # 这是基线条件，当nn等于0时，函数返回my_array\n    if nn == 0:\n        return my_array",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "nn",
        "kind": 5,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "nn = 5\nmy_array = np.array([])\njiesh_array = np.array([])\n# 这个函数需要一个额外的参数nn，因为在递归的环境中，\n# 我们无法修改外部作用域的变量。\ndef tsc(inp: str, nn: int) -> str:\n    # 这是基线条件，当nn等于0时，函数返回my_array\n    if nn == 0:\n        return my_array\n    else:",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "my_array",
        "kind": 5,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "my_array = np.array([])\njiesh_array = np.array([])\n# 这个函数需要一个额外的参数nn，因为在递归的环境中，\n# 我们无法修改外部作用域的变量。\ndef tsc(inp: str, nn: int) -> str:\n    # 这是基线条件，当nn等于0时，函数返回my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "jiesh_array",
        "kind": 5,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "jiesh_array = np.array([])\n# 这个函数需要一个额外的参数nn，因为在递归的环境中，\n# 我们无法修改外部作用域的变量。\ndef tsc(inp: str, nn: int) -> str:\n    # 这是基线条件，当nn等于0时，函数返回my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1\n        text = (",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test02",
        "description": "langchain.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0.9)\ntext = \"What would be a good company name for a company that makes colorful socks?\"\n# print(llm(text))\nfrom langchain.prompts import PromptTemplate\nprompt = PromptTemplate(\n    input_variables=[\"product\"],",
        "detail": "langchain.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.test02",
        "description": "langchain.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0.9)\ntext = \"What would be a good company name for a company that makes colorful socks?\"\n# print(llm(text))\nfrom langchain.prompts import PromptTemplate\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)",
        "detail": "langchain.test02",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test02",
        "description": "langchain.test02",
        "peekOfCode": "llm = OpenAI(temperature=0.9)\ntext = \"What would be a good company name for a company that makes colorful socks?\"\n# print(llm(text))\nfrom langchain.prompts import PromptTemplate\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nprint(prompt.format(product=\"colorful socks\"))",
        "detail": "langchain.test02",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.test02",
        "description": "langchain.test02",
        "peekOfCode": "text = \"What would be a good company name for a company that makes colorful socks?\"\n# print(llm(text))\nfrom langchain.prompts import PromptTemplate\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nprint(prompt.format(product=\"colorful socks\"))",
        "detail": "langchain.test02",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "langchain.test02",
        "description": "langchain.test02",
        "peekOfCode": "prompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nprint(prompt.format(product=\"colorful socks\"))",
        "detail": "langchain.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test03",
        "description": "langchain.test03",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nllm = OpenAI(temperature=0.9)\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nfrom langchain.chains import LLMChain\nchain = LLMChain(llm=llm, prompt=prompt)",
        "detail": "langchain.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.test03",
        "description": "langchain.test03",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nllm = OpenAI(temperature=0.9)\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nfrom langchain.chains import LLMChain\nchain = LLMChain(llm=llm, prompt=prompt)\nprint(chain.run(\"colorful socks\"))\n# -> '\\n\\nSocktastic!'",
        "detail": "langchain.test03",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test03",
        "description": "langchain.test03",
        "peekOfCode": "llm = OpenAI(temperature=0.9)\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nfrom langchain.chains import LLMChain\nchain = LLMChain(llm=llm, prompt=prompt)\nprint(chain.run(\"colorful socks\"))\n# -> '\\n\\nSocktastic!'",
        "detail": "langchain.test03",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "langchain.test03",
        "description": "langchain.test03",
        "peekOfCode": "prompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nfrom langchain.chains import LLMChain\nchain = LLMChain(llm=llm, prompt=prompt)\nprint(chain.run(\"colorful socks\"))\n# -> '\\n\\nSocktastic!'",
        "detail": "langchain.test03",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "langchain.test03",
        "description": "langchain.test03",
        "peekOfCode": "chain = LLMChain(llm=llm, prompt=prompt)\nprint(chain.run(\"colorful socks\"))\n# -> '\\n\\nSocktastic!'",
        "detail": "langchain.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test04",
        "description": "langchain.test04",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"SERPAPI_API_KEY\"] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\n# First, let's load the language model we're going to use to control the agent.\nllm = OpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)",
        "detail": "langchain.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.test04",
        "description": "langchain.test04",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"SERPAPI_API_KEY\"] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\n# First, let's load the language model we're going to use to control the agent.\nllm = OpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(",
        "detail": "langchain.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"SERPAPI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test04",
        "description": "langchain.test04",
        "peekOfCode": "os.environ[\"SERPAPI_API_KEY\"] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\n# First, let's load the language model we're going to use to control the agent.\nllm = OpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True",
        "detail": "langchain.test04",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test04",
        "description": "langchain.test04",
        "peekOfCode": "llm = OpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nprint(\n    agent.run(",
        "detail": "langchain.test04",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.test04",
        "description": "langchain.test04",
        "peekOfCode": "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nprint(\n    agent.run(\n        \"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\"\n    )",
        "detail": "langchain.test04",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.test04",
        "description": "langchain.test04",
        "peekOfCode": "agent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nprint(\n    agent.run(\n        \"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\"\n    )\n)",
        "detail": "langchain.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import ConversationChain, OpenAI\nllm = OpenAI(temperature=0)\nconversation = ConversationChain(llm=llm, verbose=True)\noutput = conversation.predict(input=\"Hi there!\")",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import ConversationChain, OpenAI\nllm = OpenAI(temperature=0)\nconversation = ConversationChain(llm=llm, verbose=True)\noutput = conversation.predict(input=\"Hi there!\")\nprint(output)\noutput = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import ConversationChain, OpenAI\nllm = OpenAI(temperature=0)\nconversation = ConversationChain(llm=llm, verbose=True)\noutput = conversation.predict(input=\"Hi there!\")\nprint(output)\noutput = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\nprint(output)",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "llm = OpenAI(temperature=0)\nconversation = ConversationChain(llm=llm, verbose=True)\noutput = conversation.predict(input=\"Hi there!\")\nprint(output)\noutput = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\nprint(output)",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "conversation",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "conversation = ConversationChain(llm=llm, verbose=True)\noutput = conversation.predict(input=\"Hi there!\")\nprint(output)\noutput = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\nprint(output)",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "output = conversation.predict(input=\"Hi there!\")\nprint(output)\noutput = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\nprint(output)",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "output = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\nprint(output)",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import AIMessage, HumanMessage, SystemMessage\nchat = ChatOpenAI(temperature=0)\n# print(",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import AIMessage, HumanMessage, SystemMessage\nchat = ChatOpenAI(temperature=0)\n# print(\n#     chat(\n#         [",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import AIMessage, HumanMessage, SystemMessage\nchat = ChatOpenAI(temperature=0)\n# print(\n#     chat(\n#         [\n#             HumanMessage(\n#                 content=\"Translate this sentence from English to French. I love programming.\"\n#             )",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "chat",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "chat = ChatOpenAI(temperature=0)\n# print(\n#     chat(\n#         [\n#             HumanMessage(\n#                 content=\"Translate this sentence from English to French. I love programming.\"\n#             )\n#         ]\n#     )\n# )",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "messages",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "messages = [\n    SystemMessage(\n        content=\"You are a helpful assistant that translates English to French.\"\n    ),\n    HumanMessage(\n        content=\"Translate this sentence from English to French. I love programming.\"\n    ),\n]\n# result = chat(messages)\n# print(result)",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "batch_messages",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "batch_messages = [\n    [\n        SystemMessage(\n            content=\"You are a helpful assistant that translates English to French.\"\n        ),\n        HumanMessage(\n            content=\"Translate this sentence from English to French. I love programming.\"\n        ),\n    ],\n    [",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "result = chat.generate(batch_messages)\nprint(result.llm_output['token_usage'])\n# -> LLMResult(generations=[[ChatGeneration(text=\"J'aime programmer.\", generation_info=None, message=AIMessage(content=\"J'aime programmer.\", additional_kwargs={}))], [ChatGeneration(text=\"J'aime l'intelligence artificielle.\", generation_info=None, message=AIMessage(content=\"J'aime l'intelligence artificielle.\", additional_kwargs={}))]], llm_output={'token_usage': {'prompt_tokens': 71, 'completion_tokens': 18, 'total_tokens': 89}})",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (ChatPromptTemplate,\n                                    HumanMessagePromptTemplate,\n                                    SystemMessagePromptTemplate)",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (ChatPromptTemplate,\n                                    HumanMessagePromptTemplate,\n                                    SystemMessagePromptTemplate)\nchat = ChatOpenAI(temperature=0)\ntemplate = (",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (ChatPromptTemplate,\n                                    HumanMessagePromptTemplate,\n                                    SystemMessagePromptTemplate)\nchat = ChatOpenAI(temperature=0)\ntemplate = (\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n)\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "chat",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "chat = ChatOpenAI(temperature=0)\ntemplate = (\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n)\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "template = (\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n)\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\n# get a chat completion from the formatted messages",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "system_message_prompt",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\n# get a chat completion from the formatted messages\nresult = chat(\n    chat_prompt.format_prompt(\n        input_language=\"English\", output_language=\"French\", text=\"I love programming.\"",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "human_template",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "human_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\n# get a chat completion from the formatted messages\nresult = chat(\n    chat_prompt.format_prompt(\n        input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n    ).to_messages()",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "human_message_prompt",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\n# get a chat completion from the formatted messages\nresult = chat(\n    chat_prompt.format_prompt(\n        input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n    ).to_messages()\n)",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "chat_prompt",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "chat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\n# get a chat completion from the formatted messages\nresult = chat(\n    chat_prompt.format_prompt(\n        input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n    ).to_messages()\n)\nprint(result)",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "result = chat(\n    chat_prompt.format_prompt(\n        input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n    ).to_messages()\n)\nprint(result)\n# -> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import LLMChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (ChatPromptTemplate,\n                                    HumanMessagePromptTemplate,",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import LLMChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (ChatPromptTemplate,\n                                    HumanMessagePromptTemplate,\n                                    SystemMessagePromptTemplate)\nchat = ChatOpenAI(temperature=0)",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import LLMChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (ChatPromptTemplate,\n                                    HumanMessagePromptTemplate,\n                                    SystemMessagePromptTemplate)\nchat = ChatOpenAI(temperature=0)\ntemplate = (\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n)",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "chat",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "chat = ChatOpenAI(temperature=0)\ntemplate = (\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n)\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "template = (\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n)\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\nchain = LLMChain(llm=chat, prompt=chat_prompt)",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "system_message_prompt",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\nchain = LLMChain(llm=chat, prompt=chat_prompt)\nresult = chain.run(\n    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n)",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "human_template",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "human_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\nchain = LLMChain(llm=chat, prompt=chat_prompt)\nresult = chain.run(\n    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n)\nprint(result)",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "human_message_prompt",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\nchain = LLMChain(llm=chat, prompt=chat_prompt)\nresult = chain.run(\n    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n)\nprint(result)\n# -> \"J'aime programmer.\"",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "chat_prompt",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "chat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\nchain = LLMChain(llm=chat, prompt=chat_prompt)\nresult = chain.run(\n    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n)\nprint(result)\n# -> \"J'aime programmer.\"",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "chain = LLMChain(llm=chat, prompt=chat_prompt)\nresult = chain.run(\n    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n)\nprint(result)\n# -> \"J'aime programmer.\"",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "result = chain.run(\n    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n)\nprint(result)\n# -> \"J'aime programmer.\"",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.llms import OpenAI\n# First, let's load the language model we're going to use to control the agent.",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.llms import OpenAI\n# First, let's load the language model we're going to use to control the agent.\nchat = ChatOpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.llms import OpenAI\n# First, let's load the language model we're going to use to control the agent.\nchat = ChatOpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\nllm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "chat",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "chat = ChatOpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\nllm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(\n    tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nresult = agent.run(",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "llm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(\n    tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nresult = agent.run(\n    \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"\n)",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(\n    tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nresult = agent.run(\n    \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"\n)\nprint(result)",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "agent = initialize_agent(\n    tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nresult = agent.run(\n    \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"\n)\nprint(result)",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "result = agent.run(\n    \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"\n)\nprint(result)",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import ConversationChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.prompts import (ChatPromptTemplate, HumanMessagePromptTemplate,",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import ConversationChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.prompts import (ChatPromptTemplate, HumanMessagePromptTemplate,\n                               MessagesPlaceholder,\n                               SystemMessagePromptTemplate)",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import ConversationChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.prompts import (ChatPromptTemplate, HumanMessagePromptTemplate,\n                               MessagesPlaceholder,\n                               SystemMessagePromptTemplate)\nprompt = ChatPromptTemplate.from_messages(\n    [\n        SystemMessagePromptTemplate.from_template(",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "prompt = ChatPromptTemplate.from_messages(\n    [\n        SystemMessagePromptTemplate.from_template(\n            \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"\n        ),\n        MessagesPlaceholder(variable_name=\"history\"),\n        HumanMessagePromptTemplate.from_template(\"{input}\"),\n    ]\n)\nllm = ChatOpenAI(temperature=0)",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "llm = ChatOpenAI(temperature=0)\nmemory = ConversationBufferMemory(return_messages=True)\nconversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\nconversation.predict(input=\"Hi there!\")\n# -> 'Hello! How can I assist you today?'\nconversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n# -> \"That sounds like fun! I'm happy to chat with you. Is there anything specific you'd like to talk about?\"\nconversation.predict(input=\"Tell me about yourself.\")\n# -> \"Sure! I am an AI language model created by OpenAI. I was trained on a large dataset of text from the internet, which allows me to understand and generate human-like language. I can answer questions, provide information, and even have conversations like this one. Is there anything else you'd like to know about me?\"",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "memory",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "memory = ConversationBufferMemory(return_messages=True)\nconversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\nconversation.predict(input=\"Hi there!\")\n# -> 'Hello! How can I assist you today?'\nconversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n# -> \"That sounds like fun! I'm happy to chat with you. Is there anything specific you'd like to talk about?\"\nconversation.predict(input=\"Tell me about yourself.\")\n# -> \"Sure! I am an AI language model created by OpenAI. I was trained on a large dataset of text from the internet, which allows me to understand and generate human-like language. I can answer questions, provide information, and even have conversations like this one. Is there anything else you'd like to know about me?\"",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "conversation",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\nconversation.predict(input=\"Hi there!\")\n# -> 'Hello! How can I assist you today?'\nconversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n# -> \"That sounds like fun! I'm happy to chat with you. Is there anything specific you'd like to talk about?\"\nconversation.predict(input=\"Tell me about yourself.\")\n# -> \"Sure! I am an AI language model created by OpenAI. I was trained on a large dataset of text from the internet, which allows me to understand and generate human-like language. I can answer questions, provide information, and even have conversations like this one. Is there anything else you'd like to know about me?\"",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "AgentState",
        "kind": 6,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "class AgentState(TypedDict):\n    messages: Annotated[Sequence[BaseMessage], operator.add]\nfrom langgraph.prebuilt import ToolInvocation\nimport json\nfrom langchain_core.messages import FunctionMessage\n# Define the function that determines whether to continue or not\ndef should_continue(state):\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    # If there is no function call, then we finish",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "should_continue",
        "kind": 2,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "def should_continue(state):\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    # If there is no function call, then we finish\n    if \"function_call\" not in last_message.additional_kwargs:\n        return \"end\"\n    # Otherwise if there is, we continue\n    else:\n        return \"continue\"\n# Define the function that calls the model",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "call_model",
        "kind": 2,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "def call_model(state):\n    messages = state[\"messages\"]\n    response = model.invoke(messages)\n    # We return a list, because this will get added to the existing list\n    return {\"messages\": [response]}\n# Define the function to execute tools\ndef call_tool(state):\n    messages = state[\"messages\"]\n    # Based on the continue condition\n    # we know the last message involves a function call",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "call_tool",
        "kind": 2,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "def call_tool(state):\n    messages = state[\"messages\"]\n    # Based on the continue condition\n    # we know the last message involves a function call\n    last_message = messages[-1]\n    # We construct an ToolInvocation from the function_call\n    action = ToolInvocation(\n        tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n        tool_input=json.loads(\n            last_message.additional_kwargs[\"function_call\"][\"arguments\"]",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"TAVILY_API_KEY\"] = \"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\"\nfrom langchain_community.tools.tavily_search import TavilySearchResults\ntools = [TavilySearchResults(max_results=1)]\nfrom langgraph.prebuilt import ToolExecutor\ntool_executor = ToolExecutor(tools)\nfrom langchain_openai import ChatOpenAI\n# We will set streaming=True so that we can stream tokens\n# See the streaming section for more information on this.",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"TAVILY_API_KEY\"] = \"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\"\nfrom langchain_community.tools.tavily_search import TavilySearchResults\ntools = [TavilySearchResults(max_results=1)]\nfrom langgraph.prebuilt import ToolExecutor\ntool_executor = ToolExecutor(tools)\nfrom langchain_openai import ChatOpenAI\n# We will set streaming=True so that we can stream tokens\n# See the streaming section for more information on this.\nmodel = ChatOpenAI(temperature=0, streaming=True)",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TAVILY_API_KEY\"]",
        "kind": 5,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "os.environ[\"TAVILY_API_KEY\"] = \"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\"\nfrom langchain_community.tools.tavily_search import TavilySearchResults\ntools = [TavilySearchResults(max_results=1)]\nfrom langgraph.prebuilt import ToolExecutor\ntool_executor = ToolExecutor(tools)\nfrom langchain_openai import ChatOpenAI\n# We will set streaming=True so that we can stream tokens\n# See the streaming section for more information on this.\nmodel = ChatOpenAI(temperature=0, streaming=True)\nfrom langchain.tools.render import format_tool_to_openai_function",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "tools = [TavilySearchResults(max_results=1)]\nfrom langgraph.prebuilt import ToolExecutor\ntool_executor = ToolExecutor(tools)\nfrom langchain_openai import ChatOpenAI\n# We will set streaming=True so that we can stream tokens\n# See the streaming section for more information on this.\nmodel = ChatOpenAI(temperature=0, streaming=True)\nfrom langchain.tools.render import format_tool_to_openai_function\nfunctions = [format_tool_to_openai_function(t) for t in tools]\nmodel = model.bind_functions(functions)",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "tool_executor",
        "kind": 5,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "tool_executor = ToolExecutor(tools)\nfrom langchain_openai import ChatOpenAI\n# We will set streaming=True so that we can stream tokens\n# See the streaming section for more information on this.\nmodel = ChatOpenAI(temperature=0, streaming=True)\nfrom langchain.tools.render import format_tool_to_openai_function\nfunctions = [format_tool_to_openai_function(t) for t in tools]\nmodel = model.bind_functions(functions)\nfrom typing import TypedDict, Annotated, Sequence\nimport operator",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "model = ChatOpenAI(temperature=0, streaming=True)\nfrom langchain.tools.render import format_tool_to_openai_function\nfunctions = [format_tool_to_openai_function(t) for t in tools]\nmodel = model.bind_functions(functions)\nfrom typing import TypedDict, Annotated, Sequence\nimport operator\nfrom langchain_core.messages import BaseMessage\nclass AgentState(TypedDict):\n    messages: Annotated[Sequence[BaseMessage], operator.add]\nfrom langgraph.prebuilt import ToolInvocation",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "functions",
        "kind": 5,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "functions = [format_tool_to_openai_function(t) for t in tools]\nmodel = model.bind_functions(functions)\nfrom typing import TypedDict, Annotated, Sequence\nimport operator\nfrom langchain_core.messages import BaseMessage\nclass AgentState(TypedDict):\n    messages: Annotated[Sequence[BaseMessage], operator.add]\nfrom langgraph.prebuilt import ToolInvocation\nimport json\nfrom langchain_core.messages import FunctionMessage",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "model = model.bind_functions(functions)\nfrom typing import TypedDict, Annotated, Sequence\nimport operator\nfrom langchain_core.messages import BaseMessage\nclass AgentState(TypedDict):\n    messages: Annotated[Sequence[BaseMessage], operator.add]\nfrom langgraph.prebuilt import ToolInvocation\nimport json\nfrom langchain_core.messages import FunctionMessage\n# Define the function that determines whether to continue or not",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "workflow",
        "kind": 5,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "workflow = StateGraph(AgentState)\n# Define the two nodes we will cycle between\nworkflow.add_node(\"agent\", call_model)\nworkflow.add_node(\"action\", call_tool)\n# Set the entrypoint as `agent`\n# This means that this node is the first one called\nworkflow.set_entry_point(\"agent\")\n# We now add a conditional edge\nworkflow.add_conditional_edges(\n    # First, we define the start node. We use `agent`.",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "app = workflow.compile()\nfrom langchain_core.messages import HumanMessage\ninputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]}\napp.invoke(inputs)\ninputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]}\nfor output in app.stream(inputs):\n    # stream() yields dictionaries with output keyed by node name\n    for key, value in output.items():\n        print(f\"Output from node '{key}':\")\n        print(\"---\")",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "inputs",
        "kind": 5,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]}\napp.invoke(inputs)\ninputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]}\nfor output in app.stream(inputs):\n    # stream() yields dictionaries with output keyed by node name\n    for key, value in output.items():\n        print(f\"Output from node '{key}':\")\n        print(\"---\")\n        print(value)\n    print(\"\\n---\\n\")",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "inputs",
        "kind": 5,
        "importPath": "langgraph.test041",
        "description": "langgraph.test041",
        "peekOfCode": "inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]}\nfor output in app.stream(inputs):\n    # stream() yields dictionaries with output keyed by node name\n    for key, value in output.items():\n        print(f\"Output from node '{key}':\")\n        print(\"---\")\n        print(value)\n    print(\"\\n---\\n\")\n# inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]}\n# for output in app.astream_log(inputs, include_types=[\"llm\"]):",
        "detail": "langgraph.test041",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "lora_train.test01",
        "description": "lora_train.test01",
        "peekOfCode": "model = Model.from_pretrained(\n    \"ZhipuAI/chatglm2-6b\", torch_dtype=torch.bfloat16, device_map=\"auto\"\n)\nlora_config = LoRAConfig(\n    r=16, target_modules=[\"query_key_value\"], lora_alpha=32, lora_dropout=0.0\n)\nmodel = Swift.prepare_model(model, lora_config)\n# use model to do other things\nprint(model)",
        "detail": "lora_train.test01",
        "documentation": {}
    },
    {
        "label": "lora_config",
        "kind": 5,
        "importPath": "lora_train.test01",
        "description": "lora_train.test01",
        "peekOfCode": "lora_config = LoRAConfig(\n    r=16, target_modules=[\"query_key_value\"], lora_alpha=32, lora_dropout=0.0\n)\nmodel = Swift.prepare_model(model, lora_config)\n# use model to do other things\nprint(model)",
        "detail": "lora_train.test01",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "lora_train.test01",
        "description": "lora_train.test01",
        "peekOfCode": "model = Swift.prepare_model(model, lora_config)\n# use model to do other things\nprint(model)",
        "detail": "lora_train.test01",
        "documentation": {}
    },
    {
        "label": "tokenize_function",
        "kind": 2,
        "importPath": "lora_train.test02",
        "description": "lora_train.test02",
        "peekOfCode": "def tokenize_function(examples):\n    return tokenizer(\n        examples[\"sentence1\"],\n        examples[\"sentence2\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=128,\n    )\ntrain_dataset = train_dataset.map(tokenize_function)\nval_dataset = val_dataset.map(tokenize_function)",
        "detail": "lora_train.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "lora_train.test02",
        "description": "lora_train.test02",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator\nfrom swift import Trainer, LoRAConfig, Swift, TrainingArguments\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)",
        "detail": "lora_train.test02",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "lora_train.test02",
        "description": "lora_train.test02",
        "peekOfCode": "model = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")",
        "detail": "lora_train.test02",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "lora_train.test02",
        "description": "lora_train.test02",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)",
        "detail": "lora_train.test02",
        "documentation": {}
    },
    {
        "label": "lora_config",
        "kind": 5,
        "importPath": "lora_train.test02",
        "description": "lora_train.test02",
        "peekOfCode": "lora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)\nval_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()",
        "detail": "lora_train.test02",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "lora_train.test02",
        "description": "lora_train.test02",
        "peekOfCode": "model = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)\nval_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()\n    .select(range(100))",
        "detail": "lora_train.test02",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "lora_train.test02",
        "description": "lora_train.test02",
        "peekOfCode": "train_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)\nval_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()\n    .select(range(100))\n)",
        "detail": "lora_train.test02",
        "documentation": {}
    },
    {
        "label": "val_dataset",
        "kind": 5,
        "importPath": "lora_train.test02",
        "description": "lora_train.test02",
        "peekOfCode": "val_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()\n    .select(range(100))\n)\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"sentence1\"],\n        examples[\"sentence2\"],\n        padding=\"max_length\",",
        "detail": "lora_train.test02",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "lora_train.test02",
        "description": "lora_train.test02",
        "peekOfCode": "train_dataset = train_dataset.map(tokenize_function)\nval_dataset = val_dataset.map(tokenize_function)\narguments = TrainingArguments(\n    output_dir=\"./outputs\",\n    per_device_train_batch_size=16,\n)\ntrainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,",
        "detail": "lora_train.test02",
        "documentation": {}
    },
    {
        "label": "val_dataset",
        "kind": 5,
        "importPath": "lora_train.test02",
        "description": "lora_train.test02",
        "peekOfCode": "val_dataset = val_dataset.map(tokenize_function)\narguments = TrainingArguments(\n    output_dir=\"./outputs\",\n    per_device_train_batch_size=16,\n)\ntrainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,",
        "detail": "lora_train.test02",
        "documentation": {}
    },
    {
        "label": "arguments",
        "kind": 5,
        "importPath": "lora_train.test02",
        "description": "lora_train.test02",
        "peekOfCode": "arguments = TrainingArguments(\n    output_dir=\"./outputs\",\n    per_device_train_batch_size=16,\n)\ntrainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=default_data_collator,",
        "detail": "lora_train.test02",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "lora_train.test02",
        "description": "lora_train.test02",
        "peekOfCode": "trainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=default_data_collator,\n)\ntrainer.train()",
        "detail": "lora_train.test02",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "lora_train.test03",
        "description": "lora_train.test03",
        "peekOfCode": "model = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))",
        "detail": "lora_train.test03",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "lora_train.test03",
        "description": "lora_train.test03",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))",
        "detail": "lora_train.test03",
        "documentation": {}
    },
    {
        "label": "lora_config",
        "kind": 5,
        "importPath": "lora_train.test03",
        "description": "lora_train.test03",
        "peekOfCode": "lora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))",
        "detail": "lora_train.test03",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "lora_train.test03",
        "description": "lora_train.test03",
        "peekOfCode": "model = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))",
        "detail": "lora_train.test03",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "lora_train.test04",
        "description": "lora_train.test04",
        "peekOfCode": "config = ResTuningConfig(\n    dims=768,\n    root_modules=r\".*blocks.0$\",\n    stem_modules=r\".*blocks\\.\\d+$\",\n    target_modules=r\"norm\",\n    tuner_cfg=\"res_adapter\",\n)\nfrom swift import Swift\nimport timm, torch\nmodel = timm.create_model(\"vit_base_patch16_224\", pretrained=False, num_classes=100)",
        "detail": "lora_train.test04",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "lora_train.test04",
        "description": "lora_train.test04",
        "peekOfCode": "model = timm.create_model(\"vit_base_patch16_224\", pretrained=False, num_classes=100)\nmodel_tune = Swift.prepare_model(model, config)\nprint(model_tune.get_trainable_parameters())\nprint(model(torch.ones(1, 3, 224, 224)).shape)",
        "detail": "lora_train.test04",
        "documentation": {}
    },
    {
        "label": "model_tune",
        "kind": 5,
        "importPath": "lora_train.test04",
        "description": "lora_train.test04",
        "peekOfCode": "model_tune = Swift.prepare_model(model, config)\nprint(model_tune.get_trainable_parameters())\nprint(model(torch.ones(1, 3, 224, 224)).shape)",
        "detail": "lora_train.test04",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "lora_train.test05",
        "description": "lora_train.test05",
        "peekOfCode": "model = SbertForSequenceClassification(SbertConfig())\nlora_config = LoraConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, lora_config)\nprint(model)",
        "detail": "lora_train.test05",
        "documentation": {}
    },
    {
        "label": "lora_config",
        "kind": 5,
        "importPath": "lora_train.test05",
        "description": "lora_train.test05",
        "peekOfCode": "lora_config = LoraConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, lora_config)\nprint(model)",
        "detail": "lora_train.test05",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "lora_train.test05",
        "description": "lora_train.test05",
        "peekOfCode": "model = Swift.prepare_model(model, lora_config)\nprint(model)",
        "detail": "lora_train.test05",
        "documentation": {}
    },
    {
        "label": "pipeline",
        "kind": 5,
        "importPath": "lora_train.test06",
        "description": "lora_train.test06",
        "peekOfCode": "pipeline = Pipeline.from_pretrained(\n    \"pyannote/speaker-diarization-3.1\",\n    use_auth_token=\"HUGGINGFACE_ACCESS_TOKEN_GOES_HERE\",\n)\n# run the pipeline on an audio file\ndiarization = pipeline(\"audio.wav\")\n# dump the diarization output to disk using RTTM format\nwith open(\"audio.rttm\", \"w\") as rttm:\n    diarization.write_rttm(rttm)",
        "detail": "lora_train.test06",
        "documentation": {}
    },
    {
        "label": "diarization",
        "kind": 5,
        "importPath": "lora_train.test06",
        "description": "lora_train.test06",
        "peekOfCode": "diarization = pipeline(\"audio.wav\")\n# dump the diarization output to disk using RTTM format\nwith open(\"audio.rttm\", \"w\") as rttm:\n    diarization.write_rttm(rttm)",
        "detail": "lora_train.test06",
        "documentation": {}
    },
    {
        "label": "m",
        "kind": 5,
        "importPath": "lora_train.timm_test01",
        "description": "lora_train.timm_test01",
        "peekOfCode": "m = timm.create_model(\"mobilenetv3_large_100\", pretrained=True)\nm.eval()",
        "detail": "lora_train.timm_test01",
        "documentation": {}
    },
    {
        "label": "model_names",
        "kind": 5,
        "importPath": "lora_train.timm_test02",
        "description": "lora_train.timm_test02",
        "peekOfCode": "model_names = timm.list_models(pretrained=True)\npprint(model_names)",
        "detail": "lora_train.timm_test02",
        "documentation": {}
    },
    {
        "label": "model_names",
        "kind": 5,
        "importPath": "lora_train.timm_test03",
        "description": "lora_train.timm_test03",
        "peekOfCode": "model_names = timm.list_models(\"*resne*t*\")\npprint(model_names)",
        "detail": "lora_train.timm_test03",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "lora_train.timm_test04",
        "description": "lora_train.timm_test04",
        "peekOfCode": "x = torch.randn(1, 3, 224, 224)\nmodel = timm.create_model(\"mobilenetv3_large_100\", pretrained=True)\nfeatures = model.forward_features(x)\nprint(features.shape)\ntimm.data.create_transform((3, 224, 224))",
        "detail": "lora_train.timm_test04",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "lora_train.timm_test04",
        "description": "lora_train.timm_test04",
        "peekOfCode": "model = timm.create_model(\"mobilenetv3_large_100\", pretrained=True)\nfeatures = model.forward_features(x)\nprint(features.shape)\ntimm.data.create_transform((3, 224, 224))",
        "detail": "lora_train.timm_test04",
        "documentation": {}
    },
    {
        "label": "features",
        "kind": 5,
        "importPath": "lora_train.timm_test04",
        "description": "lora_train.timm_test04",
        "peekOfCode": "features = model.forward_features(x)\nprint(features.shape)\ntimm.data.create_transform((3, 224, 224))",
        "detail": "lora_train.timm_test04",
        "documentation": {}
    },
    {
        "label": "sft_args",
        "kind": 5,
        "importPath": "modelScope.qwen_14b_sft",
        "description": "modelScope.qwen_14b_sft",
        "peekOfCode": "sft_args = SftArguments(\n    model_type=ModelType.qwen_14b_chat,\n    # model_cache_dir=\"Qwen-14B-Chat\",\n    dataset=[DatasetName.alpaca_zh, DatasetName.alpaca_en],\n    train_dataset_sample=500,\n    eval_steps=50,\n    batch_size=16,\n    logging_steps=20,\n    num_train_epochs=40,\n    learning_rate=1e-4,",
        "detail": "modelScope.qwen_14b_sft",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": "modelScope.qwen_14b_sft",
        "description": "modelScope.qwen_14b_sft",
        "peekOfCode": "output = sft_main(sft_args)\nbest_model_checkpoint = output[\"best_model_checkpoint\"]\nprint(f\"best_model_checkpoint: {best_model_checkpoint}\")",
        "detail": "modelScope.qwen_14b_sft",
        "documentation": {}
    },
    {
        "label": "best_model_checkpoint",
        "kind": 5,
        "importPath": "modelScope.qwen_14b_sft",
        "description": "modelScope.qwen_14b_sft",
        "peekOfCode": "best_model_checkpoint = output[\"best_model_checkpoint\"]\nprint(f\"best_model_checkpoint: {best_model_checkpoint}\")",
        "detail": "modelScope.qwen_14b_sft",
        "documentation": {}
    },
    {
        "label": "model_dir",
        "kind": 5,
        "importPath": "modelScope.test",
        "description": "modelScope.test",
        "peekOfCode": "model_dir = snapshot_download(\"AI-ModelScope/TinyLlama-1.1B-Chat-v1.0\")\npipe = pipeline(\n    \"text-generation\", model=model_dir, torch_dtype=torch.bfloat16, device_map=\"auto\"\n)\n# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },",
        "detail": "modelScope.test",
        "documentation": {}
    },
    {
        "label": "pipe",
        "kind": 5,
        "importPath": "modelScope.test",
        "description": "modelScope.test",
        "peekOfCode": "pipe = pipeline(\n    \"text-generation\", model=model_dir, torch_dtype=torch.bfloat16, device_map=\"auto\"\n)\n# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\"role\": \"user\", \"content\": \"我们能干啥？\"},",
        "detail": "modelScope.test",
        "documentation": {}
    },
    {
        "label": "messages",
        "kind": 5,
        "importPath": "modelScope.test",
        "description": "modelScope.test",
        "peekOfCode": "messages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\"role\": \"user\", \"content\": \"我们能干啥？\"},\n]\nprompt = pipe.tokenizer.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)",
        "detail": "modelScope.test",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "modelScope.test",
        "description": "modelScope.test",
        "peekOfCode": "prompt = pipe.tokenizer.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\noutputs = pipe(\n    prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95\n)\nprint(outputs[0][\"generated_text\"])\n# <|system|>\n# You are a friendly chatbot who always responds in the style of a pirate.</s>\n# <|user|>",
        "detail": "modelScope.test",
        "documentation": {}
    },
    {
        "label": "outputs",
        "kind": 5,
        "importPath": "modelScope.test",
        "description": "modelScope.test",
        "peekOfCode": "outputs = pipe(\n    prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95\n)\nprint(outputs[0][\"generated_text\"])\n# <|system|>\n# You are a friendly chatbot who always responds in the style of a pirate.</s>\n# <|user|>\n# How many helicopters can a human eat in one sitting?</s>\n# <|assistant|>\n# ...",
        "detail": "modelScope.test",
        "documentation": {}
    },
    {
        "label": "ocr_detection",
        "kind": 5,
        "importPath": "modelScope.test01",
        "description": "modelScope.test01",
        "peekOfCode": "ocr_detection = pipeline(\n    Tasks.ocr_detection, model=\"damo/cv_resnet18_ocr-detection-line-level_damo\"\n)\nresult = ocr_detection(\n    \"https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/ocr_detection.jpg\"\n)\nprint(result)",
        "detail": "modelScope.test01",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "modelScope.test01",
        "description": "modelScope.test01",
        "peekOfCode": "result = ocr_detection(\n    \"https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/ocr_detection.jpg\"\n)\nprint(result)",
        "detail": "modelScope.test01",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test02",
        "description": "modelScope.test02",
        "peekOfCode": "model = timm.create_model(\"hf_hub:notmahi/dobb-e\", pretrained=True)\nmodel.eval()",
        "detail": "modelScope.test02",
        "documentation": {}
    },
    {
        "label": "model_path",
        "kind": 5,
        "importPath": "modelScope.test03",
        "description": "modelScope.test03",
        "peekOfCode": "model_path = \"sword_out\"\npipe = StableDiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-2-1\", paddle_dtype=paddle.float32\n)\n# 注意：如果我们想从 HF Hub 加载权重，那么我们需要设置 from_hf_hub=True\npipe.unet.load_attn_procs(model_path, from_hf_hub=False)\nprompt = \"sword_1bug,sword,simple background,upward\"\nimage = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\nimage.save(\"sword.png\")",
        "detail": "modelScope.test03",
        "documentation": {}
    },
    {
        "label": "pipe",
        "kind": 5,
        "importPath": "modelScope.test03",
        "description": "modelScope.test03",
        "peekOfCode": "pipe = StableDiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-2-1\", paddle_dtype=paddle.float32\n)\n# 注意：如果我们想从 HF Hub 加载权重，那么我们需要设置 from_hf_hub=True\npipe.unet.load_attn_procs(model_path, from_hf_hub=False)\nprompt = \"sword_1bug,sword,simple background,upward\"\nimage = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\nimage.save(\"sword.png\")",
        "detail": "modelScope.test03",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "modelScope.test03",
        "description": "modelScope.test03",
        "peekOfCode": "prompt = \"sword_1bug,sword,simple background,upward\"\nimage = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\nimage.save(\"sword.png\")",
        "detail": "modelScope.test03",
        "documentation": {}
    },
    {
        "label": "image",
        "kind": 5,
        "importPath": "modelScope.test03",
        "description": "modelScope.test03",
        "peekOfCode": "image = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\nimage.save(\"sword.png\")",
        "detail": "modelScope.test03",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "modelScope.test04",
        "description": "modelScope.test04",
        "peekOfCode": "data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\ntable = pd.DataFrame.from_dict(data)\nquestion = \"how many movies does Leonardo Di Caprio have?\"\n# pipeline model\n# Note: you must to install torch-scatter first.\ntqa = pipeline(task=\"table-question-answering\", model=\"google/tapas-large-finetuned-wtq\")\n# result\nprint(tqa(table=table, query=question)['cells'][0])\n#53",
        "detail": "modelScope.test04",
        "documentation": {}
    },
    {
        "label": "table",
        "kind": 5,
        "importPath": "modelScope.test04",
        "description": "modelScope.test04",
        "peekOfCode": "table = pd.DataFrame.from_dict(data)\nquestion = \"how many movies does Leonardo Di Caprio have?\"\n# pipeline model\n# Note: you must to install torch-scatter first.\ntqa = pipeline(task=\"table-question-answering\", model=\"google/tapas-large-finetuned-wtq\")\n# result\nprint(tqa(table=table, query=question)['cells'][0])\n#53",
        "detail": "modelScope.test04",
        "documentation": {}
    },
    {
        "label": "question",
        "kind": 5,
        "importPath": "modelScope.test04",
        "description": "modelScope.test04",
        "peekOfCode": "question = \"how many movies does Leonardo Di Caprio have?\"\n# pipeline model\n# Note: you must to install torch-scatter first.\ntqa = pipeline(task=\"table-question-answering\", model=\"google/tapas-large-finetuned-wtq\")\n# result\nprint(tqa(table=table, query=question)['cells'][0])\n#53",
        "detail": "modelScope.test04",
        "documentation": {}
    },
    {
        "label": "tqa",
        "kind": 5,
        "importPath": "modelScope.test04",
        "description": "modelScope.test04",
        "peekOfCode": "tqa = pipeline(task=\"table-question-answering\", model=\"google/tapas-large-finetuned-wtq\")\n# result\nprint(tqa(table=table, query=question)['cells'][0])\n#53",
        "detail": "modelScope.test04",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "model = \"ZhipuAI/CodeGeeX-Code-Generation-13B\"\npipe = pipeline(\n    task=Tasks.code_generation,\n    model=model,\n)\ninputs = {\n    \"prompt\": 'from typing import List\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n    return\",',\n    \"language\": \"Python\",\n}\nresult = pipe(inputs)",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "pipe",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "pipe = pipeline(\n    task=Tasks.code_generation,\n    model=model,\n)\ninputs = {\n    \"prompt\": 'from typing import List\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n    return\",',\n    \"language\": \"Python\",\n}\nresult = pipe(inputs)\nprint(result)",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "inputs",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "inputs = {\n    \"prompt\": 'from typing import List\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n    return\",',\n    \"language\": \"Python\",\n}\nresult = pipe(inputs)\nprint(result)",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "result = pipe(inputs)\nprint(result)",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "tokenize_function",
        "kind": 2,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "def tokenize_function(examples):\n    return tokenizer(\n        examples[\"sentence1\"],\n        examples[\"sentence2\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=128,\n    )\ntrain_dataset = train_dataset.map(tokenize_function)\nval_dataset = val_dataset.map(tokenize_function)",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "model = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "lora_config",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "lora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)\nval_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "model = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)\nval_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()\n    .select(range(100))",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "train_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)\nval_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()\n    .select(range(100))\n)",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "val_dataset",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "val_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()\n    .select(range(100))\n)\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"sentence1\"],\n        examples[\"sentence2\"],\n        padding=\"max_length\",",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "train_dataset = train_dataset.map(tokenize_function)\nval_dataset = val_dataset.map(tokenize_function)\narguments = TrainingArguments(\n    output_dir=\"./outputs\",\n    per_device_train_batch_size=16,\n)\ntrainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "val_dataset",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "val_dataset = val_dataset.map(tokenize_function)\narguments = TrainingArguments(\n    output_dir=\"./outputs\",\n    per_device_train_batch_size=16,\n)\ntrainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "arguments",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "arguments = TrainingArguments(\n    output_dir=\"./outputs\",\n    per_device_train_batch_size=16,\n)\ntrainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=default_data_collator,",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "trainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=default_data_collator,\n)\ntrainer.train()",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "model = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(\"start\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(\"start\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))\noutputs = model(\n    **tokenizer(\"This is an example sentence for classification.\", return_tensors=\"pt\")\n)",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "lora_config",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "lora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(\"start\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))\noutputs = model(\n    **tokenizer(\"This is an example sentence for classification.\", return_tensors=\"pt\")\n)\nlogits = outputs.logits\nprobabilities = logits.softmax(dim=1)\nprint(probabilities)",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "model = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(\"start\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))\noutputs = model(\n    **tokenizer(\"This is an example sentence for classification.\", return_tensors=\"pt\")\n)\nlogits = outputs.logits\nprobabilities = logits.softmax(dim=1)\nprint(probabilities)\npredicted_class_index = probabilities.argmax(dim=1)",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "outputs",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "outputs = model(\n    **tokenizer(\"This is an example sentence for classification.\", return_tensors=\"pt\")\n)\nlogits = outputs.logits\nprobabilities = logits.softmax(dim=1)\nprint(probabilities)\npredicted_class_index = probabilities.argmax(dim=1)\nprint(predicted_class_index)\npredicted_label = tokenizer.decode(predicted_class_index)\nprint(f\"Predicted label: {predicted_label}\")",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "logits",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "logits = outputs.logits\nprobabilities = logits.softmax(dim=1)\nprint(probabilities)\npredicted_class_index = probabilities.argmax(dim=1)\nprint(predicted_class_index)\npredicted_label = tokenizer.decode(predicted_class_index)\nprint(f\"Predicted label: {predicted_label}\")\nprint(\"end\")",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "probabilities",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "probabilities = logits.softmax(dim=1)\nprint(probabilities)\npredicted_class_index = probabilities.argmax(dim=1)\nprint(predicted_class_index)\npredicted_label = tokenizer.decode(predicted_class_index)\nprint(f\"Predicted label: {predicted_label}\")\nprint(\"end\")",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "predicted_class_index",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "predicted_class_index = probabilities.argmax(dim=1)\nprint(predicted_class_index)\npredicted_label = tokenizer.decode(predicted_class_index)\nprint(f\"Predicted label: {predicted_label}\")\nprint(\"end\")",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "predicted_label",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "predicted_label = tokenizer.decode(predicted_class_index)\nprint(f\"Predicted label: {predicted_label}\")\nprint(\"end\")",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test08",
        "description": "modelScope.test08",
        "peekOfCode": "model = SbertForSequenceClassification(SbertConfig())\nlora_config = LoraConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, lora_config)",
        "detail": "modelScope.test08",
        "documentation": {}
    },
    {
        "label": "lora_config",
        "kind": 5,
        "importPath": "modelScope.test08",
        "description": "modelScope.test08",
        "peekOfCode": "lora_config = LoraConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, lora_config)",
        "detail": "modelScope.test08",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test08",
        "description": "modelScope.test08",
        "peekOfCode": "model = Swift.prepare_model(model, lora_config)",
        "detail": "modelScope.test08",
        "documentation": {}
    },
    {
        "label": "model_dir",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "model_dir = 'vx_xxx/checkpoint-100-merged'\nmodel_type = ModelType.qwen_7b_chat\ntemplate_type = get_default_template_type(model_type)\nllm_engine = get_vllm_engine(model_type, model_dir=model_dir)\ntokenizer = llm_engine.tokenizer\ntemplate = get_template(template_type, tokenizer)\nquery = '你好'\nresp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "model_type",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "model_type = ModelType.qwen_7b_chat\ntemplate_type = get_default_template_type(model_type)\nllm_engine = get_vllm_engine(model_type, model_dir=model_dir)\ntokenizer = llm_engine.tokenizer\ntemplate = get_template(template_type, tokenizer)\nquery = '你好'\nresp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "template_type",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "template_type = get_default_template_type(model_type)\nllm_engine = get_vllm_engine(model_type, model_dir=model_dir)\ntokenizer = llm_engine.tokenizer\ntemplate = get_template(template_type, tokenizer)\nquery = '你好'\nresp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "llm_engine",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "llm_engine = get_vllm_engine(model_type, model_dir=model_dir)\ntokenizer = llm_engine.tokenizer\ntemplate = get_template(template_type, tokenizer)\nquery = '你好'\nresp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "tokenizer = llm_engine.tokenizer\ntemplate = get_template(template_type, tokenizer)\nquery = '你好'\nresp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "template = get_template(template_type, tokenizer)\nquery = '你好'\nresp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "query",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "query = '你好'\nresp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "resp",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "resp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "MySQL",
        "kind": 6,
        "importPath": "my_utils.MysqlUtility",
        "description": "my_utils.MysqlUtility",
        "peekOfCode": "class MySQL:\n    u'''对MySQLdb常用函数进行封装的类'''\n    error_code = ''  # MySQL错误号码\n    _instance = None  # 本类的实例\n    _conn = None  # 数据库conn\n    _cur = None  # 游标\n    _TIMEOUT = 30  # 默认超时30秒\n    _timecount = 0\n    def __init__(self, dbconfig):\n        u'构造器：根据数据库连接参数，创建MySQL连接'",
        "detail": "my_utils.MysqlUtility",
        "documentation": {}
    },
    {
        "label": "paddstr",
        "kind": 2,
        "importPath": "my_utils.MysqlUtility",
        "description": "my_utils.MysqlUtility",
        "peekOfCode": "def paddstr(strx):\n    return \"NULL\" if strx == None else \"\\'\" + MySQLdb.escape_string(str(strx)).decode() + \"\\'\"\ndef paddnum(numx, default='NULL'):\n    return default if numx == None else str(numx)\nclass MySQL:\n    u'''对MySQLdb常用函数进行封装的类'''\n    error_code = ''  # MySQL错误号码\n    _instance = None  # 本类的实例\n    _conn = None  # 数据库conn\n    _cur = None  # 游标",
        "detail": "my_utils.MysqlUtility",
        "documentation": {}
    },
    {
        "label": "paddnum",
        "kind": 2,
        "importPath": "my_utils.MysqlUtility",
        "description": "my_utils.MysqlUtility",
        "peekOfCode": "def paddnum(numx, default='NULL'):\n    return default if numx == None else str(numx)\nclass MySQL:\n    u'''对MySQLdb常用函数进行封装的类'''\n    error_code = ''  # MySQL错误号码\n    _instance = None  # 本类的实例\n    _conn = None  # 数据库conn\n    _cur = None  # 游标\n    _TIMEOUT = 30  # 默认超时30秒\n    _timecount = 0",
        "detail": "my_utils.MysqlUtility",
        "documentation": {}
    },
    {
        "label": "ollama_host",
        "kind": 5,
        "importPath": "ollama.test01",
        "description": "ollama.test01",
        "peekOfCode": "ollama_host = \"localhost\"\n# 设置Ollama的端口\nollama_port = 11434\n# 设置Ollama的模型\nollama_model = \"llama2\"\nfrom langchain.llms import Ollama\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nif __name__ == \"__main__\":\n    # 通过API的方式调用Ollama",
        "detail": "ollama.test01",
        "documentation": {}
    },
    {
        "label": "ollama_port",
        "kind": 5,
        "importPath": "ollama.test01",
        "description": "ollama.test01",
        "peekOfCode": "ollama_port = 11434\n# 设置Ollama的模型\nollama_model = \"llama2\"\nfrom langchain.llms import Ollama\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nif __name__ == \"__main__\":\n    # 通过API的方式调用Ollama\n    # StreamingStdOutCallbackHandler 使用流式输出结果\n    llm = Ollama(",
        "detail": "ollama.test01",
        "documentation": {}
    },
    {
        "label": "ollama_model",
        "kind": 5,
        "importPath": "ollama.test01",
        "description": "ollama.test01",
        "peekOfCode": "ollama_model = \"llama2\"\nfrom langchain.llms import Ollama\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nif __name__ == \"__main__\":\n    # 通过API的方式调用Ollama\n    # StreamingStdOutCallbackHandler 使用流式输出结果\n    llm = Ollama(\n        base_url=f\"http://{ollama_host}:{ollama_port}\",\n        model=ollama_model,",
        "detail": "ollama.test01",
        "documentation": {}
    },
    {
        "label": "SuppressStdout",
        "kind": 6,
        "importPath": "ollama.test02",
        "description": "ollama.test02",
        "peekOfCode": "class SuppressStdout:\n    # __enter__ 最先开始的时候会执行一次\n    def __enter__(self):\n        self._original_stdout = sys.stdout\n        self._original_stderr = sys.stderr\n        sys.stdout = open(os.devnull, \"w\")\n        sys.stderr = open(os.devnull, \"w\")\n    # __exit__ 最后执行一次\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        sys.stdout.close()",
        "detail": "ollama.test02",
        "documentation": {}
    },
    {
        "label": "ollama_host",
        "kind": 5,
        "importPath": "ollama.test02",
        "description": "ollama.test02",
        "peekOfCode": "ollama_host = \"localhost\"\n# 设置Ollama的端口\nollama_port = 11434\n# 设置Ollama的模型\nollama_model = \"llama2\"\nfrom langchain.document_loaders import OnlinePDFLoader\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import GPT4AllEmbeddings\nfrom langchain import PromptTemplate\nfrom langchain.llms import Ollama",
        "detail": "ollama.test02",
        "documentation": {}
    },
    {
        "label": "ollama_port",
        "kind": 5,
        "importPath": "ollama.test02",
        "description": "ollama.test02",
        "peekOfCode": "ollama_port = 11434\n# 设置Ollama的模型\nollama_model = \"llama2\"\nfrom langchain.document_loaders import OnlinePDFLoader\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import GPT4AllEmbeddings\nfrom langchain import PromptTemplate\nfrom langchain.llms import Ollama\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler",
        "detail": "ollama.test02",
        "documentation": {}
    },
    {
        "label": "ollama_model",
        "kind": 5,
        "importPath": "ollama.test02",
        "description": "ollama.test02",
        "peekOfCode": "ollama_model = \"llama2\"\nfrom langchain.document_loaders import OnlinePDFLoader\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import GPT4AllEmbeddings\nfrom langchain import PromptTemplate\nfrom langchain.llms import Ollama\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.chains import RetrievalQA\nimport sys",
        "detail": "ollama.test02",
        "documentation": {}
    },
    {
        "label": "loader",
        "kind": 5,
        "importPath": "ollama.test02",
        "description": "ollama.test02",
        "peekOfCode": "loader = OnlinePDFLoader(\n    \"https://d18rn0p25nwr6d.cloudfront.net/CIK-0001813756/975b3e9b-268e-4798-a9e4-2a9a7c92dc10.pdf\"\n)\ndata = loader.load()\n# RecursiveCharacterTextSplitter是一个基于递归的字符级别的分词器。它可以将文本逐个字符地拆分，并且可以递归地处理包含其他词汇的字符，例如中文中的词语或日语中的汉字。\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n# chunk_size 设置为分割块的大小\n# chunk_overlap 设置每个分割块之间重叠的字符数。\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\nall_splits = text_splitter.split_documents(data)",
        "detail": "ollama.test02",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "ollama.test02",
        "description": "ollama.test02",
        "peekOfCode": "data = loader.load()\n# RecursiveCharacterTextSplitter是一个基于递归的字符级别的分词器。它可以将文本逐个字符地拆分，并且可以递归地处理包含其他词汇的字符，例如中文中的词语或日语中的汉字。\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n# chunk_size 设置为分割块的大小\n# chunk_overlap 设置每个分割块之间重叠的字符数。\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\nall_splits = text_splitter.split_documents(data)\n# 使用Chroma数据库进行存储所有的分片，使用的分片方式是GPT4ALL\nwith SuppressStdout():\n    vectorstore = Chroma.from_documents(",
        "detail": "ollama.test02",
        "documentation": {}
    },
    {
        "label": "text_splitter",
        "kind": 5,
        "importPath": "ollama.test02",
        "description": "ollama.test02",
        "peekOfCode": "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\nall_splits = text_splitter.split_documents(data)\n# 使用Chroma数据库进行存储所有的分片，使用的分片方式是GPT4ALL\nwith SuppressStdout():\n    vectorstore = Chroma.from_documents(\n        documents=all_splits, embedding=GPT4AllEmbeddings()\n    )\nwhile True:\n    query = input(\"\\nQuery: \")\n    # 退出设置",
        "detail": "ollama.test02",
        "documentation": {}
    },
    {
        "label": "all_splits",
        "kind": 5,
        "importPath": "ollama.test02",
        "description": "ollama.test02",
        "peekOfCode": "all_splits = text_splitter.split_documents(data)\n# 使用Chroma数据库进行存储所有的分片，使用的分片方式是GPT4ALL\nwith SuppressStdout():\n    vectorstore = Chroma.from_documents(\n        documents=all_splits, embedding=GPT4AllEmbeddings()\n    )\nwhile True:\n    query = input(\"\\nQuery: \")\n    # 退出设置\n    if query == \"exit\":",
        "detail": "ollama.test02",
        "documentation": {}
    },
    {
        "label": "truncate",
        "kind": 2,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "def truncate(q):\n    if q is None:\n        return None\n    size = len(q)\n    return q if size <= 20 else q[0:10] + str(size) + q[size-10:size]\ndef encrypt(signStr):\n    hash_algorithm = hashlib.sha256()\n    hash_algorithm.update(signStr.encode('utf-8'))\n    return hash_algorithm.hexdigest()\ndef do_request(data):",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "encrypt",
        "kind": 2,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "def encrypt(signStr):\n    hash_algorithm = hashlib.sha256()\n    hash_algorithm.update(signStr.encode('utf-8'))\n    return hash_algorithm.hexdigest()\ndef do_request(data):\n    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n    return requests.post(YOUDAO_URL, data=data, headers=headers)\ndef encrypt_tts(signStr):\n    hash_algorithm = hashlib.md5()\n    hash_algorithm.update(signStr.encode('utf-8'))",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "do_request",
        "kind": 2,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "def do_request(data):\n    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n    return requests.post(YOUDAO_URL, data=data, headers=headers)\ndef encrypt_tts(signStr):\n    hash_algorithm = hashlib.md5()\n    hash_algorithm.update(signStr.encode('utf-8'))\n    return hash_algorithm.hexdigest()\ndef do_request_tts(data):\n    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n    return requests.post(YOUDAO_URL_TTS, data=data, headers=headers)",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "encrypt_tts",
        "kind": 2,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "def encrypt_tts(signStr):\n    hash_algorithm = hashlib.md5()\n    hash_algorithm.update(signStr.encode('utf-8'))\n    return hash_algorithm.hexdigest()\ndef do_request_tts(data):\n    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n    return requests.post(YOUDAO_URL_TTS, data=data, headers=headers)\ndef wav_to_text(audio_file_path):\n    lang_type = 'zh-CHS'\n    extension = audio_file_path[audio_file_path.rindex('.')+1:]",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "do_request_tts",
        "kind": 2,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "def do_request_tts(data):\n    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n    return requests.post(YOUDAO_URL_TTS, data=data, headers=headers)\ndef wav_to_text(audio_file_path):\n    lang_type = 'zh-CHS'\n    extension = audio_file_path[audio_file_path.rindex('.')+1:]\n    if extension != 'wav':\n        print('不支持的音频类型')\n        sys.exit(1)\n    wav_info = wave.open(audio_file_path, 'rb')",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "wav_to_text",
        "kind": 2,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "def wav_to_text(audio_file_path):\n    lang_type = 'zh-CHS'\n    extension = audio_file_path[audio_file_path.rindex('.')+1:]\n    if extension != 'wav':\n        print('不支持的音频类型')\n        sys.exit(1)\n    wav_info = wave.open(audio_file_path, 'rb')\n    sample_rate = wav_info.getframerate()\n    nchannels = wav_info.getnchannels()\n    wav_info.close()",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "text_to_wav",
        "kind": 2,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "def text_to_wav(text):\n    data = {}\n    data['langType'] = 'zh-CHS'\n    salt = str(uuid.uuid1())\n    signStr = APP_KEY + text + salt + APP_SECRET\n    sign = encrypt_tts(signStr)\n    data['appKey'] = APP_KEY\n    data['q'] = text\n    data['salt'] = salt\n    data['sign'] = sign",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "trans_mp3_to_wav",
        "kind": 2,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "def trans_mp3_to_wav(filepath):\n    song = AudioSegment.from_mp3(filepath)\n    song.export(filepath.replace(\"mp3\",\"wav\"), format=\"wav\")\n    return filepath.replace(\"mp3\",\"wav\")\n# 设置音频参数\nFORMAT = pyaudio.paInt16\nCHANNELS = 1\nRATE = 16000 #44100\nCHUNK = 1024\nRECORD_SECONDS = 5",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "client = ZhipuAI(api_key=\"ac92c959bb3b9092c6f2e0da45cb3acf.kSuemdoJcPgFxieY\") # 填写您自己的APIKey\nreload(sys)\n#有道语音识别\nYOUDAO_URL = 'https://openapi.youdao.com/asrapi'\nYOUDAO_URL_TTS = 'https://openapi.youdao.com/ttsapi'\nAPP_KEY = '28d5d96858e3caee'\nAPP_SECRET = '3FXrws2dDX3Gq0p1GGc0Ek8bMoDGBl6d'\nWAVE_OUTPUT_FILENAME = \"D:/python_project/emotiVoice/output.wav\"\ndef truncate(q):\n    if q is None:",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "YOUDAO_URL",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "YOUDAO_URL = 'https://openapi.youdao.com/asrapi'\nYOUDAO_URL_TTS = 'https://openapi.youdao.com/ttsapi'\nAPP_KEY = '28d5d96858e3caee'\nAPP_SECRET = '3FXrws2dDX3Gq0p1GGc0Ek8bMoDGBl6d'\nWAVE_OUTPUT_FILENAME = \"D:/python_project/emotiVoice/output.wav\"\ndef truncate(q):\n    if q is None:\n        return None\n    size = len(q)\n    return q if size <= 20 else q[0:10] + str(size) + q[size-10:size]",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "YOUDAO_URL_TTS",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "YOUDAO_URL_TTS = 'https://openapi.youdao.com/ttsapi'\nAPP_KEY = '28d5d96858e3caee'\nAPP_SECRET = '3FXrws2dDX3Gq0p1GGc0Ek8bMoDGBl6d'\nWAVE_OUTPUT_FILENAME = \"D:/python_project/emotiVoice/output.wav\"\ndef truncate(q):\n    if q is None:\n        return None\n    size = len(q)\n    return q if size <= 20 else q[0:10] + str(size) + q[size-10:size]\ndef encrypt(signStr):",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "APP_KEY",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "APP_KEY = '28d5d96858e3caee'\nAPP_SECRET = '3FXrws2dDX3Gq0p1GGc0Ek8bMoDGBl6d'\nWAVE_OUTPUT_FILENAME = \"D:/python_project/emotiVoice/output.wav\"\ndef truncate(q):\n    if q is None:\n        return None\n    size = len(q)\n    return q if size <= 20 else q[0:10] + str(size) + q[size-10:size]\ndef encrypt(signStr):\n    hash_algorithm = hashlib.sha256()",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "APP_SECRET",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "APP_SECRET = '3FXrws2dDX3Gq0p1GGc0Ek8bMoDGBl6d'\nWAVE_OUTPUT_FILENAME = \"D:/python_project/emotiVoice/output.wav\"\ndef truncate(q):\n    if q is None:\n        return None\n    size = len(q)\n    return q if size <= 20 else q[0:10] + str(size) + q[size-10:size]\ndef encrypt(signStr):\n    hash_algorithm = hashlib.sha256()\n    hash_algorithm.update(signStr.encode('utf-8'))",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "WAVE_OUTPUT_FILENAME",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "WAVE_OUTPUT_FILENAME = \"D:/python_project/emotiVoice/output.wav\"\ndef truncate(q):\n    if q is None:\n        return None\n    size = len(q)\n    return q if size <= 20 else q[0:10] + str(size) + q[size-10:size]\ndef encrypt(signStr):\n    hash_algorithm = hashlib.sha256()\n    hash_algorithm.update(signStr.encode('utf-8'))\n    return hash_algorithm.hexdigest()",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "FORMAT",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "FORMAT = pyaudio.paInt16\nCHANNELS = 1\nRATE = 16000 #44100\nCHUNK = 1024\nRECORD_SECONDS = 5\nWAVE_OUTPUT_FILENAME = WAVE_OUTPUT_FILENAME #\"output.wav\"\n# 初始化PyAudio\naudio = pyaudio.PyAudio()\n# 打开音频流\nstream = audio.open(format=FORMAT,",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "CHANNELS",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "CHANNELS = 1\nRATE = 16000 #44100\nCHUNK = 1024\nRECORD_SECONDS = 5\nWAVE_OUTPUT_FILENAME = WAVE_OUTPUT_FILENAME #\"output.wav\"\n# 初始化PyAudio\naudio = pyaudio.PyAudio()\n# 打开音频流\nstream = audio.open(format=FORMAT,\n                    channels=CHANNELS,",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "RATE",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "RATE = 16000 #44100\nCHUNK = 1024\nRECORD_SECONDS = 5\nWAVE_OUTPUT_FILENAME = WAVE_OUTPUT_FILENAME #\"output.wav\"\n# 初始化PyAudio\naudio = pyaudio.PyAudio()\n# 打开音频流\nstream = audio.open(format=FORMAT,\n                    channels=CHANNELS,\n                    rate=RATE,",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "CHUNK",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "CHUNK = 1024\nRECORD_SECONDS = 5\nWAVE_OUTPUT_FILENAME = WAVE_OUTPUT_FILENAME #\"output.wav\"\n# 初始化PyAudio\naudio = pyaudio.PyAudio()\n# 打开音频流\nstream = audio.open(format=FORMAT,\n                    channels=CHANNELS,\n                    rate=RATE,\n                    input=True,",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "RECORD_SECONDS",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "RECORD_SECONDS = 5\nWAVE_OUTPUT_FILENAME = WAVE_OUTPUT_FILENAME #\"output.wav\"\n# 初始化PyAudio\naudio = pyaudio.PyAudio()\n# 打开音频流\nstream = audio.open(format=FORMAT,\n                    channels=CHANNELS,\n                    rate=RATE,\n                    input=True,\n                    frames_per_buffer=CHUNK)",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "WAVE_OUTPUT_FILENAME",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "WAVE_OUTPUT_FILENAME = WAVE_OUTPUT_FILENAME #\"output.wav\"\n# 初始化PyAudio\naudio = pyaudio.PyAudio()\n# 打开音频流\nstream = audio.open(format=FORMAT,\n                    channels=CHANNELS,\n                    rate=RATE,\n                    input=True,\n                    frames_per_buffer=CHUNK)\nprint(\"录音开始...\")",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "audio",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "audio = pyaudio.PyAudio()\n# 打开音频流\nstream = audio.open(format=FORMAT,\n                    channels=CHANNELS,\n                    rate=RATE,\n                    input=True,\n                    frames_per_buffer=CHUNK)\nprint(\"录音开始...\")\n# 开始录音\nframes = []",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "stream",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "stream = audio.open(format=FORMAT,\n                    channels=CHANNELS,\n                    rate=RATE,\n                    input=True,\n                    frames_per_buffer=CHUNK)\nprint(\"录音开始...\")\n# 开始录音\nframes = []\nfor i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n    data = stream.read(CHUNK)",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "frames",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "frames = []\nfor i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n    data = stream.read(CHUNK)\n    frames.append(data)\nprint(\"录音结束...\")\n# 关闭音频流\nstream.stop_stream()\nstream.close()\naudio.terminate()\n# 将录音数据保存为.wav文件",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "waveFile",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "waveFile = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\nwaveFile.setnchannels(CHANNELS)\nwaveFile.setsampwidth(audio.get_sample_size(FORMAT))\nwaveFile.setframerate(RATE)\nwaveFile.writeframes(b''.join(frames))\nwaveFile.close()\n# 语音文件转写\ntext = wav_to_text(WAVE_OUTPUT_FILENAME)\n# 语言大模型生成\nresponse = client.chat.completions.create(",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "text = wav_to_text(WAVE_OUTPUT_FILENAME)\n# 语言大模型生成\nresponse = client.chat.completions.create(\n    model=\"glm-4\",  # 填写需要调用的模型名称\n    messages=[\n        {\"role\": \"user\", \"content\": f\"{text}\"}\n    ]\n)\nprint(response.choices[0].message.content)\nprint(response.usage.total_tokens)",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "response = client.chat.completions.create(\n    model=\"glm-4\",  # 填写需要调用的模型名称\n    messages=[\n        {\"role\": \"user\", \"content\": f\"{text}\"}\n    ]\n)\nprint(response.choices[0].message.content)\nprint(response.usage.total_tokens)\n# 语音合成\noutput = text_to_wav(response.choices[0].message.content)",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": "ollama.test03",
        "description": "ollama.test03",
        "peekOfCode": "output = text_to_wav(response.choices[0].message.content)\nprint(output)\n# 播放语音\n# output = trans_mp3_to_wav(output)\nfrom playsound import playsound\nplaysound(output)",
        "detail": "ollama.test03",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "ollama.test04",
        "description": "ollama.test04",
        "peekOfCode": "client = Groq(\n    # api_key=os.environ.get(\"GROQ_API_KEY\"),\n    api_key=\"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\",\n)\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Explain the importance of low latency LLMs\",\n        }",
        "detail": "ollama.test04",
        "documentation": {}
    },
    {
        "label": "chat_completion",
        "kind": 5,
        "importPath": "ollama.test04",
        "description": "ollama.test04",
        "peekOfCode": "chat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Explain the importance of low latency LLMs\",\n        }\n    ],\n    model=\"mixtral-8x7b-32768\",\n)\nprint(chat_completion.choices[0].message.content)",
        "detail": "ollama.test04",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "openai.openai_1_3_7_azure_Test",
        "description": "openai.openai_1_3_7_azure_Test",
        "peekOfCode": "client = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\",\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://glsk-openai-ce.openai.azure.com\",\n    api_key=\"8ba61036fe7847c7be4ebaaf58b85275\"\n)\ncompletion = client.chat.completions.create(\n    model=\"gpt-35-turbo-1106\",  # e.g. gpt-35-instant\n    messages=[",
        "detail": "openai.openai_1_3_7_azure_Test",
        "documentation": {}
    },
    {
        "label": "completion",
        "kind": 5,
        "importPath": "openai.openai_1_3_7_azure_Test",
        "description": "openai.openai_1_3_7_azure_Test",
        "peekOfCode": "completion = client.chat.completions.create(\n    model=\"gpt-35-turbo-1106\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"帮我生成2条续保营销文案，每条不超过12个字，输出json格式\",\n        },\n    ],\n)\nprint(completion.usage)",
        "detail": "openai.openai_1_3_7_azure_Test",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "openai.test01",
        "description": "openai.test01",
        "peekOfCode": "client = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\",\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://glsk-openai-ce.openai.azure.com\",\n    api_key=\"8ba61036fe7847c7be4ebaaf58b85275\",\n)\nprompt = \"写一个opencv代码，实现图片的旋转\"\ncompletion = client.chat.completions.create(\n    # model=\"gpt-35-turbo-1106\",  # e.g. gpt-35-instant",
        "detail": "openai.test01",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "openai.test01",
        "description": "openai.test01",
        "peekOfCode": "prompt = \"写一个opencv代码，实现图片的旋转\"\ncompletion = client.chat.completions.create(\n    # model=\"gpt-35-turbo-1106\",  # e.g. gpt-35-instant\n    model=\"gpt-4-1106\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": prompt,\n        },\n    ],",
        "detail": "openai.test01",
        "documentation": {}
    },
    {
        "label": "completion",
        "kind": 5,
        "importPath": "openai.test01",
        "description": "openai.test01",
        "peekOfCode": "completion = client.chat.completions.create(\n    # model=\"gpt-35-turbo-1106\",  # e.g. gpt-35-instant\n    model=\"gpt-4-1106\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": prompt,\n        },\n    ],\n)",
        "detail": "openai.test01",
        "documentation": {}
    },
    {
        "label": "openai_api_key",
        "kind": 5,
        "importPath": "openai.test02",
        "description": "openai.test02",
        "peekOfCode": "openai_api_key = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\nopenai_api_base = \"https://oneapi.365jpshop.com/v1\"\n# Set up request parameters\nmodel_id = \"code-davinci-002\"\nprompt = \"Write a Python function to calculate the factorial of a number.\"\nmax_tokens = 100\ntemperature = 0.7\n# Construct request body\nheaders = {\n    \"Content-Type\": \"application/json\",",
        "detail": "openai.test02",
        "documentation": {}
    },
    {
        "label": "openai_api_base",
        "kind": 5,
        "importPath": "openai.test02",
        "description": "openai.test02",
        "peekOfCode": "openai_api_base = \"https://oneapi.365jpshop.com/v1\"\n# Set up request parameters\nmodel_id = \"code-davinci-002\"\nprompt = \"Write a Python function to calculate the factorial of a number.\"\nmax_tokens = 100\ntemperature = 0.7\n# Construct request body\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {openai_api_key}\",",
        "detail": "openai.test02",
        "documentation": {}
    },
    {
        "label": "model_id",
        "kind": 5,
        "importPath": "openai.test02",
        "description": "openai.test02",
        "peekOfCode": "model_id = \"code-davinci-002\"\nprompt = \"Write a Python function to calculate the factorial of a number.\"\nmax_tokens = 100\ntemperature = 0.7\n# Construct request body\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {openai_api_key}\",\n}\ndata = {",
        "detail": "openai.test02",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "openai.test02",
        "description": "openai.test02",
        "peekOfCode": "prompt = \"Write a Python function to calculate the factorial of a number.\"\nmax_tokens = 100\ntemperature = 0.7\n# Construct request body\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {openai_api_key}\",\n}\ndata = {\n    \"model\": model_id,",
        "detail": "openai.test02",
        "documentation": {}
    },
    {
        "label": "max_tokens",
        "kind": 5,
        "importPath": "openai.test02",
        "description": "openai.test02",
        "peekOfCode": "max_tokens = 100\ntemperature = 0.7\n# Construct request body\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {openai_api_key}\",\n}\ndata = {\n    \"model\": model_id,\n    \"prompt\": prompt,",
        "detail": "openai.test02",
        "documentation": {}
    },
    {
        "label": "temperature",
        "kind": 5,
        "importPath": "openai.test02",
        "description": "openai.test02",
        "peekOfCode": "temperature = 0.7\n# Construct request body\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {openai_api_key}\",\n}\ndata = {\n    \"model\": model_id,\n    \"prompt\": prompt,\n    \"max_tokens\": max_tokens,",
        "detail": "openai.test02",
        "documentation": {}
    },
    {
        "label": "headers",
        "kind": 5,
        "importPath": "openai.test02",
        "description": "openai.test02",
        "peekOfCode": "headers = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {openai_api_key}\",\n}\ndata = {\n    \"model\": model_id,\n    \"prompt\": prompt,\n    \"max_tokens\": max_tokens,\n    \"temperature\": temperature,\n}",
        "detail": "openai.test02",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "openai.test02",
        "description": "openai.test02",
        "peekOfCode": "data = {\n    \"model\": model_id,\n    \"prompt\": prompt,\n    \"max_tokens\": max_tokens,\n    \"temperature\": temperature,\n}\n# Send request and handle response\nresponse = requests.post(openai_api_base + \"completions\", headers=headers, json=data)\nif response.status_code == 200:\n    result = response.json()",
        "detail": "openai.test02",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "openai.test02",
        "description": "openai.test02",
        "peekOfCode": "response = requests.post(openai_api_base + \"completions\", headers=headers, json=data)\nif response.status_code == 200:\n    result = response.json()\n    print(result[\"choices\"][0][\"text\"])\nelse:\n    print(f\"Request failed with status code {response.status_code}\")",
        "detail": "openai.test02",
        "documentation": {}
    },
    {
        "label": "tsc",
        "kind": 2,
        "importPath": "openai.test03",
        "description": "openai.test03",
        "peekOfCode": "def tsc(inp: str, nn: int) -> str:\n    # 这是基线条件，当nn等于0时，函数返回my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1\n        text = (\n            \"列举机器学习算法中10个关于\"\n            + inp\n            + \"的学术词，注意只输出十个学术词，用逗号隔开，不附带其他任何信息。必须只是中文。\"",
        "detail": "openai.test03",
        "documentation": {}
    },
    {
        "label": "tzh",
        "kind": 2,
        "importPath": "openai.test03",
        "description": "openai.test03",
        "peekOfCode": "def tzh():\n    text = \"列举20个机器学习算法中的学术词，注意只输出二十个学术词，用中文逗号隔开，不附带其他任何信息。必须只是中文。\"\n    response = llm.invoke(input=text)\n    array = np.array(response.content.split(\",\"))\n    print(array)\n    for key in array:\n        arr = tsc(key, 10)\n        arr = np.array(arr.split(\",\"))\n        global my_array\n        my_array = np.append(my_array, arr)",
        "detail": "openai.test03",
        "documentation": {}
    },
    {
        "label": "jiesh",
        "kind": 2,
        "importPath": "openai.test03",
        "description": "openai.test03",
        "peekOfCode": "def jiesh():\n    arr = tzh()\n    #     arr = np.array(arr.split(','))\n    for a in arr:\n        text = \"详细解释一下，什么是\" + a + \"。回复字数在200字左右，回复必须只是中文。\"\n        # llm = OpenAIChat(temperature=0.9, model_name=model)\n        llm = ChatOpenAI(temperature=0.9, model_name=model)\n        response = llm.invoke(input=text)\n        response_content = a + \"\\n\" + response.content\n        print(response_content)",
        "detail": "openai.test03",
        "documentation": {}
    },
    {
        "label": "openai.api_base",
        "kind": 5,
        "importPath": "openai.test03",
        "description": "openai.test03",
        "peekOfCode": "openai.api_base = \"https://oneapi.365jpshop.com/v1\"\n# from config import *\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# os.environ[\"OPENAI_API_KEY\"] = \"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\"#直连的key\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nimport time\n# model = \"chatglm3\"\n# model = \"gemini-pro\"\nfrom datetime import datetime",
        "detail": "openai.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "openai.test03",
        "description": "openai.test03",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# os.environ[\"OPENAI_API_KEY\"] = \"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\"#直连的key\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nimport time\n# model = \"chatglm3\"\n# model = \"gemini-pro\"\nfrom datetime import datetime\nimport numpy as np\n# from langchain.llms import OpenAI\n# from langchain_community.llms import OpenAIChat",
        "detail": "openai.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "openai.test03",
        "description": "openai.test03",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nimport time\n# model = \"chatglm3\"\n# model = \"gemini-pro\"\nfrom datetime import datetime\nimport numpy as np\n# from langchain.llms import OpenAI\n# from langchain_community.llms import OpenAIChat\nfrom langchain_openai import ChatOpenAI\n# from langchain_community.chat_models import ChatOpenAI",
        "detail": "openai.test03",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "openai.test03",
        "description": "openai.test03",
        "peekOfCode": "model = \"gpt-4-1106-preview\"\n# llm = OpenAIChat(temperature=0.9, model_name=model)\nllm = ChatOpenAI(\n    temperature=0.9,\n    model_name=model,\n    api_key=\"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\",\n    base_url=\"https://oneapi.365jpshop.com/v1\",\n)\n# client = OpenAIChat()\n# llm = OpenAI(temperature=0.9,max_tokens=1000)",
        "detail": "openai.test03",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "openai.test03",
        "description": "openai.test03",
        "peekOfCode": "llm = ChatOpenAI(\n    temperature=0.9,\n    model_name=model,\n    api_key=\"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\",\n    base_url=\"https://oneapi.365jpshop.com/v1\",\n)\n# client = OpenAIChat()\n# llm = OpenAI(temperature=0.9,max_tokens=1000)\nnn = 5\nmy_array = np.array([])",
        "detail": "openai.test03",
        "documentation": {}
    },
    {
        "label": "nn",
        "kind": 5,
        "importPath": "openai.test03",
        "description": "openai.test03",
        "peekOfCode": "nn = 5\nmy_array = np.array([])\njiesh_array = np.array([])\n# 这个函数需要一个额外的参数nn，因为在递归的环境中，\n# 我们无法修改外部作用域的变量。\ndef tsc(inp: str, nn: int) -> str:\n    # 这是基线条件，当nn等于0时，函数返回my_array\n    if nn == 0:\n        return my_array\n    else:",
        "detail": "openai.test03",
        "documentation": {}
    },
    {
        "label": "my_array",
        "kind": 5,
        "importPath": "openai.test03",
        "description": "openai.test03",
        "peekOfCode": "my_array = np.array([])\njiesh_array = np.array([])\n# 这个函数需要一个额外的参数nn，因为在递归的环境中，\n# 我们无法修改外部作用域的变量。\ndef tsc(inp: str, nn: int) -> str:\n    # 这是基线条件，当nn等于0时，函数返回my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1",
        "detail": "openai.test03",
        "documentation": {}
    },
    {
        "label": "jiesh_array",
        "kind": 5,
        "importPath": "openai.test03",
        "description": "openai.test03",
        "peekOfCode": "jiesh_array = np.array([])\n# 这个函数需要一个额外的参数nn，因为在递归的环境中，\n# 我们无法修改外部作用域的变量。\ndef tsc(inp: str, nn: int) -> str:\n    # 这是基线条件，当nn等于0时，函数返回my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1\n        text = (",
        "detail": "openai.test03",
        "documentation": {}
    },
    {
        "label": "chat",
        "kind": 5,
        "importPath": "openai.test04",
        "description": "openai.test04",
        "peekOfCode": "chat = ChatOpenAI(\n    temperature=0.9,\n    model_name=\"gpt-4-1106-preview\",\n    api_key=\"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\",\n    base_url=\"https://oneapi.365jpshop.com/v1\",\n)\n# Use the chat object to generate a response to a prompt\n# response = chat.invoke(input=\"帮我生成2条续保营销文案，每条不超过12个字，输出json格式\")\nresponse = chat.invoke(\n    input=\"列表一个商城项目的所有功能点,实现方案,用中文逗号隔开,附带主要技术实现方案，不附带其他任何信息。必须只是中文。\"",
        "detail": "openai.test04",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "openai.test04",
        "description": "openai.test04",
        "peekOfCode": "response = chat.invoke(\n    input=\"列表一个商城项目的所有功能点,实现方案,用中文逗号隔开,附带主要技术实现方案，不附带其他任何信息。必须只是中文。\"\n    # input=\"给我一首歌的时间\"\n    # input=\"帮我生成2条续保营销文案，每条不超过12个字，输出json格式\"\n)\nprint(response.content)",
        "detail": "openai.test04",
        "documentation": {}
    },
    {
        "label": "tsc",
        "kind": 2,
        "importPath": "openai.test05",
        "description": "openai.test05",
        "peekOfCode": "def tsc(inp: str, nn: int) -> str:\n    # 这是基线条件，当nn等于0时，函数返回my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1\n        text = (\n            \"列举机器学习算法中10个关于\"\n            + inp\n            + \"的学术词，注意只输出十个学术词，用逗号隔开，不附带其他任何信息。必须只是中文。\"",
        "detail": "openai.test05",
        "documentation": {}
    },
    {
        "label": "tzh",
        "kind": 2,
        "importPath": "openai.test05",
        "description": "openai.test05",
        "peekOfCode": "def tzh():\n    # text = \"列举20个机器学习算法中的学术词，注意只输出二十个学术词，用中文逗号隔开，不附带其他任何信息。必须只是中文。\"\n    text = \"列表一个商城项目的所有功能点,实现方案,用中文逗号隔开,附带主要技术实现方案，不附带其他任何信息。必须只是中文。\"\n    response = llm.invoke(input=text)\n    array = np.array(response.content.split(\",\"))\n    print(array)\n    for key in array:\n        arr = tsc(key, 10)\n        arr = np.array(arr.split(\",\"))\n        global my_array",
        "detail": "openai.test05",
        "documentation": {}
    },
    {
        "label": "jiesh",
        "kind": 2,
        "importPath": "openai.test05",
        "description": "openai.test05",
        "peekOfCode": "def jiesh():\n    arr = tzh()\n    #     arr = np.array(arr.split(','))\n    for a in arr:\n        text = \"详细解释一下，什么是\" + a + \"。回复字数在50字左右，回复必须只是中文。\"\n        # llm = OpenAIChat(temperature=0.9, model_name=model)\n        llm = ChatOpenAI(temperature=0.9, model_name=model)\n        response = llm.invoke(input=text)\n        response_content = a + \"\\n\" + response.content\n        print(response_content)",
        "detail": "openai.test05",
        "documentation": {}
    },
    {
        "label": "openai.api_base",
        "kind": 5,
        "importPath": "openai.test05",
        "description": "openai.test05",
        "peekOfCode": "openai.api_base = \"https://oneapi.365jpshop.com/v1\"\n# from config import *\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# os.environ[\"OPENAI_API_KEY\"] = \"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\"#直连的key\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nimport time\n# model = \"chatglm3\"\n# model = \"gemini-pro\"\nfrom datetime import datetime",
        "detail": "openai.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "openai.test05",
        "description": "openai.test05",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# os.environ[\"OPENAI_API_KEY\"] = \"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\"#直连的key\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nimport time\n# model = \"chatglm3\"\n# model = \"gemini-pro\"\nfrom datetime import datetime\nimport numpy as np\n# from langchain.llms import OpenAI\n# from langchain_community.llms import OpenAIChat",
        "detail": "openai.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "openai.test05",
        "description": "openai.test05",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nimport time\n# model = \"chatglm3\"\n# model = \"gemini-pro\"\nfrom datetime import datetime\nimport numpy as np\n# from langchain.llms import OpenAI\n# from langchain_community.llms import OpenAIChat\nfrom langchain_openai import ChatOpenAI\n# from langchain_community.chat_models import ChatOpenAI",
        "detail": "openai.test05",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "openai.test05",
        "description": "openai.test05",
        "peekOfCode": "model = \"gpt-4-1106-preview\"\n# llm = OpenAIChat(temperature=0.9, model_name=model)\nllm = ChatOpenAI(\n    temperature=0.9,\n    model_name=model,\n    api_key=\"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\",\n    base_url=\"https://oneapi.365jpshop.com/v1\",\n)\n# client = OpenAIChat()\n# llm = OpenAI(temperature=0.9,max_tokens=1000)",
        "detail": "openai.test05",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "openai.test05",
        "description": "openai.test05",
        "peekOfCode": "llm = ChatOpenAI(\n    temperature=0.9,\n    model_name=model,\n    api_key=\"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\",\n    base_url=\"https://oneapi.365jpshop.com/v1\",\n)\n# client = OpenAIChat()\n# llm = OpenAI(temperature=0.9,max_tokens=1000)\nnn = 5\nmy_array = np.array([])",
        "detail": "openai.test05",
        "documentation": {}
    },
    {
        "label": "nn",
        "kind": 5,
        "importPath": "openai.test05",
        "description": "openai.test05",
        "peekOfCode": "nn = 5\nmy_array = np.array([])\njiesh_array = np.array([])\n# 这个函数需要一个额外的参数nn，因为在递归的环境中，\n# 我们无法修改外部作用域的变量。\ndef tsc(inp: str, nn: int) -> str:\n    # 这是基线条件，当nn等于0时，函数返回my_array\n    if nn == 0:\n        return my_array\n    else:",
        "detail": "openai.test05",
        "documentation": {}
    },
    {
        "label": "my_array",
        "kind": 5,
        "importPath": "openai.test05",
        "description": "openai.test05",
        "peekOfCode": "my_array = np.array([])\njiesh_array = np.array([])\n# 这个函数需要一个额外的参数nn，因为在递归的环境中，\n# 我们无法修改外部作用域的变量。\ndef tsc(inp: str, nn: int) -> str:\n    # 这是基线条件，当nn等于0时，函数返回my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1",
        "detail": "openai.test05",
        "documentation": {}
    },
    {
        "label": "jiesh_array",
        "kind": 5,
        "importPath": "openai.test05",
        "description": "openai.test05",
        "peekOfCode": "jiesh_array = np.array([])\n# 这个函数需要一个额外的参数nn，因为在递归的环境中，\n# 我们无法修改外部作用域的变量。\ndef tsc(inp: str, nn: int) -> str:\n    # 这是基线条件，当nn等于0时，函数返回my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1\n        text = (",
        "detail": "openai.test05",
        "documentation": {}
    },
    {
        "label": "tsc",
        "kind": 2,
        "importPath": "openai.zhishi",
        "description": "openai.zhishi",
        "peekOfCode": "def tsc(inp: str, nn: int) -> str:\n    # 这是基线条件，当nn等于0时，函数返回my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1\n        text = (\n            \"列举机器学习算法中10个关于\"\n            + inp\n            + \"的学术词，注意只输出十个学术词，用逗号隔开，不附带其他任何信息。必须只是中文。\"",
        "detail": "openai.zhishi",
        "documentation": {}
    },
    {
        "label": "tzh",
        "kind": 2,
        "importPath": "openai.zhishi",
        "description": "openai.zhishi",
        "peekOfCode": "def tzh():\n    text = \"列举20个机器学习算法中的学术词，注意只输出二十个学术词，用中文逗号隔开，不附带其他任何信息。必须只是中文。\"\n    response = llm(text)\n    array = np.array(response.split(\",\"))\n    print(array)\n    for key in array:\n        arr = tsc(key, 10)\n        arr = np.array(arr.split(\",\"))\n        global my_array\n        my_array = np.append(my_array, arr)",
        "detail": "openai.zhishi",
        "documentation": {}
    },
    {
        "label": "jiesh",
        "kind": 2,
        "importPath": "openai.zhishi",
        "description": "openai.zhishi",
        "peekOfCode": "def jiesh():\n    arr = tzh()\n    #     arr = np.array(arr.split(','))\n    for a in arr:\n        text = \"详细解释一下，什么是\" + a + \"。回复字数在200字左右，回复必须只是中文。\"\n        llm = OpenAIChat(temperature=0.9, model_name=model)\n        response = llm(text)\n        #         time.sleep(5)\n        con = response\n        response = a + \"\\n\" + response",
        "detail": "openai.zhishi",
        "documentation": {}
    },
    {
        "label": "openai.api_base",
        "kind": 5,
        "importPath": "openai.zhishi",
        "description": "openai.zhishi",
        "peekOfCode": "openai.api_base = \"https://oneapi.365jpshop.com/v1\"\n# from config import *\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# os.environ[\"OPENAI_API_KEY\"] = \"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\"#直连的key\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nimport time\n# model = \"chatglm3\"\n# model = \"gemini-pro\"\n# import mysql.connector",
        "detail": "openai.zhishi",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "openai.zhishi",
        "description": "openai.zhishi",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# os.environ[\"OPENAI_API_KEY\"] = \"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\"#直连的key\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nimport time\n# model = \"chatglm3\"\n# model = \"gemini-pro\"\n# import mysql.connector\nfrom datetime import datetime\nimport numpy as np\n# from langchain.llms import OpenAI",
        "detail": "openai.zhishi",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "openai.zhishi",
        "description": "openai.zhishi",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nimport time\n# model = \"chatglm3\"\n# model = \"gemini-pro\"\n# import mysql.connector\nfrom datetime import datetime\nimport numpy as np\n# from langchain.llms import OpenAI\nfrom langchain_community.llms import OpenAIChat\n# from langchain_community.chat_models import ChatOpenAI",
        "detail": "openai.zhishi",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "openai.zhishi",
        "description": "openai.zhishi",
        "peekOfCode": "model = \"gpt-4-1106-preview\"\nllm = OpenAIChat(temperature=0.9, model_name=model)\n# client = OpenAIChat()\n# llm = OpenAI(temperature=0.9,max_tokens=1000)\nnn = 5\nmy_array = np.array([])\njiesh_array = np.array([])\n# 这个函数需要一个额外的参数nn，因为在递归的环境中，\n# 我们无法修改外部作用域的变量。\ndef tsc(inp: str, nn: int) -> str:",
        "detail": "openai.zhishi",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "openai.zhishi",
        "description": "openai.zhishi",
        "peekOfCode": "llm = OpenAIChat(temperature=0.9, model_name=model)\n# client = OpenAIChat()\n# llm = OpenAI(temperature=0.9,max_tokens=1000)\nnn = 5\nmy_array = np.array([])\njiesh_array = np.array([])\n# 这个函数需要一个额外的参数nn，因为在递归的环境中，\n# 我们无法修改外部作用域的变量。\ndef tsc(inp: str, nn: int) -> str:\n    # 这是基线条件，当nn等于0时，函数返回my_array",
        "detail": "openai.zhishi",
        "documentation": {}
    },
    {
        "label": "nn",
        "kind": 5,
        "importPath": "openai.zhishi",
        "description": "openai.zhishi",
        "peekOfCode": "nn = 5\nmy_array = np.array([])\njiesh_array = np.array([])\n# 这个函数需要一个额外的参数nn，因为在递归的环境中，\n# 我们无法修改外部作用域的变量。\ndef tsc(inp: str, nn: int) -> str:\n    # 这是基线条件，当nn等于0时，函数返回my_array\n    if nn == 0:\n        return my_array\n    else:",
        "detail": "openai.zhishi",
        "documentation": {}
    },
    {
        "label": "my_array",
        "kind": 5,
        "importPath": "openai.zhishi",
        "description": "openai.zhishi",
        "peekOfCode": "my_array = np.array([])\njiesh_array = np.array([])\n# 这个函数需要一个额外的参数nn，因为在递归的环境中，\n# 我们无法修改外部作用域的变量。\ndef tsc(inp: str, nn: int) -> str:\n    # 这是基线条件，当nn等于0时，函数返回my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1",
        "detail": "openai.zhishi",
        "documentation": {}
    },
    {
        "label": "jiesh_array",
        "kind": 5,
        "importPath": "openai.zhishi",
        "description": "openai.zhishi",
        "peekOfCode": "jiesh_array = np.array([])\n# 这个函数需要一个额外的参数nn，因为在递归的环境中，\n# 我们无法修改外部作用域的变量。\ndef tsc(inp: str, nn: int) -> str:\n    # 这是基线条件，当nn等于0时，函数返回my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1\n        text = (",
        "detail": "openai.zhishi",
        "documentation": {}
    },
    {
        "label": "audio",
        "kind": 5,
        "importPath": "paddlespeech.test01",
        "description": "paddlespeech.test01",
        "peekOfCode": "audio = \"zh.wav\"\nasr = ASRExecutor()\nresult = asr(audio_file=audio, model=\"conformer_online_wenetspeech\")\nprint(result)",
        "detail": "paddlespeech.test01",
        "documentation": {}
    },
    {
        "label": "asr",
        "kind": 5,
        "importPath": "paddlespeech.test01",
        "description": "paddlespeech.test01",
        "peekOfCode": "asr = ASRExecutor()\nresult = asr(audio_file=audio, model=\"conformer_online_wenetspeech\")\nprint(result)",
        "detail": "paddlespeech.test01",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "paddlespeech.test01",
        "description": "paddlespeech.test01",
        "peekOfCode": "result = asr(audio_file=audio, model=\"conformer_online_wenetspeech\")\nprint(result)",
        "detail": "paddlespeech.test01",
        "documentation": {}
    },
    {
        "label": "tts_executor",
        "kind": 5,
        "importPath": "paddlespeech.test02",
        "description": "paddlespeech.test02",
        "peekOfCode": "tts_executor = TTSExecutor()\ntts_executor(text=\"你好，世界！\", output=\"output.wav\")\nfrom paddlespeech.cli.audio import AudioExecutor\naudio_executor = AudioExecutor()\naudio_executor(input=\"output.wav\", output=\"output01.wav\", effect=\"pitch\")",
        "detail": "paddlespeech.test02",
        "documentation": {}
    },
    {
        "label": "audio_executor",
        "kind": 5,
        "importPath": "paddlespeech.test02",
        "description": "paddlespeech.test02",
        "peekOfCode": "audio_executor = AudioExecutor()\naudio_executor(input=\"output.wav\", output=\"output01.wav\", effect=\"pitch\")",
        "detail": "paddlespeech.test02",
        "documentation": {}
    },
    {
        "label": "play_audio",
        "kind": 2,
        "importPath": "pyaudio.test01",
        "description": "pyaudio.test01",
        "peekOfCode": "def play_audio(wave_path):\n    CHUNK = 1024\n    wf = wave.open(wave_path, 'rb')\n    # instantiate PyAudio (1)\n    p = pyaudio.PyAudio()\n    # open stream (2)\n    stream = p.open(format=p.get_format_from_width(wf.getsampwidth()),\n                    channels=wf.getnchannels(),\n                    rate=wf.getframerate(),\n                    output=True)",
        "detail": "pyaudio.test01",
        "documentation": {}
    },
    {
        "label": "play",
        "kind": 2,
        "importPath": "pyaudio.test02",
        "description": "pyaudio.test02",
        "peekOfCode": "def play():\n    chunk = 1024\n    wf = wave.open(r\"测试音频.wav\", 'rb')\n    p = pyaudio.PyAudio()\n    stream = p.open(format=p.get_format_from_width(wf.getsampwidth()), channels=wf.getnchannels(),\n                    rate=wf.getframerate(), output=True)\n    data = wf.readframes(chunk)  # 读取数据\n    print(data)\n    while data != b'':  # 播放\n        stream.write(data)",
        "detail": "pyaudio.test02",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 2,
        "importPath": "suanfa.main01",
        "description": "suanfa.main01",
        "peekOfCode": "def name(args):\n    print(\"Hello World\" + args)\ndef bubble_sort(arr):\n    n = len(arr)\n    for i in range(n - 1):\n        # 每次遍历将最大的元素冒泡到末尾\n        for j in range(n - 1 - i):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\ndef my_bubble_sort(arr):",
        "detail": "suanfa.main01",
        "documentation": {}
    },
    {
        "label": "bubble_sort",
        "kind": 2,
        "importPath": "suanfa.main01",
        "description": "suanfa.main01",
        "peekOfCode": "def bubble_sort(arr):\n    n = len(arr)\n    for i in range(n - 1):\n        # 每次遍历将最大的元素冒泡到末尾\n        for j in range(n - 1 - i):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\ndef my_bubble_sort(arr):\n    n = len(arr)\n    for i in range(n - 1):",
        "detail": "suanfa.main01",
        "documentation": {}
    },
    {
        "label": "my_bubble_sort",
        "kind": 2,
        "importPath": "suanfa.main01",
        "description": "suanfa.main01",
        "peekOfCode": "def my_bubble_sort(arr):\n    n = len(arr)\n    for i in range(n - 1):\n        for j in range(n - 1 - i):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\ndef insertion_sort(arr):\n    n = len(arr)\n    for i in range(1, n):\n        key = arr[i]",
        "detail": "suanfa.main01",
        "documentation": {}
    },
    {
        "label": "insertion_sort",
        "kind": 2,
        "importPath": "suanfa.main01",
        "description": "suanfa.main01",
        "peekOfCode": "def insertion_sort(arr):\n    n = len(arr)\n    for i in range(1, n):\n        key = arr[i]\n        j = i - 1\n        # 在已排序部分找到合适的插入位置\n        while j >= 0 and arr[j] > key:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key",
        "detail": "suanfa.main01",
        "documentation": {}
    },
    {
        "label": "reader",
        "kind": 5,
        "importPath": "suanfa.mian02",
        "description": "suanfa.mian02",
        "peekOfCode": "reader = easyocr.Reader(\n    [\"en\", \"ch_sim\"],\n    gpu=True,\n)  # this needs to run only once to load the model into memory\n# 图像路径\nimage_path = \"./15889317528424945.png\"\nimage_path_out = \"./results/15889317528424945.png\"\nimage = Image.open(image_path)\n# image = image.resize((800, 600))\n# image = image.resize((800, 300))",
        "detail": "suanfa.mian02",
        "documentation": {}
    },
    {
        "label": "image_path",
        "kind": 5,
        "importPath": "suanfa.mian02",
        "description": "suanfa.mian02",
        "peekOfCode": "image_path = \"./15889317528424945.png\"\nimage_path_out = \"./results/15889317528424945.png\"\nimage = Image.open(image_path)\n# image = image.resize((800, 600))\n# image = image.resize((800, 300))\n# image.show()\nimage.save(image_path_out)\n# result = reader.readtext(\"./image/baidu_image/test5.jpg\", detail=0)\nresults = reader.readtext(image_path_out, detail=0, batch_size=20, paragraph=True)\nprint(\"results==\", results)",
        "detail": "suanfa.mian02",
        "documentation": {}
    },
    {
        "label": "image_path_out",
        "kind": 5,
        "importPath": "suanfa.mian02",
        "description": "suanfa.mian02",
        "peekOfCode": "image_path_out = \"./results/15889317528424945.png\"\nimage = Image.open(image_path)\n# image = image.resize((800, 600))\n# image = image.resize((800, 300))\n# image.show()\nimage.save(image_path_out)\n# result = reader.readtext(\"./image/baidu_image/test5.jpg\", detail=0)\nresults = reader.readtext(image_path_out, detail=0, batch_size=20, paragraph=True)\nprint(\"results==\", results)\nfor result in results:",
        "detail": "suanfa.mian02",
        "documentation": {}
    },
    {
        "label": "image",
        "kind": 5,
        "importPath": "suanfa.mian02",
        "description": "suanfa.mian02",
        "peekOfCode": "image = Image.open(image_path)\n# image = image.resize((800, 600))\n# image = image.resize((800, 300))\n# image.show()\nimage.save(image_path_out)\n# result = reader.readtext(\"./image/baidu_image/test5.jpg\", detail=0)\nresults = reader.readtext(image_path_out, detail=0, batch_size=20, paragraph=True)\nprint(\"results==\", results)\nfor result in results:\n    print(\"result\", result)",
        "detail": "suanfa.mian02",
        "documentation": {}
    },
    {
        "label": "results",
        "kind": 5,
        "importPath": "suanfa.mian02",
        "description": "suanfa.mian02",
        "peekOfCode": "results = reader.readtext(image_path_out, detail=0, batch_size=20, paragraph=True)\nprint(\"results==\", results)\nfor result in results:\n    print(\"result\", result)",
        "detail": "suanfa.mian02",
        "documentation": {}
    },
    {
        "label": "ocr",
        "kind": 5,
        "importPath": "suanfa.mian03",
        "description": "suanfa.mian03",
        "peekOfCode": "ocr = PaddleOCR(\n    lang=\"ch\",\n    # use_gpu=False,\n    use_gpu=True,\n    det_model_dir=\"../paddleORC_model/ch_ppocr_server_v2.0_det_infer/\",\n    cls_model_dir=\"ch_ppocr_mobile_v2.0_cls_infer/\",\n    rec_model_dir=\"ch_ppocr_server_v2.0_rec_infer/\",\n)\n# load dataset\n# img_path = \"./08631508_2.jpg\"",
        "detail": "suanfa.mian03",
        "documentation": {}
    },
    {
        "label": "img_path",
        "kind": 5,
        "importPath": "suanfa.mian03",
        "description": "suanfa.mian03",
        "peekOfCode": "img_path = \"./image/baidu_image/test7.jpg\"\nresult = ocr.ocr(img_path, cls=True)\nfor line in result:\n    print(line)\n    print(line[0][1][0])\n# 注：\n# result是一个list，每个item包含了文本框，文字和识别置信度\n# line的格式为：\n# [[[3.0, 149.0], [43.0, 149.0], [43.0, 163.0], [3.0, 163.0]], ('人心安', 0.6762619018554688)]\n# 文字框 boxes = line[0]，包含文字框的四个角的(x,y)坐标",
        "detail": "suanfa.mian03",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "suanfa.mian03",
        "description": "suanfa.mian03",
        "peekOfCode": "result = ocr.ocr(img_path, cls=True)\nfor line in result:\n    print(line)\n    print(line[0][1][0])\n# 注：\n# result是一个list，每个item包含了文本框，文字和识别置信度\n# line的格式为：\n# [[[3.0, 149.0], [43.0, 149.0], [43.0, 163.0], [3.0, 163.0]], ('人心安', 0.6762619018554688)]\n# 文字框 boxes = line[0]，包含文字框的四个角的(x,y)坐标\n# 文字 txts = line[1][0]",
        "detail": "suanfa.mian03",
        "documentation": {}
    },
    {
        "label": "processor",
        "kind": 5,
        "importPath": "wav2vec.test01",
        "description": "wav2vec.test01",
        "peekOfCode": "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")   # 用于ASR等，32维\naudio_input, sample_rate = sf.read(path_audio)  # (31129,)\ninput_values = processor(audio_input, sampling_rate=sample_rate, return_tensors=\"pt\").input_values  # torch.Size([1, 31129])\nlogits = model(input_values).logits     # torch.Size([1, 97, 32])\npredicted_ids = torch.argmax(logits, dim=-1)    # torch.Size([1, 97])\ntranscription = processor.decode(predicted_ids[0])  # ASR的解码结果\nfrom transformers import Wav2Vec2Model\nmodel = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")    # 用于提取通用特征，768维\nwav2vec2 = model(input_values)['last_hidden_state']     # torch.Size([1, 97, 768])，模型出来是一个BaseModelOutput的结构体。",
        "detail": "wav2vec.test01",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "wav2vec.test01",
        "description": "wav2vec.test01",
        "peekOfCode": "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")   # 用于ASR等，32维\naudio_input, sample_rate = sf.read(path_audio)  # (31129,)\ninput_values = processor(audio_input, sampling_rate=sample_rate, return_tensors=\"pt\").input_values  # torch.Size([1, 31129])\nlogits = model(input_values).logits     # torch.Size([1, 97, 32])\npredicted_ids = torch.argmax(logits, dim=-1)    # torch.Size([1, 97])\ntranscription = processor.decode(predicted_ids[0])  # ASR的解码结果\nfrom transformers import Wav2Vec2Model\nmodel = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")    # 用于提取通用特征，768维\nwav2vec2 = model(input_values)['last_hidden_state']     # torch.Size([1, 97, 768])，模型出来是一个BaseModelOutput的结构体。",
        "detail": "wav2vec.test01",
        "documentation": {}
    },
    {
        "label": "input_values",
        "kind": 5,
        "importPath": "wav2vec.test01",
        "description": "wav2vec.test01",
        "peekOfCode": "input_values = processor(audio_input, sampling_rate=sample_rate, return_tensors=\"pt\").input_values  # torch.Size([1, 31129])\nlogits = model(input_values).logits     # torch.Size([1, 97, 32])\npredicted_ids = torch.argmax(logits, dim=-1)    # torch.Size([1, 97])\ntranscription = processor.decode(predicted_ids[0])  # ASR的解码结果\nfrom transformers import Wav2Vec2Model\nmodel = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")    # 用于提取通用特征，768维\nwav2vec2 = model(input_values)['last_hidden_state']     # torch.Size([1, 97, 768])，模型出来是一个BaseModelOutput的结构体。",
        "detail": "wav2vec.test01",
        "documentation": {}
    },
    {
        "label": "logits",
        "kind": 5,
        "importPath": "wav2vec.test01",
        "description": "wav2vec.test01",
        "peekOfCode": "logits = model(input_values).logits     # torch.Size([1, 97, 32])\npredicted_ids = torch.argmax(logits, dim=-1)    # torch.Size([1, 97])\ntranscription = processor.decode(predicted_ids[0])  # ASR的解码结果\nfrom transformers import Wav2Vec2Model\nmodel = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")    # 用于提取通用特征，768维\nwav2vec2 = model(input_values)['last_hidden_state']     # torch.Size([1, 97, 768])，模型出来是一个BaseModelOutput的结构体。",
        "detail": "wav2vec.test01",
        "documentation": {}
    },
    {
        "label": "predicted_ids",
        "kind": 5,
        "importPath": "wav2vec.test01",
        "description": "wav2vec.test01",
        "peekOfCode": "predicted_ids = torch.argmax(logits, dim=-1)    # torch.Size([1, 97])\ntranscription = processor.decode(predicted_ids[0])  # ASR的解码结果\nfrom transformers import Wav2Vec2Model\nmodel = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")    # 用于提取通用特征，768维\nwav2vec2 = model(input_values)['last_hidden_state']     # torch.Size([1, 97, 768])，模型出来是一个BaseModelOutput的结构体。",
        "detail": "wav2vec.test01",
        "documentation": {}
    },
    {
        "label": "transcription",
        "kind": 5,
        "importPath": "wav2vec.test01",
        "description": "wav2vec.test01",
        "peekOfCode": "transcription = processor.decode(predicted_ids[0])  # ASR的解码结果\nfrom transformers import Wav2Vec2Model\nmodel = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")    # 用于提取通用特征，768维\nwav2vec2 = model(input_values)['last_hidden_state']     # torch.Size([1, 97, 768])，模型出来是一个BaseModelOutput的结构体。",
        "detail": "wav2vec.test01",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "wav2vec.test01",
        "description": "wav2vec.test01",
        "peekOfCode": "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")    # 用于提取通用特征，768维\nwav2vec2 = model(input_values)['last_hidden_state']     # torch.Size([1, 97, 768])，模型出来是一个BaseModelOutput的结构体。",
        "detail": "wav2vec.test01",
        "documentation": {}
    },
    {
        "label": "wav2vec2",
        "kind": 5,
        "importPath": "wav2vec.test01",
        "description": "wav2vec.test01",
        "peekOfCode": "wav2vec2 = model(input_values)['last_hidden_state']     # torch.Size([1, 97, 768])，模型出来是一个BaseModelOutput的结构体。",
        "detail": "wav2vec.test01",
        "documentation": {}
    },
    {
        "label": "update",
        "kind": 2,
        "importPath": "wav2vec.test02",
        "description": "wav2vec.test02",
        "peekOfCode": "def update():\n    global X, Y, W, lr, n\n    n = n + 1\n    O = np.dot(X, W.T)\n    O2 = np.dot(X, W.T)\n    # Q=np.array([O1,O2]);\n    # Q=np.dot(Q,W.T);\n    # 计算权值差\n    W_Tmp = lr * ((Y - O.T).dot(X))\n    W = W + W_Tmp",
        "detail": "wav2vec.test02",
        "documentation": {}
    },
    {
        "label": "X",
        "kind": 5,
        "importPath": "wav2vec.test02",
        "description": "wav2vec.test02",
        "peekOfCode": "X = np.array([[8, 6, 8], [5, 8, 8], [1, 2, 2], [2, 2, 4], [6, 6, 8], [7, 6, 8]])\nY = np.array([15, 13, 3, 5, 13, 14])\n# W1 = (np.random.random(2)-0.5)*2;\nW = np.array([0, 0, 0])\nlr = 0.002\n# 计算迭代次数\nn = 0\n# #神经网络输出\nO1 = 0\nO2 = 0",
        "detail": "wav2vec.test02",
        "documentation": {}
    },
    {
        "label": "Y",
        "kind": 5,
        "importPath": "wav2vec.test02",
        "description": "wav2vec.test02",
        "peekOfCode": "Y = np.array([15, 13, 3, 5, 13, 14])\n# W1 = (np.random.random(2)-0.5)*2;\nW = np.array([0, 0, 0])\nlr = 0.002\n# 计算迭代次数\nn = 0\n# #神经网络输出\nO1 = 0\nO2 = 0\nQ = 0",
        "detail": "wav2vec.test02",
        "documentation": {}
    },
    {
        "label": "W",
        "kind": 5,
        "importPath": "wav2vec.test02",
        "description": "wav2vec.test02",
        "peekOfCode": "W = np.array([0, 0, 0])\nlr = 0.002\n# 计算迭代次数\nn = 0\n# #神经网络输出\nO1 = 0\nO2 = 0\nQ = 0\nn = 0\ndef update():",
        "detail": "wav2vec.test02",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "wav2vec.test02",
        "description": "wav2vec.test02",
        "peekOfCode": "lr = 0.002\n# 计算迭代次数\nn = 0\n# #神经网络输出\nO1 = 0\nO2 = 0\nQ = 0\nn = 0\ndef update():\n    global X, Y, W, lr, n",
        "detail": "wav2vec.test02",
        "documentation": {}
    },
    {
        "label": "n",
        "kind": 5,
        "importPath": "wav2vec.test02",
        "description": "wav2vec.test02",
        "peekOfCode": "n = 0\n# #神经网络输出\nO1 = 0\nO2 = 0\nQ = 0\nn = 0\ndef update():\n    global X, Y, W, lr, n\n    n = n + 1\n    O = np.dot(X, W.T)",
        "detail": "wav2vec.test02",
        "documentation": {}
    },
    {
        "label": "O1",
        "kind": 5,
        "importPath": "wav2vec.test02",
        "description": "wav2vec.test02",
        "peekOfCode": "O1 = 0\nO2 = 0\nQ = 0\nn = 0\ndef update():\n    global X, Y, W, lr, n\n    n = n + 1\n    O = np.dot(X, W.T)\n    O2 = np.dot(X, W.T)\n    # Q=np.array([O1,O2]);",
        "detail": "wav2vec.test02",
        "documentation": {}
    },
    {
        "label": "O2",
        "kind": 5,
        "importPath": "wav2vec.test02",
        "description": "wav2vec.test02",
        "peekOfCode": "O2 = 0\nQ = 0\nn = 0\ndef update():\n    global X, Y, W, lr, n\n    n = n + 1\n    O = np.dot(X, W.T)\n    O2 = np.dot(X, W.T)\n    # Q=np.array([O1,O2]);\n    # Q=np.dot(Q,W.T);",
        "detail": "wav2vec.test02",
        "documentation": {}
    },
    {
        "label": "Q",
        "kind": 5,
        "importPath": "wav2vec.test02",
        "description": "wav2vec.test02",
        "peekOfCode": "Q = 0\nn = 0\ndef update():\n    global X, Y, W, lr, n\n    n = n + 1\n    O = np.dot(X, W.T)\n    O2 = np.dot(X, W.T)\n    # Q=np.array([O1,O2]);\n    # Q=np.dot(Q,W.T);\n    # 计算权值差",
        "detail": "wav2vec.test02",
        "documentation": {}
    },
    {
        "label": "n",
        "kind": 5,
        "importPath": "wav2vec.test02",
        "description": "wav2vec.test02",
        "peekOfCode": "n = 0\ndef update():\n    global X, Y, W, lr, n\n    n = n + 1\n    O = np.dot(X, W.T)\n    O2 = np.dot(X, W.T)\n    # Q=np.array([O1,O2]);\n    # Q=np.dot(Q,W.T);\n    # 计算权值差\n    W_Tmp = lr * ((Y - O.T).dot(X))",
        "detail": "wav2vec.test02",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "model = models.resnet18(pretrained=True)\n# 修改模型的输出层\nnum_classes = 10  # 替换为目标任务的类别数\nnum_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features, num_classes)\n# 定义损失函数和优化器\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n# 加载数据集并进行训练\ntrain_loader = \"\"  # 替换为训练集的数据加载器",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "num_classes",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "num_classes = 10  # 替换为目标任务的类别数\nnum_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features, num_classes)\n# 定义损失函数和优化器\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n# 加载数据集并进行训练\ntrain_loader = \"\"  # 替换为训练集的数据加载器\nnum_epochs = 10  # 迭代次数\nfor epoch in range(num_epochs):",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "num_features",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "num_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features, num_classes)\n# 定义损失函数和优化器\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n# 加载数据集并进行训练\ntrain_loader = \"\"  # 替换为训练集的数据加载器\nnum_epochs = 10  # 迭代次数\nfor epoch in range(num_epochs):\n    for images, labels in train_loader:",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "model.fc",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "model.fc = nn.Linear(num_features, num_classes)\n# 定义损失函数和优化器\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n# 加载数据集并进行训练\ntrain_loader = \"\"  # 替换为训练集的数据加载器\nnum_epochs = 10  # 迭代次数\nfor epoch in range(num_epochs):\n    for images, labels in train_loader:\n        optimizer.zero_grad()",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "criterion",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "criterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n# 加载数据集并进行训练\ntrain_loader = \"\"  # 替换为训练集的数据加载器\nnum_epochs = 10  # 迭代次数\nfor epoch in range(num_epochs):\n    for images, labels in train_loader:\n        optimizer.zero_grad()\n        # 前向传播\n        outputs = model(images)",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "optimizer",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n# 加载数据集并进行训练\ntrain_loader = \"\"  # 替换为训练集的数据加载器\nnum_epochs = 10  # 迭代次数\nfor epoch in range(num_epochs):\n    for images, labels in train_loader:\n        optimizer.zero_grad()\n        # 前向传播\n        outputs = model(images)\n        loss = criterion(outputs, labels)",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "train_loader",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "train_loader = \"\"  # 替换为训练集的数据加载器\nnum_epochs = 10  # 迭代次数\nfor epoch in range(num_epochs):\n    for images, labels in train_loader:\n        optimizer.zero_grad()\n        # 前向传播\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        # 反向传播和优化\n        loss.backward()",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "num_epochs = 10  # 迭代次数\nfor epoch in range(num_epochs):\n    for images, labels in train_loader:\n        optimizer.zero_grad()\n        # 前向传播\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        # 反向传播和优化\n        loss.backward()\n        optimizer.step()",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "api_id",
        "kind": 5,
        "importPath": "bot",
        "description": "bot",
        "peekOfCode": "api_id = \"27175683\"\napi_hash = \"ff5bd7a1d985ad6e6db389e4ef104fe6\"\nbot_token = \"6778102881:AAEJ0PvBN8qDjR3MqNSGNFWUXaLacuKbQ7Y\"\napp = Client(\n    \"kjjjkljklbot\",\n    api_id=api_id, api_hash=api_hash,\n    bot_token=bot_token\n)\n@app.on_message()\nasync def echo(client: Client, message: Message):",
        "detail": "bot",
        "documentation": {}
    },
    {
        "label": "api_hash",
        "kind": 5,
        "importPath": "bot",
        "description": "bot",
        "peekOfCode": "api_hash = \"ff5bd7a1d985ad6e6db389e4ef104fe6\"\nbot_token = \"6778102881:AAEJ0PvBN8qDjR3MqNSGNFWUXaLacuKbQ7Y\"\napp = Client(\n    \"kjjjkljklbot\",\n    api_id=api_id, api_hash=api_hash,\n    bot_token=bot_token\n)\n@app.on_message()\nasync def echo(client: Client, message: Message):\n    await message.reply(message.text)",
        "detail": "bot",
        "documentation": {}
    },
    {
        "label": "bot_token",
        "kind": 5,
        "importPath": "bot",
        "description": "bot",
        "peekOfCode": "bot_token = \"6778102881:AAEJ0PvBN8qDjR3MqNSGNFWUXaLacuKbQ7Y\"\napp = Client(\n    \"kjjjkljklbot\",\n    api_id=api_id, api_hash=api_hash,\n    bot_token=bot_token\n)\n@app.on_message()\nasync def echo(client: Client, message: Message):\n    await message.reply(message.text)\napp.run()",
        "detail": "bot",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "bot",
        "description": "bot",
        "peekOfCode": "app = Client(\n    \"kjjjkljklbot\",\n    api_id=api_id, api_hash=api_hash,\n    bot_token=bot_token\n)\n@app.on_message()\nasync def echo(client: Client, message: Message):\n    await message.reply(message.text)\napp.run()",
        "detail": "bot",
        "documentation": {}
    },
    {
        "label": "paddstr",
        "kind": 2,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "def paddstr(strx):\n    return \"NULL\" if strx == None else \"\\'\" + escape_string(str(strx))+ \"\\'\"\ndef deal_sn_data():\n    bizProConn = MySQL(edun_pro_cfg)\n    sql = \"\"\"\n       SELECT deviceType,deviceNo,sn,imsi,iccid,simPhone,isActive,activeDate,checkCode,modelId,userId,createtime,updateTime,d_LoginUserId,issub,vehicleId,partner,validateStatus,validateBy,validateNums,soft_version,imei,deviceId,source,networkCardNubmer FROM d_track_info dti WHERE dti.sn IN (\t\n       SELECT dti.sn\n       FROM d_vehicle dv,\n       d_track_info dti,\n       d_class dc",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "deal_sn_data",
        "kind": 2,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "def deal_sn_data():\n    bizProConn = MySQL(edun_pro_cfg)\n    sql = \"\"\"\n       SELECT deviceType,deviceNo,sn,imsi,iccid,simPhone,isActive,activeDate,checkCode,modelId,userId,createtime,updateTime,d_LoginUserId,issub,vehicleId,partner,validateStatus,validateBy,validateNums,soft_version,imei,deviceId,source,networkCardNubmer FROM d_track_info dti WHERE dti.sn IN (\t\n       SELECT dti.sn\n       FROM d_vehicle dv,\n       d_track_info dti,\n       d_class dc\n       WHERE dti.vehicleId = dv.vehicleId\n       AND dc.classid = dv.classId",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "logger = logging.getLogger(\"deal_equity_data\")\nlogger.setLevel(logging.DEBUG)\nfileHandler = logging.FileHandler(\"import.log\")\nfileHandler.setLevel(logging.DEBUG)\nformatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\ncnslhandler = logging.StreamHandler()\ncnslhandler.setLevel(logging.DEBUG)\ncnslhandler.setFormatter(formatter)\nlogger.addHandler(cnslhandler)\nedun_pro_cfg = {",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "fileHandler",
        "kind": 5,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "fileHandler = logging.FileHandler(\"import.log\")\nfileHandler.setLevel(logging.DEBUG)\nformatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\ncnslhandler = logging.StreamHandler()\ncnslhandler.setLevel(logging.DEBUG)\ncnslhandler.setFormatter(formatter)\nlogger.addHandler(cnslhandler)\nedun_pro_cfg = {\n    \"host\": \"192.168.5.22\",\n    \"port\": 3307,",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "formatter",
        "kind": 5,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\ncnslhandler = logging.StreamHandler()\ncnslhandler.setLevel(logging.DEBUG)\ncnslhandler.setFormatter(formatter)\nlogger.addHandler(cnslhandler)\nedun_pro_cfg = {\n    \"host\": \"192.168.5.22\",\n    \"port\": 3307,\n    \"user\": \"user_yangll\",\n    \"passwd\": \"vGxw9jWg\",",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "cnslhandler",
        "kind": 5,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "cnslhandler = logging.StreamHandler()\ncnslhandler.setLevel(logging.DEBUG)\ncnslhandler.setFormatter(formatter)\nlogger.addHandler(cnslhandler)\nedun_pro_cfg = {\n    \"host\": \"192.168.5.22\",\n    \"port\": 3307,\n    \"user\": \"user_yangll\",\n    \"passwd\": \"vGxw9jWg\",\n    \"db\": \"gps\",",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "edun_pro_cfg",
        "kind": 5,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "edun_pro_cfg = {\n    \"host\": \"192.168.5.22\",\n    \"port\": 3307,\n    \"user\": \"user_yangll\",\n    \"passwd\": \"vGxw9jWg\",\n    \"db\": \"gps\",\n    \"charset\": \"utf8\",\n}\nedun_test_cfg = {\n    \"host\": \"192.168.3.222\",",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "edun_test_cfg",
        "kind": 5,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "edun_test_cfg = {\n    \"host\": \"192.168.3.222\",\n    \"port\": 3307,\n    \"user\": \"root\",\n    \"passwd\": \"Msd^*$@online\",\n    \"db\": \"newgps\",\n    \"charset\": \"utf8\",\n}\nfrom pymysql.converters import escape_string\n#",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "demoFindLogClass",
        "kind": 6,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "class demoFindLogClass(object):\n    def __init__(self):\n        pass\n    def demoFindLog(self):\n        deault_lng = 110.35\n        deault_lat = 34.62\n        with open(\"biz-gps-kafka-pazlpush.log.2023-07-28.99\", \"r\", encoding='UTF-8') as f:\n            content_list = f.readlines()\n        for content_line in content_list:\n            if \"定位数据推送数据的实体===\" in content_line:",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "parse_json",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def parse_json():\n    path = \"H:\\PythonWorkSpaces\\deal_lnglat_data\"\n    files = os.listdir(path)\n    s = []\n    for file in files:\n        print(file)\n        # 读取文件\n        # f = open(\"biz-gps-kafka-pazlpush.log.2023-07-28.99\", 'r', encoding='UTF-8')\n        # lines = f.readlines()\n        # for line in lines:",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "print_hi",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def print_hi(name):\n    print(f'Hi, {name}')\nclass demoFindLogClass(object):\n    def __init__(self):\n        pass\n    def demoFindLog(self):\n        deault_lng = 110.35\n        deault_lat = 34.62\n        with open(\"biz-gps-kafka-pazlpush.log.2023-07-28.99\", \"r\", encoding='UTF-8') as f:\n            content_list = f.readlines()",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "print_others",
        "kind": 2,
        "importPath": "main_bot",
        "description": "main_bot",
        "peekOfCode": "def print_others(msg):\n    print(msg)\n# 回复 my_friend 的消息 (优先匹配后注册的函数!)\n@bot.register(my_friend)\ndef reply_my_friend(msg):\n    return \"received: {} ({})\".format(msg.text, msg.type)\n# 自动接受新的好友请求\n@bot.register(msg_types=FRIENDS)\ndef auto_accept_friends(msg):\n    # 接受好友请求",
        "detail": "main_bot",
        "documentation": {}
    },
    {
        "label": "reply_my_friend",
        "kind": 2,
        "importPath": "main_bot",
        "description": "main_bot",
        "peekOfCode": "def reply_my_friend(msg):\n    return \"received: {} ({})\".format(msg.text, msg.type)\n# 自动接受新的好友请求\n@bot.register(msg_types=FRIENDS)\ndef auto_accept_friends(msg):\n    # 接受好友请求\n    new_friend = msg.card.accept()\n    # 向新的好友发送消息\n    new_friend.send(\"哈哈，我自动接受了你的好友请求\")",
        "detail": "main_bot",
        "documentation": {}
    },
    {
        "label": "auto_accept_friends",
        "kind": 2,
        "importPath": "main_bot",
        "description": "main_bot",
        "peekOfCode": "def auto_accept_friends(msg):\n    # 接受好友请求\n    new_friend = msg.card.accept()\n    # 向新的好友发送消息\n    new_friend.send(\"哈哈，我自动接受了你的好友请求\")",
        "detail": "main_bot",
        "documentation": {}
    },
    {
        "label": "bot",
        "kind": 5,
        "importPath": "main_bot",
        "description": "main_bot",
        "peekOfCode": "bot = Bot()\n# **找到好友:**\n# 搜索名称含有 \"游否\" 的男性深圳好友\nmy_friend = bot.friends().search(\"游否\", sex=MALE, city=\"深圳\")[0]\n# **发送消息:**\n# 发送文本给好友\nmy_friend.send(\"Hello WeChat!\")\n# 发送图片\nmy_friend.send_image(\"my_picture.jpg\")\n# **自动响应各类消息:**",
        "detail": "main_bot",
        "documentation": {}
    },
    {
        "label": "my_friend",
        "kind": 5,
        "importPath": "main_bot",
        "description": "main_bot",
        "peekOfCode": "my_friend = bot.friends().search(\"游否\", sex=MALE, city=\"深圳\")[0]\n# **发送消息:**\n# 发送文本给好友\nmy_friend.send(\"Hello WeChat!\")\n# 发送图片\nmy_friend.send_image(\"my_picture.jpg\")\n# **自动响应各类消息:**\n# 打印来自其他好友、群聊和公众号的消息\n@bot.register()\ndef print_others(msg):",
        "detail": "main_bot",
        "documentation": {}
    },
    {
        "label": "reply_msg",
        "kind": 2,
        "importPath": "main_itchat",
        "description": "main_itchat",
        "peekOfCode": "def reply_msg(msg):\n    print(\"收到一条信息：\", msg.text)\nif __name__ == \"__main__\":\n    itchat.auto_login()\n    time.sleep(5)\n    itchat.send(\"文件助手你好哦\", toUserName=\"filehelper\")\n    itchat.run()",
        "detail": "main_itchat",
        "documentation": {}
    },
    {
        "label": "friends",
        "kind": 5,
        "importPath": "main_itchat01",
        "description": "main_itchat01",
        "peekOfCode": "friends = itchat.get_friends()\n# 我们可以使用json库将好友列表转换成json格式\nprint(json.dumps(friends))\n# 运行程序\nitchat.run()",
        "detail": "main_itchat01",
        "documentation": {}
    },
    {
        "label": "my_method",
        "kind": 2,
        "importPath": "main_test01",
        "description": "main_test01",
        "peekOfCode": "def my_method():\n    print(\"你好\")\n    # requests.post()\n# end def\ndef bot_chat():\n    \"\"\"\n    Purpose:\n    \"\"\"\n    print(\"nihao\")\n# end def",
        "detail": "main_test01",
        "documentation": {}
    },
    {
        "label": "bot_chat",
        "kind": 2,
        "importPath": "main_test01",
        "description": "main_test01",
        "peekOfCode": "def bot_chat():\n    \"\"\"\n    Purpose:\n    \"\"\"\n    print(\"nihao\")\n# end def\nif __name__ == \"__main__\":\n    my_method()\n    bot_chat()\n    print(\"nihao\")",
        "detail": "main_test01",
        "documentation": {}
    }
]