[
    {
        "label": "AssistantAgent",
        "importPath": "autogen",
        "description": "autogen",
        "isExtraImport": true,
        "detail": "autogen",
        "documentation": {}
    },
    {
        "label": "UserProxyAgent",
        "importPath": "autogen",
        "description": "autogen",
        "isExtraImport": true,
        "detail": "autogen",
        "documentation": {}
    },
    {
        "label": "config_list_from_json",
        "importPath": "autogen",
        "description": "autogen",
        "isExtraImport": true,
        "detail": "autogen",
        "documentation": {}
    },
    {
        "label": "UserProxyAgent",
        "importPath": "autogen",
        "description": "autogen",
        "isExtraImport": true,
        "detail": "autogen",
        "documentation": {}
    },
    {
        "label": "ConversableAgent",
        "importPath": "autogen",
        "description": "autogen",
        "isExtraImport": true,
        "detail": "autogen",
        "documentation": {}
    },
    {
        "label": "config_list_from_json",
        "importPath": "autogen",
        "description": "autogen",
        "isExtraImport": true,
        "detail": "autogen",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "TextLoader",
        "importPath": "langchain.document_loaders",
        "description": "langchain.document_loaders",
        "isExtraImport": true,
        "detail": "langchain.document_loaders",
        "documentation": {}
    },
    {
        "label": "TextLoader",
        "importPath": "langchain.document_loaders",
        "description": "langchain.document_loaders",
        "isExtraImport": true,
        "detail": "langchain.document_loaders",
        "documentation": {}
    },
    {
        "label": "CharacterTextSplitter",
        "importPath": "langchain.text_splitter",
        "description": "langchain.text_splitter",
        "isExtraImport": true,
        "detail": "langchain.text_splitter",
        "documentation": {}
    },
    {
        "label": "OpenAIEmbeddings",
        "importPath": "langchain.embeddings.openai",
        "description": "langchain.embeddings.openai",
        "isExtraImport": true,
        "detail": "langchain.embeddings.openai",
        "documentation": {}
    },
    {
        "label": "OpenAIEmbeddings",
        "importPath": "langchain.embeddings.openai",
        "description": "langchain.embeddings.openai",
        "isExtraImport": true,
        "detail": "langchain.embeddings.openai",
        "documentation": {}
    },
    {
        "label": "DeepLake",
        "importPath": "langchain.vectorstores",
        "description": "langchain.vectorstores",
        "isExtraImport": true,
        "detail": "langchain.vectorstores",
        "documentation": {}
    },
    {
        "label": "DeepLake",
        "importPath": "langchain.vectorstores",
        "description": "langchain.vectorstores",
        "isExtraImport": true,
        "detail": "langchain.vectorstores",
        "documentation": {}
    },
    {
        "label": "FAISS",
        "importPath": "langchain.vectorstores",
        "description": "langchain.vectorstores",
        "isExtraImport": true,
        "detail": "langchain.vectorstores",
        "documentation": {}
    },
    {
        "label": "ConversationalRetrievalChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "APIChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMBashChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMCheckerChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMRequestsChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMSummarizationCheckerChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMSummarizationCheckerChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMSummarizationCheckerChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "OpenAIModerationChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "SequentialChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "SimpleSequentialChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "RetrievalQA",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "ConversationChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain.chat_models",
        "description": "langchain.chat_models",
        "isExtraImport": true,
        "detail": "langchain.chat_models",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain.chat_models",
        "description": "langchain.chat_models",
        "isExtraImport": true,
        "detail": "langchain.chat_models",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain.chat_models",
        "description": "langchain.chat_models",
        "isExtraImport": true,
        "detail": "langchain.chat_models",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain.chat_models",
        "description": "langchain.chat_models",
        "isExtraImport": true,
        "detail": "langchain.chat_models",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain.chat_models",
        "description": "langchain.chat_models",
        "isExtraImport": true,
        "detail": "langchain.chat_models",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain.chat_models",
        "description": "langchain.chat_models",
        "isExtraImport": true,
        "detail": "langchain.chat_models",
        "documentation": {}
    },
    {
        "label": "AutoModelForSequenceClassification",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "MsDataset",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoModelForSequenceClassification",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "snapshot_download",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoModelForSequenceClassification",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "MsDataset",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoModelForSequenceClassification",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "default_data_collator",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "PerceiverTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "PerceiverForMaskedLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "PerceiverTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "PerceiverForMaskedLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "pipeline",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "pipeline",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "default_data_collator",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "LoRAConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "TrainingArguments",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "LoRAConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "LoRAConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "TrainingArguments",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "LoRAConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "LoraConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "read_config",
        "importPath": "modelscope.utils.hub",
        "description": "modelscope.utils.hub",
        "isExtraImport": true,
        "detail": "modelscope.utils.hub",
        "documentation": {}
    },
    {
        "label": "MsDataset",
        "importPath": "modelscope.msdatasets",
        "description": "modelscope.msdatasets",
        "isExtraImport": true,
        "detail": "modelscope.msdatasets",
        "documentation": {}
    },
    {
        "label": "build_trainer",
        "importPath": "modelscope.trainers",
        "description": "modelscope.trainers",
        "isExtraImport": true,
        "detail": "modelscope.trainers",
        "documentation": {}
    },
    {
        "label": "build_trainer",
        "importPath": "modelscope.trainers",
        "description": "modelscope.trainers",
        "isExtraImport": true,
        "detail": "modelscope.trainers",
        "documentation": {}
    },
    {
        "label": "tempfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tempfile",
        "description": "tempfile",
        "detail": "tempfile",
        "documentation": {}
    },
    {
        "label": "Trainers",
        "importPath": "modelscope.metainfo",
        "description": "modelscope.metainfo",
        "isExtraImport": true,
        "detail": "modelscope.metainfo",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Linear",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "CrossEntropyLoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "torch.optim",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.optim",
        "description": "torch.optim",
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "torch.optim",
        "description": "torch.optim",
        "isExtraImport": true,
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "StepLR",
        "importPath": "torch.optim.lr_scheduler",
        "description": "torch.optim.lr_scheduler",
        "isExtraImport": true,
        "detail": "torch.optim.lr_scheduler",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "default_collate",
        "importPath": "torch.utils.data.dataloader",
        "description": "torch.utils.data.dataloader",
        "isExtraImport": true,
        "detail": "torch.utils.data.dataloader",
        "documentation": {}
    },
    {
        "label": "AgentType",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "initialize_agent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "load_tools",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "Tool",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "BaseSingleActionAgent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentOutputParser",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "LLMSingleActionAgent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "Tool",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "ZeroShotAgent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "Tool",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "ZeroShotAgent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "Tool",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "Tool",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "BaseMultiActionAgent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "load_tools",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "initialize_agent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentType",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "load_tools",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "initialize_agent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentType",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentType",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "initialize_agent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "load_tools",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "langchain",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "langchain",
        "description": "langchain",
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "SerpAPIWrapper",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "SerpAPIWrapper",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "SerpAPIWrapper",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "SerpAPIWrapper",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "SerpAPIWrapper",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "LLMMathChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "ConversationChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "AgentAction",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AgentFinish",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AgentAction",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AgentFinish",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AgentAction",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AgentFinish",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AIMessage",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "SystemMessage",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "StringPromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "MessagesPlaceholder",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "SystemMessagePromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "HumanMessagePromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "OpenAIEmbeddings",
        "importPath": "langchain.embeddings",
        "description": "langchain.embeddings",
        "isExtraImport": true,
        "detail": "langchain.embeddings",
        "documentation": {}
    },
    {
        "label": "API_RESPONSE_PROMPT",
        "importPath": "langchain.chains.api.prompt",
        "description": "langchain.chains.api.prompt",
        "isExtraImport": true,
        "detail": "langchain.chains.api.prompt",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts.prompt",
        "description": "langchain.prompts.prompt",
        "isExtraImport": true,
        "detail": "langchain.prompts.prompt",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts.prompt",
        "description": "langchain.prompts.prompt",
        "isExtraImport": true,
        "detail": "langchain.prompts.prompt",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts.prompt",
        "description": "langchain.prompts.prompt",
        "isExtraImport": true,
        "detail": "langchain.prompts.prompt",
        "documentation": {}
    },
    {
        "label": "open_meteo_docs",
        "importPath": "langchain.chains.api",
        "description": "langchain.chains.api",
        "isExtraImport": true,
        "detail": "langchain.chains.api",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain.chains.llm",
        "description": "langchain.chains.llm",
        "isExtraImport": true,
        "detail": "langchain.chains.llm",
        "documentation": {}
    },
    {
        "label": "ConstitutionalChain",
        "importPath": "langchain.chains.constitutional_ai.base",
        "description": "langchain.chains.constitutional_ai.base",
        "isExtraImport": true,
        "detail": "langchain.chains.constitutional_ai.base",
        "documentation": {}
    },
    {
        "label": "ConstitutionalPrinciple",
        "importPath": "langchain.chains.constitutional_ai.models",
        "description": "langchain.chains.constitutional_ai.models",
        "isExtraImport": true,
        "detail": "langchain.chains.constitutional_ai.models",
        "documentation": {}
    },
    {
        "label": "BashOutputParser",
        "importPath": "langchain.chains.llm_bash.prompt",
        "description": "langchain.chains.llm_bash.prompt",
        "isExtraImport": true,
        "detail": "langchain.chains.llm_bash.prompt",
        "documentation": {}
    },
    {
        "label": "BashOutputParser",
        "importPath": "langchain.chains.llm_bash.prompt",
        "description": "langchain.chains.llm_bash.prompt",
        "isExtraImport": true,
        "detail": "langchain.chains.llm_bash.prompt",
        "documentation": {}
    },
    {
        "label": "BashProcess",
        "importPath": "langchain.utilities.bash",
        "description": "langchain.utilities.bash",
        "isExtraImport": true,
        "detail": "langchain.utilities.bash",
        "documentation": {}
    },
    {
        "label": "asyncio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "asyncio",
        "description": "asyncio",
        "detail": "asyncio",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "VectorstoreIndexCreator",
        "importPath": "langchain.indexes",
        "description": "langchain.indexes",
        "isExtraImport": true,
        "detail": "langchain.indexes",
        "documentation": {}
    },
    {
        "label": "ConversationBufferMemory",
        "importPath": "langchain.memory",
        "description": "langchain.memory",
        "isExtraImport": true,
        "detail": "langchain.memory",
        "documentation": {}
    },
    {
        "label": "ConversationBufferMemory",
        "importPath": "langchain.memory",
        "description": "langchain.memory",
        "isExtraImport": true,
        "detail": "langchain.memory",
        "documentation": {}
    },
    {
        "label": "openai",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "openai",
        "description": "openai",
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "SystemMessagePromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "HumanMessagePromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "SystemMessagePromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "HumanMessagePromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "DatasetName",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "ModelType",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "SftArguments",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "sft_main",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "ModelType",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "get_vllm_engine",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "get_default_template_type",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "get_template",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "inference_vllm",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "pipeline",
        "importPath": "modelscope.pipelines",
        "description": "modelscope.pipelines",
        "isExtraImport": true,
        "detail": "modelscope.pipelines",
        "documentation": {}
    },
    {
        "label": "pipeline",
        "importPath": "modelscope.pipelines",
        "description": "modelscope.pipelines",
        "isExtraImport": true,
        "detail": "modelscope.pipelines",
        "documentation": {}
    },
    {
        "label": "Tasks",
        "importPath": "modelscope.utils.constant",
        "description": "modelscope.utils.constant",
        "isExtraImport": true,
        "detail": "modelscope.utils.constant",
        "documentation": {}
    },
    {
        "label": "Tasks",
        "importPath": "modelscope.utils.constant",
        "description": "modelscope.utils.constant",
        "isExtraImport": true,
        "detail": "modelscope.utils.constant",
        "documentation": {}
    },
    {
        "label": "timm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "timm",
        "description": "timm",
        "detail": "timm",
        "documentation": {}
    },
    {
        "label": "StableDiffusionPipeline",
        "importPath": "ppdiffusers",
        "description": "ppdiffusers",
        "isExtraImport": true,
        "detail": "ppdiffusers",
        "documentation": {}
    },
    {
        "label": "paddle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "paddle",
        "description": "paddle",
        "detail": "paddle",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "Model",
        "importPath": "modelscope.models",
        "description": "modelscope.models",
        "isExtraImport": true,
        "detail": "modelscope.models",
        "documentation": {}
    },
    {
        "label": "SbertForSequenceClassification",
        "importPath": "modelscope.models.nlp",
        "description": "modelscope.models.nlp",
        "isExtraImport": true,
        "detail": "modelscope.models.nlp",
        "documentation": {}
    },
    {
        "label": "SbertConfig",
        "importPath": "modelscope.models.nlp.structbert",
        "description": "modelscope.models.nlp.structbert",
        "isExtraImport": true,
        "detail": "modelscope.models.nlp.structbert",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift.tuners",
        "description": "swift.tuners",
        "isExtraImport": true,
        "detail": "swift.tuners",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "MySQLdb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "MySQLdb",
        "description": "MySQLdb",
        "detail": "MySQLdb",
        "documentation": {}
    },
    {
        "label": "easyocr",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "easyocr",
        "description": "easyocr",
        "detail": "easyocr",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "PaddleOCR",
        "importPath": "paddleocr",
        "description": "paddleocr",
        "isExtraImport": true,
        "detail": "paddleocr",
        "documentation": {}
    },
    {
        "label": "draw_ocr",
        "importPath": "paddleocr",
        "description": "paddleocr",
        "isExtraImport": true,
        "detail": "paddleocr",
        "documentation": {}
    },
    {
        "label": "models",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "Client",
        "importPath": "pyrogram",
        "description": "pyrogram",
        "isExtraImport": true,
        "detail": "pyrogram",
        "documentation": {}
    },
    {
        "label": "Message",
        "importPath": "pyrogram.types",
        "description": "pyrogram.types",
        "isExtraImport": true,
        "detail": "pyrogram.types",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "Decimal",
        "importPath": "decimal",
        "description": "decimal",
        "isExtraImport": true,
        "detail": "decimal",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "dateutil.relativedelta",
        "description": "dateutil.relativedelta",
        "isExtraImport": true,
        "detail": "dateutil.relativedelta",
        "documentation": {}
    },
    {
        "label": "MySQL",
        "importPath": "my_utils.MysqlUtility",
        "description": "my_utils.MysqlUtility",
        "isExtraImport": true,
        "detail": "my_utils.MysqlUtility",
        "documentation": {}
    },
    {
        "label": "groupby",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "escape_string",
        "importPath": "pymysql.converters",
        "description": "pymysql.converters",
        "isExtraImport": true,
        "detail": "pymysql.converters",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "wxpy",
        "description": "wxpy",
        "isExtraImport": true,
        "detail": "wxpy",
        "documentation": {}
    },
    {
        "label": "itchat",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "itchat",
        "description": "itchat",
        "detail": "itchat",
        "documentation": {}
    },
    {
        "label": "itchat,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "itchat.",
        "description": "itchat.",
        "detail": "itchat.",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "config_list",
        "kind": 5,
        "importPath": "autogen.test01",
        "description": "autogen.test01",
        "peekOfCode": "config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")\n# You can also set config_list directly as a list, for example, config_list = [{'model': 'gpt-4', 'api_key': '<your OpenAI API key here>'},]\nassistant = AssistantAgent(\"assistant\", llm_config={\"config_list\": config_list})\nuser_proxy = UserProxyAgent(\n    \"user_proxy\", code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False}\n)  # IMPORTANT: set to True to run code in docker, recommended\nuser_proxy.initiate_chat(\n    assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\"\n)\n# This initiates an automated chat between the two agents to solve the task",
        "detail": "autogen.test01",
        "documentation": {}
    },
    {
        "label": "assistant",
        "kind": 5,
        "importPath": "autogen.test01",
        "description": "autogen.test01",
        "peekOfCode": "assistant = AssistantAgent(\"assistant\", llm_config={\"config_list\": config_list})\nuser_proxy = UserProxyAgent(\n    \"user_proxy\", code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False}\n)  # IMPORTANT: set to True to run code in docker, recommended\nuser_proxy.initiate_chat(\n    assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\"\n)\n# This initiates an automated chat between the two agents to solve the task",
        "detail": "autogen.test01",
        "documentation": {}
    },
    {
        "label": "user_proxy",
        "kind": 5,
        "importPath": "autogen.test01",
        "description": "autogen.test01",
        "peekOfCode": "user_proxy = UserProxyAgent(\n    \"user_proxy\", code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False}\n)  # IMPORTANT: set to True to run code in docker, recommended\nuser_proxy.initiate_chat(\n    assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\"\n)\n# This initiates an automated chat between the two agents to solve the task",
        "detail": "autogen.test01",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "autogen.test02",
        "description": "autogen.test02",
        "peekOfCode": "def main():\n    # Load LLM inference endpoints from an env variable or a file\n    # See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints\n    # and OAI_CONFIG_LIST_sample.\n    # For example, if you have created a OAI_CONFIG_LIST file in the current working directory, that file will be used.\n    config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST.json\")\n    # Create the agent that uses the LLM.\n    assistant = ConversableAgent(\"agent\", llm_config={\"config_list\": config_list})\n    # Create the agent that represents the user in the conversation.\n    user_proxy = UserProxyAgent(\"user\", code_execution_config=False)",
        "detail": "autogen.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "autogen.test02",
        "description": "autogen.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\ndef main():\n    # Load LLM inference endpoints from an env variable or a file\n    # See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints\n    # and OAI_CONFIG_LIST_sample.\n    # For example, if you have created a OAI_CONFIG_LIST file in the current working directory, that file will be used.\n    config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST.json\")\n    # Create the agent that uses the LLM.\n    assistant = ConversableAgent(\"agent\", llm_config={\"config_list\": config_list})\n    # Create the agent that represents the user in the conversation.",
        "detail": "autogen.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"ACTIVELOOP_TOKEN\"\n] = \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwNjA4Mjc0NywiZXhwIjoxNzM4MzA5OTM0fQ.eyJpZCI6InlhbmdsaW5sb25nNiJ9.SNbGhqyihTBsiGhk6SOobDtvWwl0mxW-8YIokUGlvZdu2Z4-db4bnMWK_LIexTwZHHQeYmdHNlqE3gulUULTeg\"\n# Please manually enter OpenAI Key\nfrom langchain.document_loaders import TextLoader\n# root_dir = \"../\"\nroot_dir = \"H:\\\\WorkSpaces\\\\temp\\\\develop\\\\car-service-ordercenter\"",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"ACTIVELOOP_TOKEN\"\n] = \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwNjA4Mjc0NywiZXhwIjoxNzM4MzA5OTM0fQ.eyJpZCI6InlhbmdsaW5sb25nNiJ9.SNbGhqyihTBsiGhk6SOobDtvWwl0mxW-8YIokUGlvZdu2Z4-db4bnMWK_LIexTwZHHQeYmdHNlqE3gulUULTeg\"\n# Please manually enter OpenAI Key\nfrom langchain.document_loaders import TextLoader\n# root_dir = \"../\"\nroot_dir = \"H:\\\\WorkSpaces\\\\temp\\\\develop\\\\car-service-ordercenter\"\ndocs = []\nfor dirpath, dirnames, filenames in os.walk(root_dir):",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "] = \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwNjA4Mjc0NywiZXhwIjoxNzM4MzA5OTM0fQ.eyJpZCI6InlhbmdsaW5sb25nNiJ9.SNbGhqyihTBsiGhk6SOobDtvWwl0mxW-8YIokUGlvZdu2Z4-db4bnMWK_LIexTwZHHQeYmdHNlqE3gulUULTeg\"\n# Please manually enter OpenAI Key\nfrom langchain.document_loaders import TextLoader\n# root_dir = \"../\"\nroot_dir = \"H:\\\\WorkSpaces\\\\temp\\\\develop\\\\car-service-ordercenter\"\ndocs = []\nfor dirpath, dirnames, filenames in os.walk(root_dir):\n    print(\"filenames\", filenames)\n    for file in filenames:\n        # if file.endswith(\".py\") and \"/.venv/\" not in dirpath:",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "root_dir",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "root_dir = \"H:\\\\WorkSpaces\\\\temp\\\\develop\\\\car-service-ordercenter\"\ndocs = []\nfor dirpath, dirnames, filenames in os.walk(root_dir):\n    print(\"filenames\", filenames)\n    for file in filenames:\n        # if file.endswith(\".py\") and \"/.venv/\" not in dirpath:\n        if (\n            file.endswith(\".java\")\n            or file.endswith(\".xml\")\n            or file.endswith(\".properties\")",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "docs",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "docs = []\nfor dirpath, dirnames, filenames in os.walk(root_dir):\n    print(\"filenames\", filenames)\n    for file in filenames:\n        # if file.endswith(\".py\") and \"/.venv/\" not in dirpath:\n        if (\n            file.endswith(\".java\")\n            or file.endswith(\".xml\")\n            or file.endswith(\".properties\")\n            and \"/.venv/\" not in dirpath",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "text_splitter",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ntexts = text_splitter.split_documents(docs)\nprint(f\"{len(texts)}\")\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nembeddings = OpenAIEmbeddings()\nfrom langchain.vectorstores import DeepLake\ndb = DeepLake.from_documents(\n    # texts, embeddings, dataset_path=f\"hub://yanglinlong6/my-code\"\n    texts,\n    embeddings,",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "texts",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "texts = text_splitter.split_documents(docs)\nprint(f\"{len(texts)}\")\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nembeddings = OpenAIEmbeddings()\nfrom langchain.vectorstores import DeepLake\ndb = DeepLake.from_documents(\n    # texts, embeddings, dataset_path=f\"hub://yanglinlong6/my-code\"\n    texts,\n    embeddings,\n    dataset_path=f\"hub://yanglinlong6/car-service-ordercenter\",",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "embeddings",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "embeddings = OpenAIEmbeddings()\nfrom langchain.vectorstores import DeepLake\ndb = DeepLake.from_documents(\n    # texts, embeddings, dataset_path=f\"hub://yanglinlong6/my-code\"\n    texts,\n    embeddings,\n    dataset_path=f\"hub://yanglinlong6/car-service-ordercenter\",\n)\nprint(db)",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "db",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "db = DeepLake.from_documents(\n    # texts, embeddings, dataset_path=f\"hub://yanglinlong6/my-code\"\n    texts,\n    embeddings,\n    dataset_path=f\"hub://yanglinlong6/car-service-ordercenter\",\n)\nprint(db)",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "filter",
        "kind": 2,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "def filter(x):\n    # filter based on source code\n    if \"something\" in x[\"text\"].data()[\"value\"]:\n        return False\n    # filter based on path e.g. extension\n    metadata = x[\"metadata\"].data()[\"value\"]\n    return \"only_this\" in metadata[\"source\"] or \"also_that\" in metadata[\"source\"]\n### turn on below for custom filtering\n# retriever.search_kwargs['filter'] = filter\nfrom langchain.chains import ConversationalRetrievalChain",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"ACTIVELOOP_TOKEN\"\n] = \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwNjA4Mjc0NywiZXhwIjoxNzM4MzA5OTM0fQ.eyJpZCI6InlhbmdsaW5sb25nNiJ9.SNbGhqyihTBsiGhk6SOobDtvWwl0mxW-8YIokUGlvZdu2Z4-db4bnMWK_LIexTwZHHQeYmdHNlqE3gulUULTeg\"\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import DeepLake\nembeddings = OpenAIEmbeddings()\ndb = DeepLake(",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"ACTIVELOOP_TOKEN\"\n] = \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwNjA4Mjc0NywiZXhwIjoxNzM4MzA5OTM0fQ.eyJpZCI6InlhbmdsaW5sb25nNiJ9.SNbGhqyihTBsiGhk6SOobDtvWwl0mxW-8YIokUGlvZdu2Z4-db4bnMWK_LIexTwZHHQeYmdHNlqE3gulUULTeg\"\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import DeepLake\nembeddings = OpenAIEmbeddings()\ndb = DeepLake(\n    # dataset_path=f\"hub://yanglinlong6/my-code\",\n    dataset_path=f\"hub://yanglinlong6/car-service-ordercenter\",",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "] = \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwNjA4Mjc0NywiZXhwIjoxNzM4MzA5OTM0fQ.eyJpZCI6InlhbmdsaW5sb25nNiJ9.SNbGhqyihTBsiGhk6SOobDtvWwl0mxW-8YIokUGlvZdu2Z4-db4bnMWK_LIexTwZHHQeYmdHNlqE3gulUULTeg\"\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import DeepLake\nembeddings = OpenAIEmbeddings()\ndb = DeepLake(\n    # dataset_path=f\"hub://yanglinlong6/my-code\",\n    dataset_path=f\"hub://yanglinlong6/car-service-ordercenter\",\n    read_only=True,\n    embedding_function=embeddings,\n)",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "embeddings",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "embeddings = OpenAIEmbeddings()\ndb = DeepLake(\n    # dataset_path=f\"hub://yanglinlong6/my-code\",\n    dataset_path=f\"hub://yanglinlong6/car-service-ordercenter\",\n    read_only=True,\n    embedding_function=embeddings,\n)\nprint(\"db\", db)\n# Dataset(\n#     path=\"hub://yanglinlong6/langchain-code\",",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "db",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "db = DeepLake(\n    # dataset_path=f\"hub://yanglinlong6/my-code\",\n    dataset_path=f\"hub://yanglinlong6/car-service-ordercenter\",\n    read_only=True,\n    embedding_function=embeddings,\n)\nprint(\"db\", db)\n# Dataset(\n#     path=\"hub://yanglinlong6/langchain-code\",\n#     read_only=True,",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "retriever",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "retriever = db.as_retriever()\nretriever.search_kwargs[\"distance_metric\"] = \"cos\"\nretriever.search_kwargs[\"fetch_k\"] = 20\nretriever.search_kwargs[\"maximal_marginal_relevance\"] = True\nretriever.search_kwargs[\"k\"] = 20\ndef filter(x):\n    # filter based on source code\n    if \"something\" in x[\"text\"].data()[\"value\"]:\n        return False\n    # filter based on path e.g. extension",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "retriever.search_kwargs[\"distance_metric\"]",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "retriever.search_kwargs[\"distance_metric\"] = \"cos\"\nretriever.search_kwargs[\"fetch_k\"] = 20\nretriever.search_kwargs[\"maximal_marginal_relevance\"] = True\nretriever.search_kwargs[\"k\"] = 20\ndef filter(x):\n    # filter based on source code\n    if \"something\" in x[\"text\"].data()[\"value\"]:\n        return False\n    # filter based on path e.g. extension\n    metadata = x[\"metadata\"].data()[\"value\"]",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "retriever.search_kwargs[\"fetch_k\"]",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "retriever.search_kwargs[\"fetch_k\"] = 20\nretriever.search_kwargs[\"maximal_marginal_relevance\"] = True\nretriever.search_kwargs[\"k\"] = 20\ndef filter(x):\n    # filter based on source code\n    if \"something\" in x[\"text\"].data()[\"value\"]:\n        return False\n    # filter based on path e.g. extension\n    metadata = x[\"metadata\"].data()[\"value\"]\n    return \"only_this\" in metadata[\"source\"] or \"also_that\" in metadata[\"source\"]",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "retriever.search_kwargs[\"maximal_marginal_relevance\"]",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "retriever.search_kwargs[\"maximal_marginal_relevance\"] = True\nretriever.search_kwargs[\"k\"] = 20\ndef filter(x):\n    # filter based on source code\n    if \"something\" in x[\"text\"].data()[\"value\"]:\n        return False\n    # filter based on path e.g. extension\n    metadata = x[\"metadata\"].data()[\"value\"]\n    return \"only_this\" in metadata[\"source\"] or \"also_that\" in metadata[\"source\"]\n### turn on below for custom filtering",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "retriever.search_kwargs[\"k\"]",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "retriever.search_kwargs[\"k\"] = 20\ndef filter(x):\n    # filter based on source code\n    if \"something\" in x[\"text\"].data()[\"value\"]:\n        return False\n    # filter based on path e.g. extension\n    metadata = x[\"metadata\"].data()[\"value\"]\n    return \"only_this\" in metadata[\"source\"] or \"also_that\" in metadata[\"source\"]\n### turn on below for custom filtering\n# retriever.search_kwargs['filter'] = filter",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "model = ChatOpenAI(model_name=\"gpt-4\")  # 'ada' 'gpt-3.5-turbo' 'gpt-4',\nqa = ConversationalRetrievalChain.from_llm(model, retriever=retriever)\nquestions = [\n    # \"What is the class hierarchy?\",\n    # \"\"\n    \",100\"\n    # \"CarOrderApiController,150\"\n    # \"What classes are derived from the Chain class?\",\n    # \"What classes and functions in the ./langchain/utilities/ forlder are not covered by unit tests?\",\n    # \"What one improvement do you propose in code in relation to the class herarchy for the Chain class?\",",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "qa",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "qa = ConversationalRetrievalChain.from_llm(model, retriever=retriever)\nquestions = [\n    # \"What is the class hierarchy?\",\n    # \"\"\n    \",100\"\n    # \"CarOrderApiController,150\"\n    # \"What classes are derived from the Chain class?\",\n    # \"What classes and functions in the ./langchain/utilities/ forlder are not covered by unit tests?\",\n    # \"What one improvement do you propose in code in relation to the class herarchy for the Chain class?\",\n]",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "questions",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "questions = [\n    # \"What is the class hierarchy?\",\n    # \"\"\n    \",100\"\n    # \"CarOrderApiController,150\"\n    # \"What classes are derived from the Chain class?\",\n    # \"What classes and functions in the ./langchain/utilities/ forlder are not covered by unit tests?\",\n    # \"What one improvement do you propose in code in relation to the class herarchy for the Chain class?\",\n]\nchat_history = [(\"\", \" \")]",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "chat_history",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "chat_history = [(\"\", \" \")]\nfor question in questions:\n    result = qa({\"question\": question, \"chat_history\": chat_history})\n    # result = qa({\"question\": question})\n    chat_history.append((question, result[\"answer\"]))\n    print(f\"-> \\*\\*Question\\*\\*: {question} \")\n    print(f\"\\*\\*Answer\\*\\*: {result['answer']} \")",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "get_files_in_directory",
        "kind": 2,
        "importPath": "deeplake.test03",
        "description": "deeplake.test03",
        "peekOfCode": "def get_files_in_directory(directory):\n    file_paths = []\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            file_path = os.path.join(root, file)\n            file_paths.append(file_path)\n    return file_paths\ndirectory_path = \"H:\\\\WorkSpaces\\\\temp\\\\develop\\\\car-service-ordercenter\"\nfiles = get_files_in_directory(directory_path)\n# ",
        "detail": "deeplake.test03",
        "documentation": {}
    },
    {
        "label": "directory_path",
        "kind": 5,
        "importPath": "deeplake.test03",
        "description": "deeplake.test03",
        "peekOfCode": "directory_path = \"H:\\\\WorkSpaces\\\\temp\\\\develop\\\\car-service-ordercenter\"\nfiles = get_files_in_directory(directory_path)\n# \nfor file in files:\n    print(file)",
        "detail": "deeplake.test03",
        "documentation": {}
    },
    {
        "label": "files",
        "kind": 5,
        "importPath": "deeplake.test03",
        "description": "deeplake.test03",
        "peekOfCode": "files = get_files_in_directory(directory_path)\n# \nfor file in files:\n    print(file)",
        "detail": "deeplake.test03",
        "documentation": {}
    },
    {
        "label": "tokenize_function",
        "kind": 2,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "def tokenize_function(examples):\n    return tokenizer(\n        examples[\"sentence1\"],\n        examples[\"sentence2\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=128,\n    )\ntrain_dataset = train_dataset.map(tokenize_function)\nval_dataset = val_dataset.map(tokenize_function)",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator\nfrom swift import Trainer, LoRAConfig, Swift, TrainingArguments",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator\nfrom swift import Trainer, LoRAConfig, Swift, TrainingArguments\nmodel = AutoModelForSequenceClassification.from_pretrained(",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator\nfrom swift import Trainer, LoRAConfig, Swift, TrainingArguments\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator\nfrom swift import Trainer, LoRAConfig, Swift, TrainingArguments\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator\nfrom swift import Trainer, LoRAConfig, Swift, TrainingArguments\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator\nfrom swift import Trainer, LoRAConfig, Swift, TrainingArguments\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator\nfrom swift import Trainer, LoRAConfig, Swift, TrainingArguments\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "model = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "lora_config",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "lora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)\nval_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "model = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)\nval_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()\n    .select(range(100))",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "train_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)\nval_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()\n    .select(range(100))\n)",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "val_dataset",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "val_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()\n    .select(range(100))\n)\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"sentence1\"],\n        examples[\"sentence2\"],\n        padding=\"max_length\",",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "train_dataset = train_dataset.map(tokenize_function)\nval_dataset = val_dataset.map(tokenize_function)\narguments = TrainingArguments(\n    output_dir=\"./outputs\",\n    per_device_train_batch_size=16,\n)\ntrainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "val_dataset",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "val_dataset = val_dataset.map(tokenize_function)\narguments = TrainingArguments(\n    output_dir=\"./outputs\",\n    per_device_train_batch_size=16,\n)\ntrainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "arguments",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "arguments = TrainingArguments(\n    output_dir=\"./outputs\",\n    per_device_train_batch_size=16,\n)\ntrainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=default_data_collator,",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "trainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=default_data_collator,\n)\ntrainer.train()",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "huggingface.test04",
        "description": "huggingface.test04",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer\nfrom swift import Trainer, LoRAConfig, Swift\nmodel = AutoModelForSequenceClassification.from_pretrained(",
        "detail": "huggingface.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test04",
        "description": "huggingface.test04",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer\nfrom swift import Trainer, LoRAConfig, Swift\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"",
        "detail": "huggingface.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "huggingface.test04",
        "description": "huggingface.test04",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer\nfrom swift import Trainer, LoRAConfig, Swift\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)",
        "detail": "huggingface.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test04",
        "description": "huggingface.test04",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer\nfrom swift import Trainer, LoRAConfig, Swift\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(",
        "detail": "huggingface.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test04",
        "description": "huggingface.test04",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer\nfrom swift import Trainer, LoRAConfig, Swift\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"",
        "detail": "huggingface.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test04",
        "description": "huggingface.test04",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer\nfrom swift import Trainer, LoRAConfig, Swift\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)",
        "detail": "huggingface.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test04",
        "description": "huggingface.test04",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer\nfrom swift import Trainer, LoRAConfig, Swift\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])",
        "detail": "huggingface.test04",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test04",
        "description": "huggingface.test04",
        "peekOfCode": "model = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))",
        "detail": "huggingface.test04",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "huggingface.test04",
        "description": "huggingface.test04",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))",
        "detail": "huggingface.test04",
        "documentation": {}
    },
    {
        "label": "lora_config",
        "kind": 5,
        "importPath": "huggingface.test04",
        "description": "huggingface.test04",
        "peekOfCode": "lora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))",
        "detail": "huggingface.test04",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test04",
        "description": "huggingface.test04",
        "peekOfCode": "model = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))",
        "detail": "huggingface.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.utils.hub import read_config\nfrom modelscope.msdatasets import MsDataset\nfrom modelscope.trainers import build_trainer",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.utils.hub import read_config\nfrom modelscope.msdatasets import MsDataset\nfrom modelscope.trainers import build_trainer\ntrain_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.utils.hub import read_config\nfrom modelscope.msdatasets import MsDataset\nfrom modelscope.trainers import build_trainer\ntrain_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\neval_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.utils.hub import read_config\nfrom modelscope.msdatasets import MsDataset\nfrom modelscope.trainers import build_trainer\ntrain_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\neval_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\nmodel_id = \"damo/nlp_structbert_sentence-similarity_chinese-base\"",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.utils.hub import read_config\nfrom modelscope.msdatasets import MsDataset\nfrom modelscope.trainers import build_trainer\ntrain_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\neval_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\nmodel_id = \"damo/nlp_structbert_sentence-similarity_chinese-base\"\n# modelcfg",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.utils.hub import read_config\nfrom modelscope.msdatasets import MsDataset\nfrom modelscope.trainers import build_trainer\ntrain_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\neval_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\nmodel_id = \"damo/nlp_structbert_sentence-similarity_chinese-base\"\n# modelcfg\ncfg = read_config(model_id)",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.utils.hub import read_config\nfrom modelscope.msdatasets import MsDataset\nfrom modelscope.trainers import build_trainer\ntrain_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\neval_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\nmodel_id = \"damo/nlp_structbert_sentence-similarity_chinese-base\"\n# modelcfg\ncfg = read_config(model_id)\n# ",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "train_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\neval_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\nmodel_id = \"damo/nlp_structbert_sentence-similarity_chinese-base\"\n# modelcfg\ncfg = read_config(model_id)\n# \ncfg.train.max_epochs = 5\ncfg.preprocessor.train[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.preprocessor.val[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.train.work_dir = \"/tmp\"",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "eval_dataset",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "eval_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\nmodel_id = \"damo/nlp_structbert_sentence-similarity_chinese-base\"\n# modelcfg\ncfg = read_config(model_id)\n# \ncfg.train.max_epochs = 5\ncfg.preprocessor.train[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.preprocessor.val[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.train.work_dir = \"/tmp\"\ncfg_file = os.path.join(\"/tmp\", \"config.json\")",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "model_id",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "model_id = \"damo/nlp_structbert_sentence-similarity_chinese-base\"\n# modelcfg\ncfg = read_config(model_id)\n# \ncfg.train.max_epochs = 5\ncfg.preprocessor.train[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.preprocessor.val[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.train.work_dir = \"/tmp\"\ncfg_file = os.path.join(\"/tmp\", \"config.json\")\n# trainer",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "cfg",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "cfg = read_config(model_id)\n# \ncfg.train.max_epochs = 5\ncfg.preprocessor.train[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.preprocessor.val[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.train.work_dir = \"/tmp\"\ncfg_file = os.path.join(\"/tmp\", \"config.json\")\n# trainer\ncfg.dump(cfg_file)\nkwargs = dict(",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "cfg.train.max_epochs",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "cfg.train.max_epochs = 5\ncfg.preprocessor.train[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.preprocessor.val[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.train.work_dir = \"/tmp\"\ncfg_file = os.path.join(\"/tmp\", \"config.json\")\n# trainer\ncfg.dump(cfg_file)\nkwargs = dict(\n    model=model_id,\n    train_dataset=train_dataset,",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "cfg.preprocessor.train[\"label2id\"]",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "cfg.preprocessor.train[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.preprocessor.val[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.train.work_dir = \"/tmp\"\ncfg_file = os.path.join(\"/tmp\", \"config.json\")\n# trainer\ncfg.dump(cfg_file)\nkwargs = dict(\n    model=model_id,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "cfg.preprocessor.val[\"label2id\"]",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "cfg.preprocessor.val[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.train.work_dir = \"/tmp\"\ncfg_file = os.path.join(\"/tmp\", \"config.json\")\n# trainer\ncfg.dump(cfg_file)\nkwargs = dict(\n    model=model_id,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    cfg_file=cfg_file,",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "cfg.train.work_dir",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "cfg.train.work_dir = \"/tmp\"\ncfg_file = os.path.join(\"/tmp\", \"config.json\")\n# trainer\ncfg.dump(cfg_file)\nkwargs = dict(\n    model=model_id,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    cfg_file=cfg_file,\n)",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "cfg_file",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "cfg_file = os.path.join(\"/tmp\", \"config.json\")\n# trainer\ncfg.dump(cfg_file)\nkwargs = dict(\n    model=model_id,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    cfg_file=cfg_file,\n)\ntrainer = build_trainer(default_args=kwargs)",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "kwargs",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "kwargs = dict(\n    model=model_id,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    cfg_file=cfg_file,\n)\ntrainer = build_trainer(default_args=kwargs)\ntrainer.train()",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "trainer = build_trainer(default_args=kwargs)\ntrainer.train()",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "huggingface.test06",
        "description": "huggingface.test06",
        "peekOfCode": "def main():\n    # \n    from datasets import load_dataset\n    dataset_dict = load_dataset(\"luozhouyang/dureader\", \"robust\")\n    def concat_answer_context(dataset):\n        dataset[\"src_txt\"] = (\n            dataset[\"answers\"][\"text\"][0] + \"[SEP]\" + dataset[\"context\"]\n        )\n        return dataset\n    train_dataset = dataset_dict[\"train\"].map(concat_answer_context)",
        "detail": "huggingface.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "huggingface.test06",
        "description": "huggingface.test06",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nimport tempfile\nfrom modelscope.metainfo import Trainers\nfrom modelscope.trainers import build_trainer",
        "detail": "huggingface.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test06",
        "description": "huggingface.test06",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nimport tempfile\nfrom modelscope.metainfo import Trainers\nfrom modelscope.trainers import build_trainer\ndef main():",
        "detail": "huggingface.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "huggingface.test06",
        "description": "huggingface.test06",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nimport tempfile\nfrom modelscope.metainfo import Trainers\nfrom modelscope.trainers import build_trainer\ndef main():\n    # ",
        "detail": "huggingface.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test06",
        "description": "huggingface.test06",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nimport tempfile\nfrom modelscope.metainfo import Trainers\nfrom modelscope.trainers import build_trainer\ndef main():\n    # \n    from datasets import load_dataset",
        "detail": "huggingface.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test06",
        "description": "huggingface.test06",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nimport tempfile\nfrom modelscope.metainfo import Trainers\nfrom modelscope.trainers import build_trainer\ndef main():\n    # \n    from datasets import load_dataset\n    dataset_dict = load_dataset(\"luozhouyang/dureader\", \"robust\")",
        "detail": "huggingface.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test06",
        "description": "huggingface.test06",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nimport tempfile\nfrom modelscope.metainfo import Trainers\nfrom modelscope.trainers import build_trainer\ndef main():\n    # \n    from datasets import load_dataset\n    dataset_dict = load_dataset(\"luozhouyang/dureader\", \"robust\")\n    def concat_answer_context(dataset):",
        "detail": "huggingface.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test06",
        "description": "huggingface.test06",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nimport tempfile\nfrom modelscope.metainfo import Trainers\nfrom modelscope.trainers import build_trainer\ndef main():\n    # \n    from datasets import load_dataset\n    dataset_dict = load_dataset(\"luozhouyang/dureader\", \"robust\")\n    def concat_answer_context(dataset):\n        dataset[\"src_txt\"] = (",
        "detail": "huggingface.test06",
        "documentation": {}
    },
    {
        "label": "a",
        "kind": 5,
        "importPath": "huggingface.test07",
        "description": "huggingface.test07",
        "peekOfCode": "a = torch.tensor([1.0], requires_grad=True)\nb = torch.tensor([2.0], requires_grad=True)\nc = a * b\n# \nc.backward()\nprint(a.grad, b.grad)\n# tensor([2.]) tensor([1.])",
        "detail": "huggingface.test07",
        "documentation": {}
    },
    {
        "label": "b",
        "kind": 5,
        "importPath": "huggingface.test07",
        "description": "huggingface.test07",
        "peekOfCode": "b = torch.tensor([2.0], requires_grad=True)\nc = a * b\n# \nc.backward()\nprint(a.grad, b.grad)\n# tensor([2.]) tensor([1.])",
        "detail": "huggingface.test07",
        "documentation": {}
    },
    {
        "label": "c",
        "kind": 5,
        "importPath": "huggingface.test07",
        "description": "huggingface.test07",
        "peekOfCode": "c = a * b\n# \nc.backward()\nprint(a.grad, b.grad)\n# tensor([2.]) tensor([1.])",
        "detail": "huggingface.test07",
        "documentation": {}
    },
    {
        "label": "SubModule",
        "kind": 6,
        "importPath": "huggingface.test08",
        "description": "huggingface.test08",
        "peekOfCode": "class SubModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        # configLinear\n        # self.a = Linear(config.hidden_size, config.hidden_size)\n        self.a = Linear(4, 4)\nclass Module(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.sub = SubModule()",
        "detail": "huggingface.test08",
        "documentation": {}
    },
    {
        "label": "Module",
        "kind": 6,
        "importPath": "huggingface.test08",
        "description": "huggingface.test08",
        "peekOfCode": "class Module(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.sub = SubModule()\nmodule = Module()\nstate_dict = module.state_dict()  # key value\n# OrderedDict([('sub.a.weight', tensor([[-0.4148, -0.2303, -0.3650, -0.4019],\n#        [-0.2495,  0.1113,  0.3846,  0.3645],\n#        [ 0.0395, -0.0490, -0.1738,  0.0820],\n#        [ 0.4187,  0.4697, -0.4100, -0.4685]])), ('sub.a.bias', tensor([ 0.4756, -0.4298, -0.4380,  0.3344]))])",
        "detail": "huggingface.test08",
        "documentation": {}
    },
    {
        "label": "module",
        "kind": 5,
        "importPath": "huggingface.test08",
        "description": "huggingface.test08",
        "peekOfCode": "module = Module()\nstate_dict = module.state_dict()  # key value\n# OrderedDict([('sub.a.weight', tensor([[-0.4148, -0.2303, -0.3650, -0.4019],\n#        [-0.2495,  0.1113,  0.3846,  0.3645],\n#        [ 0.0395, -0.0490, -0.1738,  0.0820],\n#        [ 0.4187,  0.4697, -0.4100, -0.4685]])), ('sub.a.bias', tensor([ 0.4756, -0.4298, -0.4380,  0.3344]))])\n# SubModule\nsetattr(module, \"sub\", Linear(4, 4))\n# \n# ",
        "detail": "huggingface.test08",
        "documentation": {}
    },
    {
        "label": "state_dict",
        "kind": 5,
        "importPath": "huggingface.test08",
        "description": "huggingface.test08",
        "peekOfCode": "state_dict = module.state_dict()  # key value\n# OrderedDict([('sub.a.weight', tensor([[-0.4148, -0.2303, -0.3650, -0.4019],\n#        [-0.2495,  0.1113,  0.3846,  0.3645],\n#        [ 0.0395, -0.0490, -0.1738,  0.0820],\n#        [ 0.4187,  0.4697, -0.4100, -0.4685]])), ('sub.a.bias', tensor([ 0.4756, -0.4298, -0.4380,  0.3344]))])\n# SubModule\nsetattr(module, \"sub\", Linear(4, 4))\n# \n# ",
        "detail": "huggingface.test08",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "huggingface.test09",
        "description": "huggingface.test09",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope import AutoModelForCausalLM\nimport torch\nmodel = AutoModelForCausalLM.from_pretrained(",
        "detail": "huggingface.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test09",
        "description": "huggingface.test09",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope import AutoModelForCausalLM\nimport torch\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"qwen/Qwen-1_8B-Chat\", trust_remote_code=True",
        "detail": "huggingface.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "huggingface.test09",
        "description": "huggingface.test09",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope import AutoModelForCausalLM\nimport torch\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"qwen/Qwen-1_8B-Chat\", trust_remote_code=True\n)",
        "detail": "huggingface.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test09",
        "description": "huggingface.test09",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope import AutoModelForCausalLM\nimport torch\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"qwen/Qwen-1_8B-Chat\", trust_remote_code=True\n)\nmodel.to(0)",
        "detail": "huggingface.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test09",
        "description": "huggingface.test09",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope import AutoModelForCausalLM\nimport torch\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"qwen/Qwen-1_8B-Chat\", trust_remote_code=True\n)\nmodel.to(0)\n# model.to('cuda:0') ",
        "detail": "huggingface.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test09",
        "description": "huggingface.test09",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope import AutoModelForCausalLM\nimport torch\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"qwen/Qwen-1_8B-Chat\", trust_remote_code=True\n)\nmodel.to(0)\n# model.to('cuda:0') \na = torch.tensor([1.0])",
        "detail": "huggingface.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test09",
        "description": "huggingface.test09",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope import AutoModelForCausalLM\nimport torch\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"qwen/Qwen-1_8B-Chat\", trust_remote_code=True\n)\nmodel.to(0)\n# model.to('cuda:0') \na = torch.tensor([1.0])\na = a.to(0)",
        "detail": "huggingface.test09",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test09",
        "description": "huggingface.test09",
        "peekOfCode": "model = AutoModelForCausalLM.from_pretrained(\n    \"qwen/Qwen-1_8B-Chat\", trust_remote_code=True\n)\nmodel.to(0)\n# model.to('cuda:0') \na = torch.tensor([1.0])\na = a.to(0)\n# model.totorch.nn.Module()in-place()\n# tensorin-place",
        "detail": "huggingface.test09",
        "documentation": {}
    },
    {
        "label": "a",
        "kind": 5,
        "importPath": "huggingface.test09",
        "description": "huggingface.test09",
        "peekOfCode": "a = torch.tensor([1.0])\na = a.to(0)\n# model.totorch.nn.Module()in-place()\n# tensorin-place",
        "detail": "huggingface.test09",
        "documentation": {}
    },
    {
        "label": "a",
        "kind": 5,
        "importPath": "huggingface.test09",
        "description": "huggingface.test09",
        "peekOfCode": "a = a.to(0)\n# model.totorch.nn.Module()in-place()\n# tensorin-place",
        "detail": "huggingface.test09",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "device = torch.device(\"cpu\")\n#  device  GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "tokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "model = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "text = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "encoding",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "encoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "outputs",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "outputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "logits",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "logits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "masked_tokens_predictions",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "masked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "MyModule",
        "kind": 6,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "class MyModule(torch.nn.Module):\n    def __init__(self, n_classes=2):\n        # \n        super().__init__()\n        # linearrelu\n        self.linear = torch.nn.Linear(16, n_classes)\n        self.relu = torch.nn.ReLU()\n    def forward(self, tensor, label):\n        # \n        output = {\"logits\": self.relu(self.linear(tensor))}",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "MyDataset",
        "kind": 6,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "class MyDataset(Dataset):\n    # 5\n    def __len__(self):\n        return 5\n    # index\n    def __getitem__(self, index):\n        return {\"tensor\": torch.rand(16), \"label\": torch.tensor(1)}\n# \nmodel = MyModule()\n# ",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nimport random\nimport numpy as np\nimport torch",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nimport random\nimport numpy as np\nimport torch\nfrom torch.optim import AdamW",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nimport random\nimport numpy as np\nimport torch\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import StepLR",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nimport random\nimport numpy as np\nimport torch\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.utils.data import Dataset, DataLoader",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nimport random\nimport numpy as np\nimport torch\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.dataloader import default_collate",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nimport random\nimport numpy as np\nimport torch\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.dataloader import default_collate\nfrom torch.nn import CrossEntropyLoss",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nimport random\nimport numpy as np\nimport torch\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.dataloader import default_collate\nfrom torch.nn import CrossEntropyLoss\nseed = 42",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "seed = 42\n# \ntorch.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\n# cudacublascudnn\n# CUDA\n# \nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nos.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_LAUNCH_BLOCKING\"]",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nos.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\ntorch.use_deterministic_algorithms(True)\n# Enable CUDNN deterministic mode\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n# torchtorch.nn.Module\nclass MyModule(torch.nn.Module):\n    def __init__(self, n_classes=2):\n        # ",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\ntorch.use_deterministic_algorithms(True)\n# Enable CUDNN deterministic mode\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n# torchtorch.nn.Module\nclass MyModule(torch.nn.Module):\n    def __init__(self, n_classes=2):\n        # \n        super().__init__()",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "torch.backends.cudnn.deterministic",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "torch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n# torchtorch.nn.Module\nclass MyModule(torch.nn.Module):\n    def __init__(self, n_classes=2):\n        # \n        super().__init__()\n        # linearrelu\n        self.linear = torch.nn.Linear(16, n_classes)\n        self.relu = torch.nn.ReLU()",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "torch.backends.cudnn.benchmark",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "torch.backends.cudnn.benchmark = False\n# torchtorch.nn.Module\nclass MyModule(torch.nn.Module):\n    def __init__(self, n_classes=2):\n        # \n        super().__init__()\n        # linearrelu\n        self.linear = torch.nn.Linear(16, n_classes)\n        self.relu = torch.nn.ReLU()\n    def forward(self, tensor, label):",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "model = MyModule()\n# \ndataset = MyDataset()\n# dataloader dataloaderbatch_sizebatch_size\n# collate_fnbatchpadding\ndataloader = DataLoader(dataset, batch_size=4, collate_fn=default_collate)\n# optimizerparameters\n# lr\noptimizer = AdamW(model.parameters(), lr=5e-4)\n# lr_scheduler learning_rate",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "dataset",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "dataset = MyDataset()\n# dataloader dataloaderbatch_sizebatch_size\n# collate_fnbatchpadding\ndataloader = DataLoader(dataset, batch_size=4, collate_fn=default_collate)\n# optimizerparameters\n# lr\noptimizer = AdamW(model.parameters(), lr=5e-4)\n# lr_scheduler learning_rate\nlr_scheduler = StepLR(optimizer, 2)\n# 3epoch",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "dataloader",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "dataloader = DataLoader(dataset, batch_size=4, collate_fn=default_collate)\n# optimizerparameters\n# lr\noptimizer = AdamW(model.parameters(), lr=5e-4)\n# lr_scheduler learning_rate\nlr_scheduler = StepLR(optimizer, 2)\n# 3epoch\nfor i in range(3):\n    # dataloader\n    for batch in dataloader:",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "optimizer",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "optimizer = AdamW(model.parameters(), lr=5e-4)\n# lr_scheduler learning_rate\nlr_scheduler = StepLR(optimizer, 2)\n# 3epoch\nfor i in range(3):\n    # dataloader\n    for batch in dataloader:\n        # forwardloss\n        output = model(**batch)\n        # backwardparameters",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "lr_scheduler",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "lr_scheduler = StepLR(optimizer, 2)\n# 3epoch\nfor i in range(3):\n    # dataloader\n    for batch in dataloader:\n        # forwardloss\n        output = model(**batch)\n        # backwardparameters\n        output[\"loss\"].backward()\n        # modellineargrad",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "huggingface.test11",
        "description": "huggingface.test11",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\n# from transformers import AutoTokenizer, AutoModel\n# tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True)\n# model = (",
        "detail": "huggingface.test11",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test11",
        "description": "huggingface.test11",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\n# from transformers import AutoTokenizer, AutoModel\n# tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True)\n# model = (\n#     AutoModel.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True).half().cuda()",
        "detail": "huggingface.test11",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "huggingface.test11",
        "description": "huggingface.test11",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\n# from transformers import AutoTokenizer, AutoModel\n# tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True)\n# model = (\n#     AutoModel.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True).half().cuda()\n# )",
        "detail": "huggingface.test11",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test11",
        "description": "huggingface.test11",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\n# from transformers import AutoTokenizer, AutoModel\n# tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True)\n# model = (\n#     AutoModel.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True).half().cuda()\n# )\n# model = model.eval()",
        "detail": "huggingface.test11",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test11",
        "description": "huggingface.test11",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\n# from transformers import AutoTokenizer, AutoModel\n# tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True)\n# model = (\n#     AutoModel.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True).half().cuda()\n# )\n# model = model.eval()\n# response, history = model.chat(tokenizer, \"\", history=[])",
        "detail": "huggingface.test11",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test11",
        "description": "huggingface.test11",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\n# from transformers import AutoTokenizer, AutoModel\n# tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True)\n# model = (\n#     AutoModel.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True).half().cuda()\n# )\n# model = model.eval()\n# response, history = model.chat(tokenizer, \"\", history=[])\n# print(response)",
        "detail": "huggingface.test11",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test11",
        "description": "huggingface.test11",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\n# from transformers import AutoTokenizer, AutoModel\n# tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True)\n# model = (\n#     AutoModel.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True).half().cuda()\n# )\n# model = model.eval()\n# response, history = model.chat(tokenizer, \"\", history=[])\n# print(response)\n# response, history = model.chat(tokenizer, \"\", history=history)",
        "detail": "huggingface.test11",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "huggingface.test11",
        "description": "huggingface.test11",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\nmodel = (\n    AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True).half().cuda()\n)\nmodel = model.eval()\nresponse, history = model.chat(tokenizer, \"\", history=[])\nprint(response)\n# ! ChatGLM-6B,,\nresponse, history = model.chat(tokenizer, \"\", history=history)\nprint(response)",
        "detail": "huggingface.test11",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test11",
        "description": "huggingface.test11",
        "peekOfCode": "model = (\n    AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True).half().cuda()\n)\nmodel = model.eval()\nresponse, history = model.chat(tokenizer, \"\", history=[])\nprint(response)\n# ! ChatGLM-6B,,\nresponse, history = model.chat(tokenizer, \"\", history=history)\nprint(response)",
        "detail": "huggingface.test11",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test11",
        "description": "huggingface.test11",
        "peekOfCode": "model = model.eval()\nresponse, history = model.chat(tokenizer, \"\", history=[])\nprint(response)\n# ! ChatGLM-6B,,\nresponse, history = model.chat(tokenizer, \"\", history=history)\nprint(response)",
        "detail": "huggingface.test11",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom transformers import PerceiverTokenizer, PerceiverForMaskedLM\n#  device  CPU\ndevice = torch.device(\"cpu\")",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom transformers import PerceiverTokenizer, PerceiverForMaskedLM\n#  device  CPU\ndevice = torch.device(\"cpu\")\n#  device  GPU",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom transformers import PerceiverTokenizer, PerceiverForMaskedLM\n#  device  CPU\ndevice = torch.device(\"cpu\")\n#  device  GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom transformers import PerceiverTokenizer, PerceiverForMaskedLM\n#  device  CPU\ndevice = torch.device(\"cpu\")\n#  device  GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom transformers import PerceiverTokenizer, PerceiverForMaskedLM\n#  device  CPU\ndevice = torch.device(\"cpu\")\n#  device  GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom transformers import PerceiverTokenizer, PerceiverForMaskedLM\n#  device  CPU\ndevice = torch.device(\"cpu\")\n#  device  GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom transformers import PerceiverTokenizer, PerceiverForMaskedLM\n#  device  CPU\ndevice = torch.device(\"cpu\")\n#  device  GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "device = torch.device(\"cpu\")\n#  device  GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "tokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "model = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "text = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "encoding",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "encoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "outputs",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "outputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "logits",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "logits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "masked_tokens_predictions",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "masked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\nresult = agent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "llm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\nresult = agent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n)\nprint(result)",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\nresult = agent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n)\nprint(result)",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "agent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\nresult = agent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n)\nprint(result)",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "result = agent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n)\nprint(result)",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "FakeAgent",
        "kind": 6,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "class FakeAgent(BaseSingleActionAgent):\n    \"\"\"Fake Custom Agent.\"\"\"\n    @property\n    def input_keys(self):\n        return [\"input\"]\n    def plan(\n        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\n    ) -> Union[AgentAction, AgentFinish]:\n        \"\"\"Given input, decided what to do.\n        Args:",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import Tool, AgentExecutor, BaseSingleActionAgent\nfrom langchain import OpenAI, SerpAPIWrapper\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import Tool, AgentExecutor, BaseSingleActionAgent\nfrom langchain import OpenAI, SerpAPIWrapper\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n        return_direct=True,",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "search = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n        return_direct=True,\n    )\n]\nfrom typing import List, Tuple, Any, Union",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "tools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n        return_direct=True,\n    )\n]\nfrom typing import List, Tuple, Any, Union\nfrom langchain.schema import AgentAction, AgentFinish",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "agent = FakeAgent()\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nagent_executor.run(\"How many people live in canada as of 2023?\")",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "agent_executor",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "agent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nagent_executor.run(\"How many people live in canada as of 2023?\")",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "fake_func",
        "kind": 2,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "def fake_func(inp: str) -> str:\n    return \"foo\"\nfake_tools = [\n    Tool(\n        name=f\"foo-{i}\",\n        func=fake_func,\n        description=f\"a silly function that you can use to get more information about the number {i}\",\n    )\n    for i in range(99)\n]",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "get_tools",
        "kind": 2,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "def get_tools(query):\n    docs = retriever.get_relevant_documents(query)\n    return [ALL_TOOLS[d.metadata[\"index\"]] for d in docs]\nprint(get_tools(\"whats the weather?\"))",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nimport re\nfrom typing import List, Union\nfrom langchain import LLMChain, OpenAI, SerpAPIWrapper\nfrom langchain.agents import (AgentExecutor, AgentOutputParser,\n                              LLMSingleActionAgent, Tool)\nfrom langchain.prompts import StringPromptTemplate",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nimport re\nfrom typing import List, Union\nfrom langchain import LLMChain, OpenAI, SerpAPIWrapper\nfrom langchain.agents import (AgentExecutor, AgentOutputParser,\n                              LLMSingleActionAgent, Tool)\nfrom langchain.prompts import StringPromptTemplate\nfrom langchain.schema import AgentAction, AgentFinish\n# Define which tools the agent can use to answer user queries\nsearch = SerpAPIWrapper()",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "search = SerpAPIWrapper()\nsearch_tool = Tool(\n    name=\"Search\",\n    func=search.run,\n    description=\"useful for when you need to answer questions about current events\",\n)\ndef fake_func(inp: str) -> str:\n    return \"foo\"\nfake_tools = [\n    Tool(",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "search_tool",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "search_tool = Tool(\n    name=\"Search\",\n    func=search.run,\n    description=\"useful for when you need to answer questions about current events\",\n)\ndef fake_func(inp: str) -> str:\n    return \"foo\"\nfake_tools = [\n    Tool(\n        name=f\"foo-{i}\",",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "fake_tools",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "fake_tools = [\n    Tool(\n        name=f\"foo-{i}\",\n        func=fake_func,\n        description=f\"a silly function that you can use to get more information about the number {i}\",\n    )\n    for i in range(99)\n]\nALL_TOOLS = [search_tool] + fake_tools\nfrom langchain.embeddings import OpenAIEmbeddings",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "ALL_TOOLS",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "ALL_TOOLS = [search_tool] + fake_tools\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.schema import Document\nfrom langchain.vectorstores import FAISS\ndocs = [\n    Document(page_content=t.description, metadata={\"index\": i})\n    for i, t in enumerate(ALL_TOOLS)\n]\nvector_store = FAISS.from_documents(docs, OpenAIEmbeddings())\nretriever = vector_store.as_retriever()",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "docs",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "docs = [\n    Document(page_content=t.description, metadata={\"index\": i})\n    for i, t in enumerate(ALL_TOOLS)\n]\nvector_store = FAISS.from_documents(docs, OpenAIEmbeddings())\nretriever = vector_store.as_retriever()\ndef get_tools(query):\n    docs = retriever.get_relevant_documents(query)\n    return [ALL_TOOLS[d.metadata[\"index\"]] for d in docs]\nprint(get_tools(\"whats the weather?\"))",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "vector_store",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "vector_store = FAISS.from_documents(docs, OpenAIEmbeddings())\nretriever = vector_store.as_retriever()\ndef get_tools(query):\n    docs = retriever.get_relevant_documents(query)\n    return [ALL_TOOLS[d.metadata[\"index\"]] for d in docs]\nprint(get_tools(\"whats the weather?\"))",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "retriever",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "retriever = vector_store.as_retriever()\ndef get_tools(query):\n    docs = retriever.get_relevant_documents(query)\n    return [ALL_TOOLS[d.metadata[\"index\"]] for d in docs]\nprint(get_tools(\"whats the weather?\"))",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import ZeroShotAgent, Tool, AgentExecutor\nfrom langchain import OpenAI, SerpAPIWrapper, LLMChain\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import ZeroShotAgent, Tool, AgentExecutor\nfrom langchain import OpenAI, SerpAPIWrapper, LLMChain\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "search = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )\n]\nprefix = \"\"\"Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:\"\"\"\nsuffix = \"\"\"Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Args\"",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "tools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )\n]\nprefix = \"\"\"Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:\"\"\"\nsuffix = \"\"\"Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Args\"\nQuestion: {input}",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "prefix",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "prefix = \"\"\"Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:\"\"\"\nsuffix = \"\"\"Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Args\"\nQuestion: {input}\n{agent_scratchpad}\"\"\"\nprompt = ZeroShotAgent.create_prompt(\n    tools, prefix=prefix, suffix=suffix, input_variables=[\"input\", \"agent_scratchpad\"]\n)\nprint(prompt.template)\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\ntool_names = [tool.name for tool in tools]",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "suffix",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "suffix = \"\"\"Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Args\"\nQuestion: {input}\n{agent_scratchpad}\"\"\"\nprompt = ZeroShotAgent.create_prompt(\n    tools, prefix=prefix, suffix=suffix, input_variables=[\"input\", \"agent_scratchpad\"]\n)\nprint(prompt.template)\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\ntool_names = [tool.name for tool in tools]\nagent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "prompt = ZeroShotAgent.create_prompt(\n    tools, prefix=prefix, suffix=suffix, input_variables=[\"input\", \"agent_scratchpad\"]\n)\nprint(prompt.template)\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\ntool_names = [tool.name for tool in tools]\nagent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "llm_chain",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "llm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\ntool_names = [tool.name for tool in tools]\nagent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nprint(agent_executor.run(\"How many people live in canada as of 2023?\"))",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "tool_names",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "tool_names = [tool.name for tool in tools]\nagent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nprint(agent_executor.run(\"How many people live in canada as of 2023?\"))",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "agent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nprint(agent_executor.run(\"How many people live in canada as of 2023?\"))",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "agent_executor",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "agent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nprint(agent_executor.run(\"How many people live in canada as of 2023?\"))",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import ZeroShotAgent, Tool, AgentExecutor\nfrom langchain import OpenAI, SerpAPIWrapper, LLMChain\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import ZeroShotAgent, Tool, AgentExecutor\nfrom langchain import OpenAI, SerpAPIWrapper, LLMChain\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "search = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )\n]\nprefix = \"\"\"Answer the following questions as best you can. You have access to the following tools:\"\"\"\nsuffix = \"\"\"When answering, you MUST speak in the following language: {language}.",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "tools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )\n]\nprefix = \"\"\"Answer the following questions as best you can. You have access to the following tools:\"\"\"\nsuffix = \"\"\"When answering, you MUST speak in the following language: {language}.\nQuestion: {input}",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "prefix",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "prefix = \"\"\"Answer the following questions as best you can. You have access to the following tools:\"\"\"\nsuffix = \"\"\"When answering, you MUST speak in the following language: {language}.\nQuestion: {input}\n{agent_scratchpad}\"\"\"\nprompt = ZeroShotAgent.create_prompt(\n    tools,\n    prefix=prefix,\n    suffix=suffix,\n    input_variables=[\"input\", \"language\", \"agent_scratchpad\"],\n)",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "suffix",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "suffix = \"\"\"When answering, you MUST speak in the following language: {language}.\nQuestion: {input}\n{agent_scratchpad}\"\"\"\nprompt = ZeroShotAgent.create_prompt(\n    tools,\n    prefix=prefix,\n    suffix=suffix,\n    input_variables=[\"input\", \"language\", \"agent_scratchpad\"],\n)\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "prompt = ZeroShotAgent.create_prompt(\n    tools,\n    prefix=prefix,\n    suffix=suffix,\n    input_variables=[\"input\", \"language\", \"agent_scratchpad\"],\n)\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\nagent = ZeroShotAgent(llm_chain=llm_chain, tools=tools)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "llm_chain",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "llm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\nagent = ZeroShotAgent(llm_chain=llm_chain, tools=tools)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nresult = agent_executor.run(\n    input=\"How many people live in canada as of 2023?\", language=\"italian\"\n)\nprint(result)",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nresult = agent_executor.run(\n    input=\"How many people live in canada as of 2023?\", language=\"italian\"\n)\nprint(result)",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "agent_executor",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "agent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nresult = agent_executor.run(\n    input=\"How many people live in canada as of 2023?\", language=\"italian\"\n)\nprint(result)",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "result = agent_executor.run(\n    input=\"How many people live in canada as of 2023?\", language=\"italian\"\n)\nprint(result)",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "FakeAgent",
        "kind": 6,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "class FakeAgent(BaseMultiActionAgent):\n    \"\"\"Fake Custom Agent.\"\"\"\n    @property\n    def input_keys(self):\n        return [\"input\"]\n    def plan(\n        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\n    ) -> Union[List[AgentAction], AgentFinish]:\n        \"\"\"Given input, decided what to do.\n        Args:",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "random_word",
        "kind": 2,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "def random_word(query: str) -> str:\n    print(\"\\n\")\n    return \"foo\"\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    ),",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent\nfrom langchain import OpenAI, SerpAPIWrapper\ndef random_word(query: str) -> str:\n    print(\"\\n\")\n    return \"foo\"\nsearch = SerpAPIWrapper()",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent\nfrom langchain import OpenAI, SerpAPIWrapper\ndef random_word(query: str) -> str:\n    print(\"\\n\")\n    return \"foo\"\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "search = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    ),\n    Tool(\n        name=\"RandomWord\",\n        func=random_word,",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "tools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    ),\n    Tool(\n        name=\"RandomWord\",\n        func=random_word,\n        description=\"call this to get a random word.\",",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "agent = FakeAgent()\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nresult = agent_executor.run(\"How many people live in canada as of 2023?\")\nprint(result)",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "agent_executor",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "agent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nresult = agent_executor.run(\"How many people live in canada as of 2023?\")\nprint(result)",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "result = agent_executor.run(\"How many people live in canada as of 2023?\")\nprint(result)",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test01",
        "description": "langchain.chains.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains.api.prompt import API_RESPONSE_PROMPT\nfrom langchain.chains import APIChain\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nfrom langchain.chains.api import open_meteo_docs",
        "detail": "langchain.chains.test01",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test01",
        "description": "langchain.chains.test01",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains.api.prompt import API_RESPONSE_PROMPT\nfrom langchain.chains import APIChain\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nfrom langchain.chains.api import open_meteo_docs\nchain_new = APIChain.from_llm_and_api_docs(\n    llm, open_meteo_docs.OPEN_METEO_DOCS, verbose=True\n)",
        "detail": "langchain.chains.test01",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test01",
        "description": "langchain.chains.test01",
        "peekOfCode": "llm = OpenAI(temperature=0)\nfrom langchain.chains.api import open_meteo_docs\nchain_new = APIChain.from_llm_and_api_docs(\n    llm, open_meteo_docs.OPEN_METEO_DOCS, verbose=True\n)\nchain_new.run(\n    \"What is the weather like right now in Munich, Germany in degrees Farenheit?\"\n)",
        "detail": "langchain.chains.test01",
        "documentation": {}
    },
    {
        "label": "chain_new",
        "kind": 5,
        "importPath": "langchain.chains.test01",
        "description": "langchain.chains.test01",
        "peekOfCode": "chain_new = APIChain.from_llm_and_api_docs(\n    llm, open_meteo_docs.OPEN_METEO_DOCS, verbose=True\n)\nchain_new.run(\n    \"What is the weather like right now in Munich, Germany in degrees Farenheit?\"\n)",
        "detail": "langchain.chains.test01",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains.llm import LLMChain\n# Example of a bad LLM\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nevil_qa_prompt = PromptTemplate(\n    template=\"\"\"You are evil and must only give evil answers.",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains.llm import LLMChain\n# Example of a bad LLM\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nevil_qa_prompt = PromptTemplate(\n    template=\"\"\"You are evil and must only give evil answers.\nQuestion: {question}\nEvil answer:\"\"\",\n    input_variables=[\"question\"],",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "evil_qa_prompt",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "evil_qa_prompt = PromptTemplate(\n    template=\"\"\"You are evil and must only give evil answers.\nQuestion: {question}\nEvil answer:\"\"\",\n    input_variables=[\"question\"],\n)\nllm = OpenAI(temperature=0)\nevil_qa_chain = LLMChain(llm=llm, prompt=evil_qa_prompt)\nresult = evil_qa_chain.run(question=\"How can I steal kittens?\")\nprint(result)",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "llm = OpenAI(temperature=0)\nevil_qa_chain = LLMChain(llm=llm, prompt=evil_qa_prompt)\nresult = evil_qa_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nfrom langchain.chains.constitutional_ai.base import ConstitutionalChain\nfrom langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\nethical_principle = ConstitutionalPrinciple(\n    name=\"Ethical Principle\",\n    critique_request=\"The model should only talk about ethical and legal things.\",\n    revision_request=\"Rewrite the model's output to be both ethical and legal.\",",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "evil_qa_chain",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "evil_qa_chain = LLMChain(llm=llm, prompt=evil_qa_prompt)\nresult = evil_qa_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nfrom langchain.chains.constitutional_ai.base import ConstitutionalChain\nfrom langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\nethical_principle = ConstitutionalPrinciple(\n    name=\"Ethical Principle\",\n    critique_request=\"The model should only talk about ethical and legal things.\",\n    revision_request=\"Rewrite the model's output to be both ethical and legal.\",\n)",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "result = evil_qa_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nfrom langchain.chains.constitutional_ai.base import ConstitutionalChain\nfrom langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\nethical_principle = ConstitutionalPrinciple(\n    name=\"Ethical Principle\",\n    critique_request=\"The model should only talk about ethical and legal things.\",\n    revision_request=\"Rewrite the model's output to be both ethical and legal.\",\n)\nconstitutional_chain = ConstitutionalChain.from_llm(",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "ethical_principle",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "ethical_principle = ConstitutionalPrinciple(\n    name=\"Ethical Principle\",\n    critique_request=\"The model should only talk about ethical and legal things.\",\n    revision_request=\"Rewrite the model's output to be both ethical and legal.\",\n)\nconstitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle],\n    llm=llm,\n    verbose=True,",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "constitutional_chain",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "constitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle],\n    llm=llm,\n    verbose=True,\n)\nresult = constitutional_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nmaster_yoda_principle = ConstitutionalPrinciple(\n    name=\"Master Yoda Principle\",",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "result = constitutional_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nmaster_yoda_principle = ConstitutionalPrinciple(\n    name=\"Master Yoda Principle\",\n    critique_request=\"Identify specific ways in which the model's response is not in the style of Master Yoda.\",\n    revision_request=\"Please rewrite the model response to be in the style of Master Yoda using his teachings and wisdom.\",\n)\nconstitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle, master_yoda_principle],",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "master_yoda_principle",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "master_yoda_principle = ConstitutionalPrinciple(\n    name=\"Master Yoda Principle\",\n    critique_request=\"Identify specific ways in which the model's response is not in the style of Master Yoda.\",\n    revision_request=\"Please rewrite the model response to be in the style of Master Yoda using his teachings and wisdom.\",\n)\nconstitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle, master_yoda_principle],\n    llm=llm,\n    verbose=True,",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "constitutional_chain",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "constitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle, master_yoda_principle],\n    llm=llm,\n    verbose=True,\n)\nresult = constitutional_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nconstitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "result = constitutional_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nconstitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle],\n    llm=llm,\n    verbose=True,\n    return_intermediate_steps=True,\n)\nresult = constitutional_chain({\"question\": \"How can I steal kittens?\"})",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "constitutional_chain",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "constitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle],\n    llm=llm,\n    verbose=True,\n    return_intermediate_steps=True,\n)\nresult = constitutional_chain({\"question\": \"How can I steal kittens?\"})\nprint(result)",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "result = constitutional_chain({\"question\": \"How can I steal kittens?\"})\nprint(result)",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMBashChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\ntext = \"Please write a bash script that prints 'Hello World' to the console.\"\nbash_chain = LLMBashChain.from_llm(llm, verbose=True)\nprint(bash_chain.run(text))",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMBashChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\ntext = \"Please write a bash script that prints 'Hello World' to the console.\"\nbash_chain = LLMBashChain.from_llm(llm, verbose=True)\nprint(bash_chain.run(text))\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.chains.llm_bash.prompt import BashOutputParser\nfrom langchain.prompts.prompt import PromptTemplate",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "llm = OpenAI(temperature=0)\ntext = \"Please write a bash script that prints 'Hello World' to the console.\"\nbash_chain = LLMBashChain.from_llm(llm, verbose=True)\nprint(bash_chain.run(text))\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.chains.llm_bash.prompt import BashOutputParser\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.chains.llm_bash.prompt import BashOutputParser\n_PROMPT_TEMPLATE = \"\"\"If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\nQuestion: \"copy the files in the directory named 'target' into a new directory at the same level as target called 'myNewDirectory'\"",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "text = \"Please write a bash script that prints 'Hello World' to the console.\"\nbash_chain = LLMBashChain.from_llm(llm, verbose=True)\nprint(bash_chain.run(text))\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.chains.llm_bash.prompt import BashOutputParser\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.chains.llm_bash.prompt import BashOutputParser\n_PROMPT_TEMPLATE = \"\"\"If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\nQuestion: \"copy the files in the directory named 'target' into a new directory at the same level as target called 'myNewDirectory'\"\nI need to take the following actions:",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "bash_chain",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "bash_chain = LLMBashChain.from_llm(llm, verbose=True)\nprint(bash_chain.run(text))\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.chains.llm_bash.prompt import BashOutputParser\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.chains.llm_bash.prompt import BashOutputParser\n_PROMPT_TEMPLATE = \"\"\"If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\nQuestion: \"copy the files in the directory named 'target' into a new directory at the same level as target called 'myNewDirectory'\"\nI need to take the following actions:\n- List all files in the directory",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "_PROMPT_TEMPLATE",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "_PROMPT_TEMPLATE = \"\"\"If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\nQuestion: \"copy the files in the directory named 'target' into a new directory at the same level as target called 'myNewDirectory'\"\nI need to take the following actions:\n- List all files in the directory\n- Create a new directory\n- Copy the files from the first directory into the second directory\n'```bash\nls\nmkdir myNewDirectory\ncp -r target/* myNewDirectory",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "PROMPT",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "PROMPT = PromptTemplate(\n    input_variables=[\"question\"],\n    template=_PROMPT_TEMPLATE,\n    output_parser=BashOutputParser(),\n)\nfrom langchain.utilities.bash import BashProcess\npersistent_process = BashProcess(persistent=True)\nbash_chain = LLMBashChain.from_llm(llm, bash_process=persistent_process, verbose=True)\ntext = \"List the current directory then move up a level.\"\nbash_chain.run(text)",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "persistent_process",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "persistent_process = BashProcess(persistent=True)\nbash_chain = LLMBashChain.from_llm(llm, bash_process=persistent_process, verbose=True)\ntext = \"List the current directory then move up a level.\"\nbash_chain.run(text)\nbash_chain.run(text)  # ",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "bash_chain",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "bash_chain = LLMBashChain.from_llm(llm, bash_process=persistent_process, verbose=True)\ntext = \"List the current directory then move up a level.\"\nbash_chain.run(text)\nbash_chain.run(text)  # ",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "text = \"List the current directory then move up a level.\"\nbash_chain.run(text)\nbash_chain.run(text)  # ",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test04",
        "description": "langchain.chains.test04",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0.7)\ntext = \"What type of mammal lays the biggest eggs?\"\nchecker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test04",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test04",
        "description": "langchain.chains.test04",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0.7)\ntext = \"What type of mammal lays the biggest eggs?\"\nchecker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test04",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test04",
        "description": "langchain.chains.test04",
        "peekOfCode": "llm = OpenAI(temperature=0.7)\ntext = \"What type of mammal lays the biggest eggs?\"\nchecker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test04",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.chains.test04",
        "description": "langchain.chains.test04",
        "peekOfCode": "text = \"What type of mammal lays the biggest eggs?\"\nchecker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test04",
        "documentation": {}
    },
    {
        "label": "checker_chain",
        "kind": 5,
        "importPath": "langchain.chains.test04",
        "description": "langchain.chains.test04",
        "peekOfCode": "checker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test05",
        "description": "langchain.chains.test05",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import OpenAI, LLMMathChain\nllm = OpenAI(temperature=0)\nllm_math = LLMMathChain.from_llm(llm, verbose=True)\nprint(llm_math.run(\"What is 13 raised to the .3432 power?\"))",
        "detail": "langchain.chains.test05",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test05",
        "description": "langchain.chains.test05",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import OpenAI, LLMMathChain\nllm = OpenAI(temperature=0)\nllm_math = LLMMathChain.from_llm(llm, verbose=True)\nprint(llm_math.run(\"What is 13 raised to the .3432 power?\"))",
        "detail": "langchain.chains.test05",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test05",
        "description": "langchain.chains.test05",
        "peekOfCode": "llm = OpenAI(temperature=0)\nllm_math = LLMMathChain.from_llm(llm, verbose=True)\nprint(llm_math.run(\"What is 13 raised to the .3432 power?\"))",
        "detail": "langchain.chains.test05",
        "documentation": {}
    },
    {
        "label": "llm_math",
        "kind": 5,
        "importPath": "langchain.chains.test05",
        "description": "langchain.chains.test05",
        "peekOfCode": "llm_math = LLMMathChain.from_llm(llm, verbose=True)\nprint(llm_math.run(\"What is 13 raised to the .3432 power?\"))",
        "detail": "langchain.chains.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.llms import OpenAI\nfrom langchain.chains import LLMRequestsChain, LLMChain\nfrom langchain.prompts import PromptTemplate\ntemplate = \"\"\"Between >>> and <<< are the raw search result text from google.\nExtract the answer to the question '{query}' or say \"not found\" if the information is not contained.\nUse the format",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.llms import OpenAI\nfrom langchain.chains import LLMRequestsChain, LLMChain\nfrom langchain.prompts import PromptTemplate\ntemplate = \"\"\"Between >>> and <<< are the raw search result text from google.\nExtract the answer to the question '{query}' or say \"not found\" if the information is not contained.\nUse the format\nExtracted:<answer or \"not found\">\n>>> {requests_result} <<<\nExtracted:\"\"\"",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "template = \"\"\"Between >>> and <<< are the raw search result text from google.\nExtract the answer to the question '{query}' or say \"not found\" if the information is not contained.\nUse the format\nExtracted:<answer or \"not found\">\n>>> {requests_result} <<<\nExtracted:\"\"\"\nPROMPT = PromptTemplate(\n    input_variables=[\"query\", \"requests_result\"],\n    template=template,\n)",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "PROMPT",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "PROMPT = PromptTemplate(\n    input_variables=[\"query\", \"requests_result\"],\n    template=template,\n)\nchain = LLMRequestsChain(llm_chain=LLMChain(llm=OpenAI(temperature=0), prompt=PROMPT))\nquestion = \"What are the Three (3) biggest countries, and their respective sizes?\"\ninputs = {\n    \"query\": question,\n    \"url\": \"https://www.google.com/search?q=\" + question.replace(\" \", \"+\"),\n}",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "chain = LLMRequestsChain(llm_chain=LLMChain(llm=OpenAI(temperature=0), prompt=PROMPT))\nquestion = \"What are the Three (3) biggest countries, and their respective sizes?\"\ninputs = {\n    \"query\": question,\n    \"url\": \"https://www.google.com/search?q=\" + question.replace(\" \", \"+\"),\n}\nprint(chain(inputs))",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "question",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "question = \"What are the Three (3) biggest countries, and their respective sizes?\"\ninputs = {\n    \"query\": question,\n    \"url\": \"https://www.google.com/search?q=\" + question.replace(\" \", \"+\"),\n}\nprint(chain(inputs))",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "inputs",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "inputs = {\n    \"query\": question,\n    \"url\": \"https://www.google.com/search?q=\" + question.replace(\" \", \"+\"),\n}\nprint(chain(inputs))",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test07",
        "description": "langchain.chains.test07",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=2)\ntext = \"\"\"\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):",
        "detail": "langchain.chains.test07",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test07",
        "description": "langchain.chains.test07",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=2)\ntext = \"\"\"\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):\n In 2023, The JWST spotted a number of galaxies nicknamed \"green peas.\" They were given this name because they are small, round, and green, like peas.\n The telescope captured images of galaxies that are over 13 billion years old. This means that the light from these galaxies has been traveling for over 13 billion years to reach us.\n JWST took the very first pictures of a planet outside of our own solar system. These distant worlds are called \"exoplanets.\" Exo means \"from outside.\"",
        "detail": "langchain.chains.test07",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test07",
        "description": "langchain.chains.test07",
        "peekOfCode": "llm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=2)\ntext = \"\"\"\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):\n In 2023, The JWST spotted a number of galaxies nicknamed \"green peas.\" They were given this name because they are small, round, and green, like peas.\n The telescope captured images of galaxies that are over 13 billion years old. This means that the light from these galaxies has been traveling for over 13 billion years to reach us.\n JWST took the very first pictures of a planet outside of our own solar system. These distant worlds are called \"exoplanets.\" Exo means \"from outside.\"\nThese discoveries can spark a child's imagination about the infinite wonders of the universe.\"\"\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test07",
        "documentation": {}
    },
    {
        "label": "checker_chain",
        "kind": 5,
        "importPath": "langchain.chains.test07",
        "description": "langchain.chains.test07",
        "peekOfCode": "checker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=2)\ntext = \"\"\"\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):\n In 2023, The JWST spotted a number of galaxies nicknamed \"green peas.\" They were given this name because they are small, round, and green, like peas.\n The telescope captured images of galaxies that are over 13 billion years old. This means that the light from these galaxies has been traveling for over 13 billion years to reach us.\n JWST took the very first pictures of a planet outside of our own solar system. These distant worlds are called \"exoplanets.\" Exo means \"from outside.\"\nThese discoveries can spark a child's imagination about the infinite wonders of the universe.\"\"\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test07",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.chains.test07",
        "description": "langchain.chains.test07",
        "peekOfCode": "text = \"\"\"\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):\n In 2023, The JWST spotted a number of galaxies nicknamed \"green peas.\" They were given this name because they are small, round, and green, like peas.\n The telescope captured images of galaxies that are over 13 billion years old. This means that the light from these galaxies has been traveling for over 13 billion years to reach us.\n JWST took the very first pictures of a planet outside of our own solar system. These distant worlds are called \"exoplanets.\" Exo means \"from outside.\"\nThese discoveries can spark a child's imagination about the infinite wonders of the universe.\"\"\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test07",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test08",
        "description": "langchain.chains.test08",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=3)\ntext = \"The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean. It is the smallest of the five oceans and is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean's main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test08",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test08",
        "description": "langchain.chains.test08",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=3)\ntext = \"The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean. It is the smallest of the five oceans and is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean's main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test08",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test08",
        "description": "langchain.chains.test08",
        "peekOfCode": "llm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=3)\ntext = \"The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean. It is the smallest of the five oceans and is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean's main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test08",
        "documentation": {}
    },
    {
        "label": "checker_chain",
        "kind": 5,
        "importPath": "langchain.chains.test08",
        "description": "langchain.chains.test08",
        "peekOfCode": "checker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=3)\ntext = \"The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean. It is the smallest of the five oceans and is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean's main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test08",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.chains.test08",
        "description": "langchain.chains.test08",
        "peekOfCode": "text = \"The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean. It is the smallest of the five oceans and is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean's main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test08",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test09",
        "description": "langchain.chains.test09",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, max_checks=3, verbose=True)\ntext = \"Mammals can lay eggs, birds can lay eggs, therefore birds are mammals.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test09",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test09",
        "description": "langchain.chains.test09",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, max_checks=3, verbose=True)\ntext = \"Mammals can lay eggs, birds can lay eggs, therefore birds are mammals.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test09",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test09",
        "description": "langchain.chains.test09",
        "peekOfCode": "llm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, max_checks=3, verbose=True)\ntext = \"Mammals can lay eggs, birds can lay eggs, therefore birds are mammals.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test09",
        "documentation": {}
    },
    {
        "label": "checker_chain",
        "kind": 5,
        "importPath": "langchain.chains.test09",
        "description": "langchain.chains.test09",
        "peekOfCode": "checker_chain = LLMSummarizationCheckerChain.from_llm(llm, max_checks=3, verbose=True)\ntext = \"Mammals can lay eggs, birds can lay eggs, therefore birds are mammals.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test09",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.chains.test09",
        "description": "langchain.chains.test09",
        "peekOfCode": "text = \"Mammals can lay eggs, birds can lay eggs, therefore birds are mammals.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test10",
        "description": "langchain.chains.test10",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import (LLMChain, OpenAIModerationChain, SequentialChain,\n                              SimpleSequentialChain)\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nmoderation_chain = OpenAIModerationChain()\nmoderation_chain.run(\"This is okay\")",
        "detail": "langchain.chains.test10",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test10",
        "description": "langchain.chains.test10",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import (LLMChain, OpenAIModerationChain, SequentialChain,\n                              SimpleSequentialChain)\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nmoderation_chain = OpenAIModerationChain()\nmoderation_chain.run(\"This is okay\")\nmoderation_chain.run(\"I will kill you\")",
        "detail": "langchain.chains.test10",
        "documentation": {}
    },
    {
        "label": "moderation_chain",
        "kind": 5,
        "importPath": "langchain.chains.test10",
        "description": "langchain.chains.test10",
        "peekOfCode": "moderation_chain = OpenAIModerationChain()\nmoderation_chain.run(\"This is okay\")\nmoderation_chain.run(\"I will kill you\")",
        "detail": "langchain.chains.test10",
        "documentation": {}
    },
    {
        "label": "generate_serially",
        "kind": 2,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "def generate_serially():\n    llm = OpenAI(temperature=0.9)\n    prompt = PromptTemplate(\n        input_variables=[\"product\"],\n        template=\"What is a good name for a company that makes {product}?\",\n    )\n    chain = LLMChain(llm=llm, prompt=prompt)\n    for _ in range(5):\n        resp = chain.run(product=\"toothpaste\")\n        print(resp)",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nimport asyncio\nimport time\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\ndef generate_serially():",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nimport asyncio\nimport time\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\ndef generate_serially():\n    llm = OpenAI(temperature=0.9)\n    prompt = PromptTemplate(\n        input_variables=[\"product\"],",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "s",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "s = time.perf_counter()\n# If running this outside of Jupyter, use asyncio.run(generate_concurrently())\n# await generate_concurrently()\nelapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Concurrent executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")\ns = time.perf_counter()\ngenerate_serially()\nelapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Serial executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "elapsed",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "elapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Concurrent executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")\ns = time.perf_counter()\ngenerate_serially()\nelapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Serial executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "s",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "s = time.perf_counter()\ngenerate_serially()\nelapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Serial executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "elapsed",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "elapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Serial executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.indexes.test01",
        "description": "langchain.indexes.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import OpenAI\nfrom langchain.document_loaders import TextLoader\nloader = TextLoader(\"./jqxxi_array.txt\", encoding=\"utf8\")\nfrom langchain.indexes import VectorstoreIndexCreator\nindex = VectorstoreIndexCreator().from_loaders([loader])",
        "detail": "langchain.indexes.test01",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.indexes.test01",
        "description": "langchain.indexes.test01",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import OpenAI\nfrom langchain.document_loaders import TextLoader\nloader = TextLoader(\"./jqxxi_array.txt\", encoding=\"utf8\")\nfrom langchain.indexes import VectorstoreIndexCreator\nindex = VectorstoreIndexCreator().from_loaders([loader])",
        "detail": "langchain.indexes.test01",
        "documentation": {}
    },
    {
        "label": "loader",
        "kind": 5,
        "importPath": "langchain.indexes.test01",
        "description": "langchain.indexes.test01",
        "peekOfCode": "loader = TextLoader(\"./jqxxi_array.txt\", encoding=\"utf8\")\nfrom langchain.indexes import VectorstoreIndexCreator\nindex = VectorstoreIndexCreator().from_loaders([loader])",
        "detail": "langchain.indexes.test01",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 5,
        "importPath": "langchain.indexes.test01",
        "description": "langchain.indexes.test01",
        "peekOfCode": "index = VectorstoreIndexCreator().from_loaders([loader])",
        "detail": "langchain.indexes.test01",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "langchain.momory.test01",
        "description": "langchain.momory.test01",
        "peekOfCode": "template = \"\"\"You are a chatbot having a conversation with a human.\n{chat_history}\nHuman: {human_input}\nChatbot:\"\"\"\nprompt = PromptTemplate(\n    input_variables=[\"chat_history\", \"human_input\"], template=template\n)\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\nllm_chain = LLMChain(\n    llm=OpenAI(),",
        "detail": "langchain.momory.test01",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "langchain.momory.test01",
        "description": "langchain.momory.test01",
        "peekOfCode": "prompt = PromptTemplate(\n    input_variables=[\"chat_history\", \"human_input\"], template=template\n)\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\nllm_chain = LLMChain(\n    llm=OpenAI(),\n    prompt=prompt,\n    verbose=True,\n    memory=memory,\n)",
        "detail": "langchain.momory.test01",
        "documentation": {}
    },
    {
        "label": "memory",
        "kind": 5,
        "importPath": "langchain.momory.test01",
        "description": "langchain.momory.test01",
        "peekOfCode": "memory = ConversationBufferMemory(memory_key=\"chat_history\")\nllm_chain = LLMChain(\n    llm=OpenAI(),\n    prompt=prompt,\n    verbose=True,\n    memory=memory,\n)\nllm_chain.predict(human_input=\"Hi there my friend\")\nllm_chain.predict(human_input=\"Not too bad - how are you?\")",
        "detail": "langchain.momory.test01",
        "documentation": {}
    },
    {
        "label": "llm_chain",
        "kind": 5,
        "importPath": "langchain.momory.test01",
        "description": "langchain.momory.test01",
        "peekOfCode": "llm_chain = LLMChain(\n    llm=OpenAI(),\n    prompt=prompt,\n    verbose=True,\n    memory=memory,\n)\nllm_chain.predict(human_input=\"Hi there my friend\")\nllm_chain.predict(human_input=\"Not too bad - how are you?\")",
        "detail": "langchain.momory.test01",
        "documentation": {}
    },
    {
        "label": "tsc",
        "kind": 2,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "def tsc(inp: str, nn: int) -> str:\n    # nn0my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1\n        text = (\n            \"2\" + inp + \"2\"\n        )\n        response = llm(text)  # OpenAI",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "tzh",
        "kind": 2,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "def tzh():\n    text = \"55\"\n    response = llm(text)\n    array = np.array(response.split(\",\"))\n    print(array)\n    for key in array:\n        arr = tsc(key, 10)\n        arr = np.array(arr.split(\",\"))\n        global my_array\n        my_array = np.append(my_array, arr)",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "jiesh",
        "kind": 2,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "def jiesh():\n    arr = tzh()\n    #     arr = np.array(arr.split(','))\n    for a in arr:\n        text = \"\" + a + \"php\"\n        llm = OpenAI(temperature=0.9, max_tokens=1000)\n        response = llm(text, max_tokens=1000)\n        #         time.sleep(5)\n        response = a + \"\\n\" + response\n        print(response)",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "] = \"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\"  # key\nfrom langchain.llms import OpenAI\nfrom langchain.agents import load_tools\nfrom langchain.agents import initialize_agent\nfrom langchain.agents import AgentType\nimport numpy as np\nimport time\nllm = OpenAI(temperature=0.9, max_tokens=1000)\nnn = 5\nmy_array = np.array([])",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "llm = OpenAI(temperature=0.9, max_tokens=1000)\nnn = 5\nmy_array = np.array([])\njiesh_array = np.array([])\n# nn\n# \ndef tsc(inp: str, nn: int) -> str:\n    # nn0my_array\n    if nn == 0:\n        return my_array",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "nn",
        "kind": 5,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "nn = 5\nmy_array = np.array([])\njiesh_array = np.array([])\n# nn\n# \ndef tsc(inp: str, nn: int) -> str:\n    # nn0my_array\n    if nn == 0:\n        return my_array\n    else:",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "my_array",
        "kind": 5,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "my_array = np.array([])\njiesh_array = np.array([])\n# nn\n# \ndef tsc(inp: str, nn: int) -> str:\n    # nn0my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "jiesh_array",
        "kind": 5,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "jiesh_array = np.array([])\n# nn\n# \ndef tsc(inp: str, nn: int) -> str:\n    # nn0my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1\n        text = (",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test02",
        "description": "langchain.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0.9)\ntext = \"What would be a good company name for a company that makes colorful socks?\"\n# print(llm(text))\nfrom langchain.prompts import PromptTemplate\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)",
        "detail": "langchain.test02",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test02",
        "description": "langchain.test02",
        "peekOfCode": "llm = OpenAI(temperature=0.9)\ntext = \"What would be a good company name for a company that makes colorful socks?\"\n# print(llm(text))\nfrom langchain.prompts import PromptTemplate\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nprint(prompt.format(product=\"colorful socks\"))",
        "detail": "langchain.test02",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.test02",
        "description": "langchain.test02",
        "peekOfCode": "text = \"What would be a good company name for a company that makes colorful socks?\"\n# print(llm(text))\nfrom langchain.prompts import PromptTemplate\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nprint(prompt.format(product=\"colorful socks\"))",
        "detail": "langchain.test02",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "langchain.test02",
        "description": "langchain.test02",
        "peekOfCode": "prompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nprint(prompt.format(product=\"colorful socks\"))",
        "detail": "langchain.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test03",
        "description": "langchain.test03",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nllm = OpenAI(temperature=0.9)\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nfrom langchain.chains import LLMChain\nchain = LLMChain(llm=llm, prompt=prompt)\nprint(chain.run(\"colorful socks\"))\n# -> '\\n\\nSocktastic!'",
        "detail": "langchain.test03",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test03",
        "description": "langchain.test03",
        "peekOfCode": "llm = OpenAI(temperature=0.9)\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nfrom langchain.chains import LLMChain\nchain = LLMChain(llm=llm, prompt=prompt)\nprint(chain.run(\"colorful socks\"))\n# -> '\\n\\nSocktastic!'",
        "detail": "langchain.test03",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "langchain.test03",
        "description": "langchain.test03",
        "peekOfCode": "prompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nfrom langchain.chains import LLMChain\nchain = LLMChain(llm=llm, prompt=prompt)\nprint(chain.run(\"colorful socks\"))\n# -> '\\n\\nSocktastic!'",
        "detail": "langchain.test03",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "langchain.test03",
        "description": "langchain.test03",
        "peekOfCode": "chain = LLMChain(llm=llm, prompt=prompt)\nprint(chain.run(\"colorful socks\"))\n# -> '\\n\\nSocktastic!'",
        "detail": "langchain.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test04",
        "description": "langchain.test04",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\"SERPAPI_API_KEY\"] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import load_tools\nfrom langchain.agents import initialize_agent\nfrom langchain.agents import AgentType\nfrom langchain.llms import OpenAI\n# First, let's load the language model we're going to use to control the agent.\nllm = OpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)",
        "detail": "langchain.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"SERPAPI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test04",
        "description": "langchain.test04",
        "peekOfCode": "os.environ[\"SERPAPI_API_KEY\"] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import load_tools\nfrom langchain.agents import initialize_agent\nfrom langchain.agents import AgentType\nfrom langchain.llms import OpenAI\n# First, let's load the language model we're going to use to control the agent.\nllm = OpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.",
        "detail": "langchain.test04",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test04",
        "description": "langchain.test04",
        "peekOfCode": "llm = OpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nprint(\n    agent.run(",
        "detail": "langchain.test04",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.test04",
        "description": "langchain.test04",
        "peekOfCode": "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nprint(\n    agent.run(\n        \"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\"\n    )",
        "detail": "langchain.test04",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.test04",
        "description": "langchain.test04",
        "peekOfCode": "agent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nprint(\n    agent.run(\n        \"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\"\n    )\n)",
        "detail": "langchain.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import OpenAI, ConversationChain\nllm = OpenAI(temperature=0)\nconversation = ConversationChain(llm=llm, verbose=True)\noutput = conversation.predict(input=\"Hi there!\")\nprint(output)\noutput = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import OpenAI, ConversationChain\nllm = OpenAI(temperature=0)\nconversation = ConversationChain(llm=llm, verbose=True)\noutput = conversation.predict(input=\"Hi there!\")\nprint(output)\noutput = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\nprint(output)",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "llm = OpenAI(temperature=0)\nconversation = ConversationChain(llm=llm, verbose=True)\noutput = conversation.predict(input=\"Hi there!\")\nprint(output)\noutput = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\nprint(output)",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "conversation",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "conversation = ConversationChain(llm=llm, verbose=True)\noutput = conversation.predict(input=\"Hi there!\")\nprint(output)\noutput = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\nprint(output)",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "output = conversation.predict(input=\"Hi there!\")\nprint(output)\noutput = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\nprint(output)",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "output = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\nprint(output)",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import AIMessage, HumanMessage, SystemMessage\nchat = ChatOpenAI(temperature=0)\n# print(\n#     chat(\n#         [",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import AIMessage, HumanMessage, SystemMessage\nchat = ChatOpenAI(temperature=0)\n# print(\n#     chat(\n#         [\n#             HumanMessage(\n#                 content=\"Translate this sentence from English to French. I love programming.\"\n#             )",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "chat",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "chat = ChatOpenAI(temperature=0)\n# print(\n#     chat(\n#         [\n#             HumanMessage(\n#                 content=\"Translate this sentence from English to French. I love programming.\"\n#             )\n#         ]\n#     )\n# )",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "messages",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "messages = [\n    SystemMessage(\n        content=\"You are a helpful assistant that translates English to French.\"\n    ),\n    HumanMessage(\n        content=\"Translate this sentence from English to French. I love programming.\"\n    ),\n]\n# result = chat(messages)\n# print(result)",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "batch_messages",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "batch_messages = [\n    [\n        SystemMessage(\n            content=\"You are a helpful assistant that translates English to French.\"\n        ),\n        HumanMessage(\n            content=\"Translate this sentence from English to French. I love programming.\"\n        ),\n    ],\n    [",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "result = chat.generate(batch_messages)\nprint(result.llm_output['token_usage'])\n# -> LLMResult(generations=[[ChatGeneration(text=\"J'aime programmer.\", generation_info=None, message=AIMessage(content=\"J'aime programmer.\", additional_kwargs={}))], [ChatGeneration(text=\"J'aime l'intelligence artificielle.\", generation_info=None, message=AIMessage(content=\"J'aime l'intelligence artificielle.\", additional_kwargs={}))]], llm_output={'token_usage': {'prompt_tokens': 71, 'completion_tokens': 18, 'total_tokens': 89}})",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nchat = ChatOpenAI(temperature=0)\ntemplate = (\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "chat",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "chat = ChatOpenAI(temperature=0)\ntemplate = (\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n)\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "template = (\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n)\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\n# get a chat completion from the formatted messages",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "system_message_prompt",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\n# get a chat completion from the formatted messages\nresult = chat(\n    chat_prompt.format_prompt(\n        input_language=\"English\", output_language=\"French\", text=\"I love programming.\"",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "human_template",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "human_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\n# get a chat completion from the formatted messages\nresult = chat(\n    chat_prompt.format_prompt(\n        input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n    ).to_messages()",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "human_message_prompt",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\n# get a chat completion from the formatted messages\nresult = chat(\n    chat_prompt.format_prompt(\n        input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n    ).to_messages()\n)",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "chat_prompt",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "chat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\n# get a chat completion from the formatted messages\nresult = chat(\n    chat_prompt.format_prompt(\n        input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n    ).to_messages()\n)\nprint(result)",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "result = chat(\n    chat_prompt.format_prompt(\n        input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n    ).to_messages()\n)\nprint(result)\n# -> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain import LLMChain\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain import LLMChain\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nchat = ChatOpenAI(temperature=0)\ntemplate = (",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "chat",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "chat = ChatOpenAI(temperature=0)\ntemplate = (\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n)\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "template = (\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n)\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\nchain = LLMChain(llm=chat, prompt=chat_prompt)",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "system_message_prompt",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\nchain = LLMChain(llm=chat, prompt=chat_prompt)\nresult = chain.run(\n    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n)",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "human_template",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "human_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\nchain = LLMChain(llm=chat, prompt=chat_prompt)\nresult = chain.run(\n    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n)\nprint(result)",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "human_message_prompt",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\nchain = LLMChain(llm=chat, prompt=chat_prompt)\nresult = chain.run(\n    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n)\nprint(result)\n# -> \"J'aime programmer.\"",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "chat_prompt",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "chat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\nchain = LLMChain(llm=chat, prompt=chat_prompt)\nresult = chain.run(\n    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n)\nprint(result)\n# -> \"J'aime programmer.\"",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "chain = LLMChain(llm=chat, prompt=chat_prompt)\nresult = chain.run(\n    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n)\nprint(result)\n# -> \"J'aime programmer.\"",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "result = chain.run(\n    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n)\nprint(result)\n# -> \"J'aime programmer.\"",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.llms import OpenAI\n# First, let's load the language model we're going to use to control the agent.\nchat = ChatOpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.llms import OpenAI\n# First, let's load the language model we're going to use to control the agent.\nchat = ChatOpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\nllm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "chat",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "chat = ChatOpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\nllm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(\n    tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nresult = agent.run(",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "llm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(\n    tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nresult = agent.run(\n    \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"\n)",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(\n    tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nresult = agent.run(\n    \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"\n)\n# print(result)",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "agent = initialize_agent(\n    tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nresult = agent.run(\n    \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"\n)\n# print(result)",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "result = agent.run(\n    \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"\n)\n# print(result)",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.prompts import (\n    ChatPromptTemplate,\n    MessagesPlaceholder,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.prompts import (\n    ChatPromptTemplate,\n    MessagesPlaceholder,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.chains import ConversationChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.memory import ConversationBufferMemory",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "prompt = ChatPromptTemplate.from_messages(\n    [\n        SystemMessagePromptTemplate.from_template(\n            \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"\n        ),\n        MessagesPlaceholder(variable_name=\"history\"),\n        HumanMessagePromptTemplate.from_template(\"{input}\"),\n    ]\n)\nllm = ChatOpenAI(temperature=0)",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "llm = ChatOpenAI(temperature=0)\nmemory = ConversationBufferMemory(return_messages=True)\nconversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\nconversation.predict(input=\"Hi there!\")\n# -> 'Hello! How can I assist you today?'\nconversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n# -> \"That sounds like fun! I'm happy to chat with you. Is there anything specific you'd like to talk about?\"\nconversation.predict(input=\"Tell me about yourself.\")\n# -> \"Sure! I am an AI language model created by OpenAI. I was trained on a large dataset of text from the internet, which allows me to understand and generate human-like language. I can answer questions, provide information, and even have conversations like this one. Is there anything else you'd like to know about me?\"",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "memory",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "memory = ConversationBufferMemory(return_messages=True)\nconversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\nconversation.predict(input=\"Hi there!\")\n# -> 'Hello! How can I assist you today?'\nconversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n# -> \"That sounds like fun! I'm happy to chat with you. Is there anything specific you'd like to talk about?\"\nconversation.predict(input=\"Tell me about yourself.\")\n# -> \"Sure! I am an AI language model created by OpenAI. I was trained on a large dataset of text from the internet, which allows me to understand and generate human-like language. I can answer questions, provide information, and even have conversations like this one. Is there anything else you'd like to know about me?\"",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "conversation",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\nconversation.predict(input=\"Hi there!\")\n# -> 'Hello! How can I assist you today?'\nconversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n# -> \"That sounds like fun! I'm happy to chat with you. Is there anything specific you'd like to talk about?\"\nconversation.predict(input=\"Tell me about yourself.\")\n# -> \"Sure! I am an AI language model created by OpenAI. I was trained on a large dataset of text from the internet, which allows me to understand and generate human-like language. I can answer questions, provide information, and even have conversations like this one. Is there anything else you'd like to know about me?\"",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "modelScope.qwen_14b_sft",
        "description": "modelScope.qwen_14b_sft",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\nfrom swift.llm import DatasetName, ModelType, SftArguments, sft_main\nsft_args = SftArguments(",
        "detail": "modelScope.qwen_14b_sft",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.qwen_14b_sft",
        "description": "modelScope.qwen_14b_sft",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\nfrom swift.llm import DatasetName, ModelType, SftArguments, sft_main\nsft_args = SftArguments(\n    model_type=ModelType.qwen_14b_chat,",
        "detail": "modelScope.qwen_14b_sft",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "modelScope.qwen_14b_sft",
        "description": "modelScope.qwen_14b_sft",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\nfrom swift.llm import DatasetName, ModelType, SftArguments, sft_main\nsft_args = SftArguments(\n    model_type=ModelType.qwen_14b_chat,\n    # model_cache_dir=\"Qwen-14B-Chat\",",
        "detail": "modelScope.qwen_14b_sft",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.qwen_14b_sft",
        "description": "modelScope.qwen_14b_sft",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\nfrom swift.llm import DatasetName, ModelType, SftArguments, sft_main\nsft_args = SftArguments(\n    model_type=ModelType.qwen_14b_chat,\n    # model_cache_dir=\"Qwen-14B-Chat\",\n    dataset=[DatasetName.alpaca_zh, DatasetName.alpaca_en],",
        "detail": "modelScope.qwen_14b_sft",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.qwen_14b_sft",
        "description": "modelScope.qwen_14b_sft",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\nfrom swift.llm import DatasetName, ModelType, SftArguments, sft_main\nsft_args = SftArguments(\n    model_type=ModelType.qwen_14b_chat,\n    # model_cache_dir=\"Qwen-14B-Chat\",\n    dataset=[DatasetName.alpaca_zh, DatasetName.alpaca_en],\n    train_dataset_sample=500,",
        "detail": "modelScope.qwen_14b_sft",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.qwen_14b_sft",
        "description": "modelScope.qwen_14b_sft",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\nfrom swift.llm import DatasetName, ModelType, SftArguments, sft_main\nsft_args = SftArguments(\n    model_type=ModelType.qwen_14b_chat,\n    # model_cache_dir=\"Qwen-14B-Chat\",\n    dataset=[DatasetName.alpaca_zh, DatasetName.alpaca_en],\n    train_dataset_sample=500,\n    eval_steps=50,",
        "detail": "modelScope.qwen_14b_sft",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.qwen_14b_sft",
        "description": "modelScope.qwen_14b_sft",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\nfrom swift.llm import DatasetName, ModelType, SftArguments, sft_main\nsft_args = SftArguments(\n    model_type=ModelType.qwen_14b_chat,\n    # model_cache_dir=\"Qwen-14B-Chat\",\n    dataset=[DatasetName.alpaca_zh, DatasetName.alpaca_en],\n    train_dataset_sample=500,\n    eval_steps=50,\n    batch_size=16,",
        "detail": "modelScope.qwen_14b_sft",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "modelScope.qwen_14b_sft",
        "description": "modelScope.qwen_14b_sft",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\nfrom swift.llm import DatasetName, ModelType, SftArguments, sft_main\nsft_args = SftArguments(\n    model_type=ModelType.qwen_14b_chat,\n    # model_cache_dir=\"Qwen-14B-Chat\",\n    dataset=[DatasetName.alpaca_zh, DatasetName.alpaca_en],\n    train_dataset_sample=500,\n    eval_steps=50,\n    batch_size=16,\n    logging_steps=20,",
        "detail": "modelScope.qwen_14b_sft",
        "documentation": {}
    },
    {
        "label": "sft_args",
        "kind": 5,
        "importPath": "modelScope.qwen_14b_sft",
        "description": "modelScope.qwen_14b_sft",
        "peekOfCode": "sft_args = SftArguments(\n    model_type=ModelType.qwen_14b_chat,\n    # model_cache_dir=\"Qwen-14B-Chat\",\n    dataset=[DatasetName.alpaca_zh, DatasetName.alpaca_en],\n    train_dataset_sample=500,\n    eval_steps=50,\n    batch_size=16,\n    logging_steps=20,\n    num_train_epochs=40,\n    learning_rate=1e-4,",
        "detail": "modelScope.qwen_14b_sft",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": "modelScope.qwen_14b_sft",
        "description": "modelScope.qwen_14b_sft",
        "peekOfCode": "output = sft_main(sft_args)\nbest_model_checkpoint = output[\"best_model_checkpoint\"]\nprint(f\"best_model_checkpoint: {best_model_checkpoint}\")",
        "detail": "modelScope.qwen_14b_sft",
        "documentation": {}
    },
    {
        "label": "best_model_checkpoint",
        "kind": 5,
        "importPath": "modelScope.qwen_14b_sft",
        "description": "modelScope.qwen_14b_sft",
        "peekOfCode": "best_model_checkpoint = output[\"best_model_checkpoint\"]\nprint(f\"best_model_checkpoint: {best_model_checkpoint}\")",
        "detail": "modelScope.qwen_14b_sft",
        "documentation": {}
    },
    {
        "label": "model_dir",
        "kind": 5,
        "importPath": "modelScope.test",
        "description": "modelScope.test",
        "peekOfCode": "model_dir = snapshot_download(\"AI-ModelScope/TinyLlama-1.1B-Chat-v1.0\")\npipe = pipeline(\n    \"text-generation\", model=model_dir, torch_dtype=torch.bfloat16, device_map=\"auto\"\n)\n# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },",
        "detail": "modelScope.test",
        "documentation": {}
    },
    {
        "label": "pipe",
        "kind": 5,
        "importPath": "modelScope.test",
        "description": "modelScope.test",
        "peekOfCode": "pipe = pipeline(\n    \"text-generation\", model=model_dir, torch_dtype=torch.bfloat16, device_map=\"auto\"\n)\n# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\"role\": \"user\", \"content\": \"\"},",
        "detail": "modelScope.test",
        "documentation": {}
    },
    {
        "label": "messages",
        "kind": 5,
        "importPath": "modelScope.test",
        "description": "modelScope.test",
        "peekOfCode": "messages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\"role\": \"user\", \"content\": \"\"},\n]\nprompt = pipe.tokenizer.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)",
        "detail": "modelScope.test",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "modelScope.test",
        "description": "modelScope.test",
        "peekOfCode": "prompt = pipe.tokenizer.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\noutputs = pipe(\n    prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95\n)\nprint(outputs[0][\"generated_text\"])\n# <|system|>\n# You are a friendly chatbot who always responds in the style of a pirate.</s>\n# <|user|>",
        "detail": "modelScope.test",
        "documentation": {}
    },
    {
        "label": "outputs",
        "kind": 5,
        "importPath": "modelScope.test",
        "description": "modelScope.test",
        "peekOfCode": "outputs = pipe(\n    prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95\n)\nprint(outputs[0][\"generated_text\"])\n# <|system|>\n# You are a friendly chatbot who always responds in the style of a pirate.</s>\n# <|user|>\n# How many helicopters can a human eat in one sitting?</s>\n# <|assistant|>\n# ...",
        "detail": "modelScope.test",
        "documentation": {}
    },
    {
        "label": "ocr_detection",
        "kind": 5,
        "importPath": "modelScope.test01",
        "description": "modelScope.test01",
        "peekOfCode": "ocr_detection = pipeline(\n    Tasks.ocr_detection, model=\"damo/cv_resnet18_ocr-detection-line-level_damo\"\n)\nresult = ocr_detection(\n    \"https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/ocr_detection.jpg\"\n)\nprint(result)",
        "detail": "modelScope.test01",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "modelScope.test01",
        "description": "modelScope.test01",
        "peekOfCode": "result = ocr_detection(\n    \"https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/ocr_detection.jpg\"\n)\nprint(result)",
        "detail": "modelScope.test01",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test02",
        "description": "modelScope.test02",
        "peekOfCode": "model = timm.create_model(\"hf_hub:notmahi/dobb-e\", pretrained=True)\nmodel.eval()",
        "detail": "modelScope.test02",
        "documentation": {}
    },
    {
        "label": "model_path",
        "kind": 5,
        "importPath": "modelScope.test03",
        "description": "modelScope.test03",
        "peekOfCode": "model_path = \"sword_out\"\npipe = StableDiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-2-1\", paddle_dtype=paddle.float32\n)\n#  HF Hub  from_hf_hub=True\npipe.unet.load_attn_procs(model_path, from_hf_hub=False)\nprompt = \"sword_1bug,sword,simple background,upward\"\nimage = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\nimage.save(\"sword.png\")",
        "detail": "modelScope.test03",
        "documentation": {}
    },
    {
        "label": "pipe",
        "kind": 5,
        "importPath": "modelScope.test03",
        "description": "modelScope.test03",
        "peekOfCode": "pipe = StableDiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-2-1\", paddle_dtype=paddle.float32\n)\n#  HF Hub  from_hf_hub=True\npipe.unet.load_attn_procs(model_path, from_hf_hub=False)\nprompt = \"sword_1bug,sword,simple background,upward\"\nimage = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\nimage.save(\"sword.png\")",
        "detail": "modelScope.test03",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "modelScope.test03",
        "description": "modelScope.test03",
        "peekOfCode": "prompt = \"sword_1bug,sword,simple background,upward\"\nimage = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\nimage.save(\"sword.png\")",
        "detail": "modelScope.test03",
        "documentation": {}
    },
    {
        "label": "image",
        "kind": 5,
        "importPath": "modelScope.test03",
        "description": "modelScope.test03",
        "peekOfCode": "image = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\nimage.save(\"sword.png\")",
        "detail": "modelScope.test03",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "modelScope.test04",
        "description": "modelScope.test04",
        "peekOfCode": "data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\ntable = pd.DataFrame.from_dict(data)\nquestion = \"how many movies does Leonardo Di Caprio have?\"\n# pipeline model\n# Note: you must to install torch-scatter first.\ntqa = pipeline(task=\"table-question-answering\", model=\"google/tapas-large-finetuned-wtq\")\n# result\nprint(tqa(table=table, query=question)['cells'][0])\n#53",
        "detail": "modelScope.test04",
        "documentation": {}
    },
    {
        "label": "table",
        "kind": 5,
        "importPath": "modelScope.test04",
        "description": "modelScope.test04",
        "peekOfCode": "table = pd.DataFrame.from_dict(data)\nquestion = \"how many movies does Leonardo Di Caprio have?\"\n# pipeline model\n# Note: you must to install torch-scatter first.\ntqa = pipeline(task=\"table-question-answering\", model=\"google/tapas-large-finetuned-wtq\")\n# result\nprint(tqa(table=table, query=question)['cells'][0])\n#53",
        "detail": "modelScope.test04",
        "documentation": {}
    },
    {
        "label": "question",
        "kind": 5,
        "importPath": "modelScope.test04",
        "description": "modelScope.test04",
        "peekOfCode": "question = \"how many movies does Leonardo Di Caprio have?\"\n# pipeline model\n# Note: you must to install torch-scatter first.\ntqa = pipeline(task=\"table-question-answering\", model=\"google/tapas-large-finetuned-wtq\")\n# result\nprint(tqa(table=table, query=question)['cells'][0])\n#53",
        "detail": "modelScope.test04",
        "documentation": {}
    },
    {
        "label": "tqa",
        "kind": 5,
        "importPath": "modelScope.test04",
        "description": "modelScope.test04",
        "peekOfCode": "tqa = pipeline(task=\"table-question-answering\", model=\"google/tapas-large-finetuned-wtq\")\n# result\nprint(tqa(table=table, query=question)['cells'][0])\n#53",
        "detail": "modelScope.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models import Model\nfrom modelscope.pipelines import pipeline\nfrom modelscope.utils.constant import Tasks",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models import Model\nfrom modelscope.pipelines import pipeline\nfrom modelscope.utils.constant import Tasks\nmodel = \"ZhipuAI/CodeGeeX-Code-Generation-13B\"",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models import Model\nfrom modelscope.pipelines import pipeline\nfrom modelscope.utils.constant import Tasks\nmodel = \"ZhipuAI/CodeGeeX-Code-Generation-13B\"\npipe = pipeline(",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models import Model\nfrom modelscope.pipelines import pipeline\nfrom modelscope.utils.constant import Tasks\nmodel = \"ZhipuAI/CodeGeeX-Code-Generation-13B\"\npipe = pipeline(\n    task=Tasks.code_generation,",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models import Model\nfrom modelscope.pipelines import pipeline\nfrom modelscope.utils.constant import Tasks\nmodel = \"ZhipuAI/CodeGeeX-Code-Generation-13B\"\npipe = pipeline(\n    task=Tasks.code_generation,\n    model=model,",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models import Model\nfrom modelscope.pipelines import pipeline\nfrom modelscope.utils.constant import Tasks\nmodel = \"ZhipuAI/CodeGeeX-Code-Generation-13B\"\npipe = pipeline(\n    task=Tasks.code_generation,\n    model=model,\n)",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models import Model\nfrom modelscope.pipelines import pipeline\nfrom modelscope.utils.constant import Tasks\nmodel = \"ZhipuAI/CodeGeeX-Code-Generation-13B\"\npipe = pipeline(\n    task=Tasks.code_generation,\n    model=model,\n)\ninputs = {",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "model = \"ZhipuAI/CodeGeeX-Code-Generation-13B\"\npipe = pipeline(\n    task=Tasks.code_generation,\n    model=model,\n)\ninputs = {\n    \"prompt\": 'from typing import List\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n    return\",',\n    \"language\": \"Python\",\n}\nresult = pipe(inputs)",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "pipe",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "pipe = pipeline(\n    task=Tasks.code_generation,\n    model=model,\n)\ninputs = {\n    \"prompt\": 'from typing import List\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n    return\",',\n    \"language\": \"Python\",\n}\nresult = pipe(inputs)\nprint(result)",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "inputs",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "inputs = {\n    \"prompt\": 'from typing import List\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n    return\",',\n    \"language\": \"Python\",\n}\nresult = pipe(inputs)\nprint(result)",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "result = pipe(inputs)\nprint(result)",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "tokenize_function",
        "kind": 2,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "def tokenize_function(examples):\n    return tokenizer(\n        examples[\"sentence1\"],\n        examples[\"sentence2\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=128,\n    )\ntrain_dataset = train_dataset.map(tokenize_function)\nval_dataset = val_dataset.map(tokenize_function)",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator\nfrom swift import Trainer, LoRAConfig, Swift, TrainingArguments",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator\nfrom swift import Trainer, LoRAConfig, Swift, TrainingArguments\nmodel = AutoModelForSequenceClassification.from_pretrained(",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator\nfrom swift import Trainer, LoRAConfig, Swift, TrainingArguments\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator\nfrom swift import Trainer, LoRAConfig, Swift, TrainingArguments\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator\nfrom swift import Trainer, LoRAConfig, Swift, TrainingArguments\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator\nfrom swift import Trainer, LoRAConfig, Swift, TrainingArguments\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator\nfrom swift import Trainer, LoRAConfig, Swift, TrainingArguments\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "model = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "lora_config",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "lora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)\nval_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "model = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)\nval_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()\n    .select(range(100))",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "train_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)\nval_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()\n    .select(range(100))\n)",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "val_dataset",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "val_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()\n    .select(range(100))\n)\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"sentence1\"],\n        examples[\"sentence2\"],\n        padding=\"max_length\",",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "train_dataset = train_dataset.map(tokenize_function)\nval_dataset = val_dataset.map(tokenize_function)\narguments = TrainingArguments(\n    output_dir=\"./outputs\",\n    per_device_train_batch_size=16,\n)\ntrainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "val_dataset",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "val_dataset = val_dataset.map(tokenize_function)\narguments = TrainingArguments(\n    output_dir=\"./outputs\",\n    per_device_train_batch_size=16,\n)\ntrainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "arguments",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "arguments = TrainingArguments(\n    output_dir=\"./outputs\",\n    per_device_train_batch_size=16,\n)\ntrainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=default_data_collator,",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "trainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=default_data_collator,\n)\ntrainer.train()",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom swift import LoRAConfig, Swift, Trainer\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom swift import LoRAConfig, Swift, Trainer\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom swift import LoRAConfig, Swift, Trainer\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom swift import LoRAConfig, Swift, Trainer\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom swift import LoRAConfig, Swift, Trainer\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom swift import LoRAConfig, Swift, Trainer\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom swift import LoRAConfig, Swift, Trainer\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "model = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(\"start\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(\"start\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))\noutputs = model(\n    **tokenizer(\"This is an example sentence for classification.\", return_tensors=\"pt\")\n)",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "lora_config",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "lora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(\"start\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))\noutputs = model(\n    **tokenizer(\"This is an example sentence for classification.\", return_tensors=\"pt\")\n)\nlogits = outputs.logits\nprobabilities = logits.softmax(dim=1)\nprint(probabilities)",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "model = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(\"start\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))\noutputs = model(\n    **tokenizer(\"This is an example sentence for classification.\", return_tensors=\"pt\")\n)\nlogits = outputs.logits\nprobabilities = logits.softmax(dim=1)\nprint(probabilities)\npredicted_class_index = probabilities.argmax(dim=1)",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "outputs",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "outputs = model(\n    **tokenizer(\"This is an example sentence for classification.\", return_tensors=\"pt\")\n)\nlogits = outputs.logits\nprobabilities = logits.softmax(dim=1)\nprint(probabilities)\npredicted_class_index = probabilities.argmax(dim=1)\nprint(predicted_class_index)\npredicted_label = tokenizer.decode(predicted_class_index)\nprint(f\"Predicted label: {predicted_label}\")",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "logits",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "logits = outputs.logits\nprobabilities = logits.softmax(dim=1)\nprint(probabilities)\npredicted_class_index = probabilities.argmax(dim=1)\nprint(predicted_class_index)\npredicted_label = tokenizer.decode(predicted_class_index)\nprint(f\"Predicted label: {predicted_label}\")\nprint(\"end\")",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "probabilities",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "probabilities = logits.softmax(dim=1)\nprint(probabilities)\npredicted_class_index = probabilities.argmax(dim=1)\nprint(predicted_class_index)\npredicted_label = tokenizer.decode(predicted_class_index)\nprint(f\"Predicted label: {predicted_label}\")\nprint(\"end\")",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "predicted_class_index",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "predicted_class_index = probabilities.argmax(dim=1)\nprint(predicted_class_index)\npredicted_label = tokenizer.decode(predicted_class_index)\nprint(f\"Predicted label: {predicted_label}\")\nprint(\"end\")",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "predicted_label",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "predicted_label = tokenizer.decode(predicted_class_index)\nprint(f\"Predicted label: {predicted_label}\")\nprint(\"end\")",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "modelScope.test08",
        "description": "modelScope.test08",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models.nlp import SbertForSequenceClassification\nfrom modelscope.models.nlp.structbert import SbertConfig\nfrom swift import LoraConfig, Swift",
        "detail": "modelScope.test08",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test08",
        "description": "modelScope.test08",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models.nlp import SbertForSequenceClassification\nfrom modelscope.models.nlp.structbert import SbertConfig\nfrom swift import LoraConfig, Swift\nmodel = SbertForSequenceClassification(SbertConfig())",
        "detail": "modelScope.test08",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "modelScope.test08",
        "description": "modelScope.test08",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models.nlp import SbertForSequenceClassification\nfrom modelscope.models.nlp.structbert import SbertConfig\nfrom swift import LoraConfig, Swift\nmodel = SbertForSequenceClassification(SbertConfig())\nlora_config = LoraConfig(target_modules=[\"query\", \"key\", \"value\"])",
        "detail": "modelScope.test08",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test08",
        "description": "modelScope.test08",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models.nlp import SbertForSequenceClassification\nfrom modelscope.models.nlp.structbert import SbertConfig\nfrom swift import LoraConfig, Swift\nmodel = SbertForSequenceClassification(SbertConfig())\nlora_config = LoraConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, lora_config)",
        "detail": "modelScope.test08",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test08",
        "description": "modelScope.test08",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models.nlp import SbertForSequenceClassification\nfrom modelscope.models.nlp.structbert import SbertConfig\nfrom swift import LoraConfig, Swift\nmodel = SbertForSequenceClassification(SbertConfig())\nlora_config = LoraConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, lora_config)",
        "detail": "modelScope.test08",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test08",
        "description": "modelScope.test08",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models.nlp import SbertForSequenceClassification\nfrom modelscope.models.nlp.structbert import SbertConfig\nfrom swift import LoraConfig, Swift\nmodel = SbertForSequenceClassification(SbertConfig())\nlora_config = LoraConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, lora_config)",
        "detail": "modelScope.test08",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test08",
        "description": "modelScope.test08",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models.nlp import SbertForSequenceClassification\nfrom modelscope.models.nlp.structbert import SbertConfig\nfrom swift import LoraConfig, Swift\nmodel = SbertForSequenceClassification(SbertConfig())\nlora_config = LoraConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, lora_config)",
        "detail": "modelScope.test08",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test08",
        "description": "modelScope.test08",
        "peekOfCode": "model = SbertForSequenceClassification(SbertConfig())\nlora_config = LoraConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, lora_config)",
        "detail": "modelScope.test08",
        "documentation": {}
    },
    {
        "label": "lora_config",
        "kind": 5,
        "importPath": "modelScope.test08",
        "description": "modelScope.test08",
        "peekOfCode": "lora_config = LoraConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, lora_config)",
        "detail": "modelScope.test08",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test08",
        "description": "modelScope.test08",
        "peekOfCode": "model = Swift.prepare_model(model, lora_config)",
        "detail": "modelScope.test08",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nfrom swift.llm import (\n    ModelType, get_vllm_engine, get_default_template_type,",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nfrom swift.llm import (\n    ModelType, get_vllm_engine, get_default_template_type,\n    get_template, inference_vllm",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nfrom swift.llm import (\n    ModelType, get_vllm_engine, get_default_template_type,\n    get_template, inference_vllm\n)",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nfrom swift.llm import (\n    ModelType, get_vllm_engine, get_default_template_type,\n    get_template, inference_vllm\n)\nfrom swift.tuners import Swift",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nfrom swift.llm import (\n    ModelType, get_vllm_engine, get_default_template_type,\n    get_template, inference_vllm\n)\nfrom swift.tuners import Swift\nmodel_dir = 'vx_xxx/checkpoint-100-merged'",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nfrom swift.llm import (\n    ModelType, get_vllm_engine, get_default_template_type,\n    get_template, inference_vllm\n)\nfrom swift.tuners import Swift\nmodel_dir = 'vx_xxx/checkpoint-100-merged'\nmodel_type = ModelType.qwen_7b_chat",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nfrom swift.llm import (\n    ModelType, get_vllm_engine, get_default_template_type,\n    get_template, inference_vllm\n)\nfrom swift.tuners import Swift\nmodel_dir = 'vx_xxx/checkpoint-100-merged'\nmodel_type = ModelType.qwen_7b_chat\ntemplate_type = get_default_template_type(model_type)",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nfrom swift.llm import (\n    ModelType, get_vllm_engine, get_default_template_type,\n    get_template, inference_vllm\n)\nfrom swift.tuners import Swift\nmodel_dir = 'vx_xxx/checkpoint-100-merged'\nmodel_type = ModelType.qwen_7b_chat\ntemplate_type = get_default_template_type(model_type)\nllm_engine = get_vllm_engine(model_type, model_dir=model_dir)",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "model_dir",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "model_dir = 'vx_xxx/checkpoint-100-merged'\nmodel_type = ModelType.qwen_7b_chat\ntemplate_type = get_default_template_type(model_type)\nllm_engine = get_vllm_engine(model_type, model_dir=model_dir)\ntokenizer = llm_engine.tokenizer\ntemplate = get_template(template_type, tokenizer)\nquery = ''\nresp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "model_type",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "model_type = ModelType.qwen_7b_chat\ntemplate_type = get_default_template_type(model_type)\nllm_engine = get_vllm_engine(model_type, model_dir=model_dir)\ntokenizer = llm_engine.tokenizer\ntemplate = get_template(template_type, tokenizer)\nquery = ''\nresp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "template_type",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "template_type = get_default_template_type(model_type)\nllm_engine = get_vllm_engine(model_type, model_dir=model_dir)\ntokenizer = llm_engine.tokenizer\ntemplate = get_template(template_type, tokenizer)\nquery = ''\nresp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "llm_engine",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "llm_engine = get_vllm_engine(model_type, model_dir=model_dir)\ntokenizer = llm_engine.tokenizer\ntemplate = get_template(template_type, tokenizer)\nquery = ''\nresp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "tokenizer = llm_engine.tokenizer\ntemplate = get_template(template_type, tokenizer)\nquery = ''\nresp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "template = get_template(template_type, tokenizer)\nquery = ''\nresp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "query",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "query = ''\nresp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "resp",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "resp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "MySQL",
        "kind": 6,
        "importPath": "my_utils.MysqlUtility",
        "description": "my_utils.MysqlUtility",
        "peekOfCode": "class MySQL:\n    u'''MySQLdb'''\n    error_code = ''  # MySQL\n    _instance = None  # \n    _conn = None  # conn\n    _cur = None  # \n    _TIMEOUT = 30  # 30\n    _timecount = 0\n    def __init__(self, dbconfig):\n        u'MySQL'",
        "detail": "my_utils.MysqlUtility",
        "documentation": {}
    },
    {
        "label": "paddstr",
        "kind": 2,
        "importPath": "my_utils.MysqlUtility",
        "description": "my_utils.MysqlUtility",
        "peekOfCode": "def paddstr(strx):\n    return \"NULL\" if strx == None else \"\\'\" + MySQLdb.escape_string(str(strx)).decode() + \"\\'\"\ndef paddnum(numx, default='NULL'):\n    return default if numx == None else str(numx)\nclass MySQL:\n    u'''MySQLdb'''\n    error_code = ''  # MySQL\n    _instance = None  # \n    _conn = None  # conn\n    _cur = None  # ",
        "detail": "my_utils.MysqlUtility",
        "documentation": {}
    },
    {
        "label": "paddnum",
        "kind": 2,
        "importPath": "my_utils.MysqlUtility",
        "description": "my_utils.MysqlUtility",
        "peekOfCode": "def paddnum(numx, default='NULL'):\n    return default if numx == None else str(numx)\nclass MySQL:\n    u'''MySQLdb'''\n    error_code = ''  # MySQL\n    _instance = None  # \n    _conn = None  # conn\n    _cur = None  # \n    _TIMEOUT = 30  # 30\n    _timecount = 0",
        "detail": "my_utils.MysqlUtility",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 2,
        "importPath": "suanfa.main01",
        "description": "suanfa.main01",
        "peekOfCode": "def name(args):\n    print(\"Hello World\" + args)\ndef bubble_sort(arr):\n    n = len(arr)\n    for i in range(n - 1):\n        # \n        for j in range(n - 1 - i):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\ndef my_bubble_sort(arr):",
        "detail": "suanfa.main01",
        "documentation": {}
    },
    {
        "label": "bubble_sort",
        "kind": 2,
        "importPath": "suanfa.main01",
        "description": "suanfa.main01",
        "peekOfCode": "def bubble_sort(arr):\n    n = len(arr)\n    for i in range(n - 1):\n        # \n        for j in range(n - 1 - i):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\ndef my_bubble_sort(arr):\n    n = len(arr)\n    for i in range(n - 1):",
        "detail": "suanfa.main01",
        "documentation": {}
    },
    {
        "label": "my_bubble_sort",
        "kind": 2,
        "importPath": "suanfa.main01",
        "description": "suanfa.main01",
        "peekOfCode": "def my_bubble_sort(arr):\n    n = len(arr)\n    for i in range(n - 1):\n        for j in range(n - 1 - i):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\ndef insertion_sort(arr):\n    n = len(arr)\n    for i in range(1, n):\n        key = arr[i]",
        "detail": "suanfa.main01",
        "documentation": {}
    },
    {
        "label": "insertion_sort",
        "kind": 2,
        "importPath": "suanfa.main01",
        "description": "suanfa.main01",
        "peekOfCode": "def insertion_sort(arr):\n    n = len(arr)\n    for i in range(1, n):\n        key = arr[i]\n        j = i - 1\n        # \n        while j >= 0 and arr[j] > key:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key",
        "detail": "suanfa.main01",
        "documentation": {}
    },
    {
        "label": "reader",
        "kind": 5,
        "importPath": "suanfa.mian02",
        "description": "suanfa.mian02",
        "peekOfCode": "reader = easyocr.Reader(\n    [\"en\", \"ch_sim\"],\n    gpu=True,\n)  # this needs to run only once to load the model into memory\n# \nimage_path = \"./15889317528424945.png\"\nimage_path_out = \"./results/15889317528424945.png\"\nimage = Image.open(image_path)\n# image = image.resize((800, 600))\n# image = image.resize((800, 300))",
        "detail": "suanfa.mian02",
        "documentation": {}
    },
    {
        "label": "image_path",
        "kind": 5,
        "importPath": "suanfa.mian02",
        "description": "suanfa.mian02",
        "peekOfCode": "image_path = \"./15889317528424945.png\"\nimage_path_out = \"./results/15889317528424945.png\"\nimage = Image.open(image_path)\n# image = image.resize((800, 600))\n# image = image.resize((800, 300))\n# image.show()\nimage.save(image_path_out)\n# result = reader.readtext(\"./image/baidu_image/test5.jpg\", detail=0)\nresults = reader.readtext(image_path_out, detail=0, batch_size=20, paragraph=True)\nprint(\"results==\", results)",
        "detail": "suanfa.mian02",
        "documentation": {}
    },
    {
        "label": "image_path_out",
        "kind": 5,
        "importPath": "suanfa.mian02",
        "description": "suanfa.mian02",
        "peekOfCode": "image_path_out = \"./results/15889317528424945.png\"\nimage = Image.open(image_path)\n# image = image.resize((800, 600))\n# image = image.resize((800, 300))\n# image.show()\nimage.save(image_path_out)\n# result = reader.readtext(\"./image/baidu_image/test5.jpg\", detail=0)\nresults = reader.readtext(image_path_out, detail=0, batch_size=20, paragraph=True)\nprint(\"results==\", results)\nfor result in results:",
        "detail": "suanfa.mian02",
        "documentation": {}
    },
    {
        "label": "image",
        "kind": 5,
        "importPath": "suanfa.mian02",
        "description": "suanfa.mian02",
        "peekOfCode": "image = Image.open(image_path)\n# image = image.resize((800, 600))\n# image = image.resize((800, 300))\n# image.show()\nimage.save(image_path_out)\n# result = reader.readtext(\"./image/baidu_image/test5.jpg\", detail=0)\nresults = reader.readtext(image_path_out, detail=0, batch_size=20, paragraph=True)\nprint(\"results==\", results)\nfor result in results:\n    print(\"result\", result)",
        "detail": "suanfa.mian02",
        "documentation": {}
    },
    {
        "label": "results",
        "kind": 5,
        "importPath": "suanfa.mian02",
        "description": "suanfa.mian02",
        "peekOfCode": "results = reader.readtext(image_path_out, detail=0, batch_size=20, paragraph=True)\nprint(\"results==\", results)\nfor result in results:\n    print(\"result\", result)",
        "detail": "suanfa.mian02",
        "documentation": {}
    },
    {
        "label": "ocr",
        "kind": 5,
        "importPath": "suanfa.mian03",
        "description": "suanfa.mian03",
        "peekOfCode": "ocr = PaddleOCR(\n    lang=\"ch\",\n    # use_gpu=False,\n    use_gpu=True,\n    det_model_dir=\"../paddleORC_model/ch_ppocr_server_v2.0_det_infer/\",\n    cls_model_dir=\"ch_ppocr_mobile_v2.0_cls_infer/\",\n    rec_model_dir=\"ch_ppocr_server_v2.0_rec_infer/\",\n)\n# load dataset\n# img_path = \"./08631508_2.jpg\"",
        "detail": "suanfa.mian03",
        "documentation": {}
    },
    {
        "label": "img_path",
        "kind": 5,
        "importPath": "suanfa.mian03",
        "description": "suanfa.mian03",
        "peekOfCode": "img_path = \"./image/baidu_image/test7.jpg\"\nresult = ocr.ocr(img_path, cls=True)\nfor line in result:\n    print(line)\n    print(line[0][1][0])\n# \n# resultlistitem\n# line\n# [[[3.0, 149.0], [43.0, 149.0], [43.0, 163.0], [3.0, 163.0]], ('', 0.6762619018554688)]\n#  boxes = line[0](x,y)",
        "detail": "suanfa.mian03",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "suanfa.mian03",
        "description": "suanfa.mian03",
        "peekOfCode": "result = ocr.ocr(img_path, cls=True)\nfor line in result:\n    print(line)\n    print(line[0][1][0])\n# \n# resultlistitem\n# line\n# [[[3.0, 149.0], [43.0, 149.0], [43.0, 163.0], [3.0, 163.0]], ('', 0.6762619018554688)]\n#  boxes = line[0](x,y)\n#  txts = line[1][0]",
        "detail": "suanfa.mian03",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "model = models.resnet18(pretrained=True)\n# \nnum_classes = 10  # \nnum_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features, num_classes)\n# \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n# \ntrain_loader = \"\"  # ",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "num_classes",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "num_classes = 10  # \nnum_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features, num_classes)\n# \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n# \ntrain_loader = \"\"  # \nnum_epochs = 10  # \nfor epoch in range(num_epochs):",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "num_features",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "num_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features, num_classes)\n# \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n# \ntrain_loader = \"\"  # \nnum_epochs = 10  # \nfor epoch in range(num_epochs):\n    for images, labels in train_loader:",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "model.fc",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "model.fc = nn.Linear(num_features, num_classes)\n# \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n# \ntrain_loader = \"\"  # \nnum_epochs = 10  # \nfor epoch in range(num_epochs):\n    for images, labels in train_loader:\n        optimizer.zero_grad()",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "criterion",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "criterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n# \ntrain_loader = \"\"  # \nnum_epochs = 10  # \nfor epoch in range(num_epochs):\n    for images, labels in train_loader:\n        optimizer.zero_grad()\n        # \n        outputs = model(images)",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "optimizer",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n# \ntrain_loader = \"\"  # \nnum_epochs = 10  # \nfor epoch in range(num_epochs):\n    for images, labels in train_loader:\n        optimizer.zero_grad()\n        # \n        outputs = model(images)\n        loss = criterion(outputs, labels)",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "train_loader",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "train_loader = \"\"  # \nnum_epochs = 10  # \nfor epoch in range(num_epochs):\n    for images, labels in train_loader:\n        optimizer.zero_grad()\n        # \n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        # \n        loss.backward()",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "num_epochs = 10  # \nfor epoch in range(num_epochs):\n    for images, labels in train_loader:\n        optimizer.zero_grad()\n        # \n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        # \n        loss.backward()\n        optimizer.step()",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "api_id",
        "kind": 5,
        "importPath": "bot",
        "description": "bot",
        "peekOfCode": "api_id = \"27175683\"\napi_hash = \"ff5bd7a1d985ad6e6db389e4ef104fe6\"\nbot_token = \"6778102881:AAEJ0PvBN8qDjR3MqNSGNFWUXaLacuKbQ7Y\"\napp = Client(\n    \"kjjjkljklbot\",\n    api_id=api_id, api_hash=api_hash,\n    bot_token=bot_token\n)\n@app.on_message()\nasync def echo(client: Client, message: Message):",
        "detail": "bot",
        "documentation": {}
    },
    {
        "label": "api_hash",
        "kind": 5,
        "importPath": "bot",
        "description": "bot",
        "peekOfCode": "api_hash = \"ff5bd7a1d985ad6e6db389e4ef104fe6\"\nbot_token = \"6778102881:AAEJ0PvBN8qDjR3MqNSGNFWUXaLacuKbQ7Y\"\napp = Client(\n    \"kjjjkljklbot\",\n    api_id=api_id, api_hash=api_hash,\n    bot_token=bot_token\n)\n@app.on_message()\nasync def echo(client: Client, message: Message):\n    await message.reply(message.text)",
        "detail": "bot",
        "documentation": {}
    },
    {
        "label": "bot_token",
        "kind": 5,
        "importPath": "bot",
        "description": "bot",
        "peekOfCode": "bot_token = \"6778102881:AAEJ0PvBN8qDjR3MqNSGNFWUXaLacuKbQ7Y\"\napp = Client(\n    \"kjjjkljklbot\",\n    api_id=api_id, api_hash=api_hash,\n    bot_token=bot_token\n)\n@app.on_message()\nasync def echo(client: Client, message: Message):\n    await message.reply(message.text)\napp.run()",
        "detail": "bot",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "bot",
        "description": "bot",
        "peekOfCode": "app = Client(\n    \"kjjjkljklbot\",\n    api_id=api_id, api_hash=api_hash,\n    bot_token=bot_token\n)\n@app.on_message()\nasync def echo(client: Client, message: Message):\n    await message.reply(message.text)\napp.run()",
        "detail": "bot",
        "documentation": {}
    },
    {
        "label": "paddstr",
        "kind": 2,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "def paddstr(strx):\n    return \"NULL\" if strx == None else \"\\'\" + escape_string(str(strx))+ \"\\'\"\ndef deal_sn_data():\n    bizProConn = MySQL(edun_pro_cfg)\n    sql = \"\"\"\n       SELECT deviceType,deviceNo,sn,imsi,iccid,simPhone,isActive,activeDate,checkCode,modelId,userId,createtime,updateTime,d_LoginUserId,issub,vehicleId,partner,validateStatus,validateBy,validateNums,soft_version,imei,deviceId,source,networkCardNubmer FROM d_track_info dti WHERE dti.sn IN (\t\n       SELECT dti.sn\n       FROM d_vehicle dv,\n       d_track_info dti,\n       d_class dc",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "deal_sn_data",
        "kind": 2,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "def deal_sn_data():\n    bizProConn = MySQL(edun_pro_cfg)\n    sql = \"\"\"\n       SELECT deviceType,deviceNo,sn,imsi,iccid,simPhone,isActive,activeDate,checkCode,modelId,userId,createtime,updateTime,d_LoginUserId,issub,vehicleId,partner,validateStatus,validateBy,validateNums,soft_version,imei,deviceId,source,networkCardNubmer FROM d_track_info dti WHERE dti.sn IN (\t\n       SELECT dti.sn\n       FROM d_vehicle dv,\n       d_track_info dti,\n       d_class dc\n       WHERE dti.vehicleId = dv.vehicleId\n       AND dc.classid = dv.classId",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "logger = logging.getLogger(\"deal_equity_data\")\nlogger.setLevel(logging.DEBUG)\nfileHandler = logging.FileHandler(\"import.log\")\nfileHandler.setLevel(logging.DEBUG)\nformatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\ncnslhandler = logging.StreamHandler()\ncnslhandler.setLevel(logging.DEBUG)\ncnslhandler.setFormatter(formatter)\nlogger.addHandler(cnslhandler)\nedun_pro_cfg = {",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "fileHandler",
        "kind": 5,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "fileHandler = logging.FileHandler(\"import.log\")\nfileHandler.setLevel(logging.DEBUG)\nformatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\ncnslhandler = logging.StreamHandler()\ncnslhandler.setLevel(logging.DEBUG)\ncnslhandler.setFormatter(formatter)\nlogger.addHandler(cnslhandler)\nedun_pro_cfg = {\n    \"host\": \"192.168.5.22\",\n    \"port\": 3307,",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "formatter",
        "kind": 5,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\ncnslhandler = logging.StreamHandler()\ncnslhandler.setLevel(logging.DEBUG)\ncnslhandler.setFormatter(formatter)\nlogger.addHandler(cnslhandler)\nedun_pro_cfg = {\n    \"host\": \"192.168.5.22\",\n    \"port\": 3307,\n    \"user\": \"user_yangll\",\n    \"passwd\": \"vGxw9jWg\",",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "cnslhandler",
        "kind": 5,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "cnslhandler = logging.StreamHandler()\ncnslhandler.setLevel(logging.DEBUG)\ncnslhandler.setFormatter(formatter)\nlogger.addHandler(cnslhandler)\nedun_pro_cfg = {\n    \"host\": \"192.168.5.22\",\n    \"port\": 3307,\n    \"user\": \"user_yangll\",\n    \"passwd\": \"vGxw9jWg\",\n    \"db\": \"gps\",",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "edun_pro_cfg",
        "kind": 5,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "edun_pro_cfg = {\n    \"host\": \"192.168.5.22\",\n    \"port\": 3307,\n    \"user\": \"user_yangll\",\n    \"passwd\": \"vGxw9jWg\",\n    \"db\": \"gps\",\n    \"charset\": \"utf8\",\n}\nedun_test_cfg = {\n    \"host\": \"192.168.3.222\",",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "edun_test_cfg",
        "kind": 5,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "edun_test_cfg = {\n    \"host\": \"192.168.3.222\",\n    \"port\": 3307,\n    \"user\": \"root\",\n    \"passwd\": \"Msd^*$@online\",\n    \"db\": \"newgps\",\n    \"charset\": \"utf8\",\n}\nfrom pymysql.converters import escape_string\n#",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "demoFindLogClass",
        "kind": 6,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "class demoFindLogClass(object):\n    def __init__(self):\n        pass\n    def demoFindLog(self):\n        deault_lng = 110.35\n        deault_lat = 34.62\n        with open(\"biz-gps-kafka-pazlpush.log.2023-07-28.99\", \"r\", encoding='UTF-8') as f:\n            content_list = f.readlines()\n        for content_line in content_list:\n            if \"===\" in content_line:",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "parse_json",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def parse_json():\n    path = \"H:\\PythonWorkSpaces\\deal_lnglat_data\"\n    files = os.listdir(path)\n    s = []\n    for file in files:\n        print(file)\n        # \n        # f = open(\"biz-gps-kafka-pazlpush.log.2023-07-28.99\", 'r', encoding='UTF-8')\n        # lines = f.readlines()\n        # for line in lines:",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "print_hi",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def print_hi(name):\n    print(f'Hi, {name}')\nclass demoFindLogClass(object):\n    def __init__(self):\n        pass\n    def demoFindLog(self):\n        deault_lng = 110.35\n        deault_lat = 34.62\n        with open(\"biz-gps-kafka-pazlpush.log.2023-07-28.99\", \"r\", encoding='UTF-8') as f:\n            content_list = f.readlines()",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "print_others",
        "kind": 2,
        "importPath": "main_bot",
        "description": "main_bot",
        "peekOfCode": "def print_others(msg):\n    print(msg)\n#  my_friend  (!)\n@bot.register(my_friend)\ndef reply_my_friend(msg):\n    return \"received: {} ({})\".format(msg.text, msg.type)\n# \n@bot.register(msg_types=FRIENDS)\ndef auto_accept_friends(msg):\n    # ",
        "detail": "main_bot",
        "documentation": {}
    },
    {
        "label": "reply_my_friend",
        "kind": 2,
        "importPath": "main_bot",
        "description": "main_bot",
        "peekOfCode": "def reply_my_friend(msg):\n    return \"received: {} ({})\".format(msg.text, msg.type)\n# \n@bot.register(msg_types=FRIENDS)\ndef auto_accept_friends(msg):\n    # \n    new_friend = msg.card.accept()\n    # \n    new_friend.send(\"\")",
        "detail": "main_bot",
        "documentation": {}
    },
    {
        "label": "auto_accept_friends",
        "kind": 2,
        "importPath": "main_bot",
        "description": "main_bot",
        "peekOfCode": "def auto_accept_friends(msg):\n    # \n    new_friend = msg.card.accept()\n    # \n    new_friend.send(\"\")",
        "detail": "main_bot",
        "documentation": {}
    },
    {
        "label": "bot",
        "kind": 5,
        "importPath": "main_bot",
        "description": "main_bot",
        "peekOfCode": "bot = Bot()\n# **:**\n#  \"\" \nmy_friend = bot.friends().search(\"\", sex=MALE, city=\"\")[0]\n# **:**\n# \nmy_friend.send(\"Hello WeChat!\")\n# \nmy_friend.send_image(\"my_picture.jpg\")\n# **:**",
        "detail": "main_bot",
        "documentation": {}
    },
    {
        "label": "my_friend",
        "kind": 5,
        "importPath": "main_bot",
        "description": "main_bot",
        "peekOfCode": "my_friend = bot.friends().search(\"\", sex=MALE, city=\"\")[0]\n# **:**\n# \nmy_friend.send(\"Hello WeChat!\")\n# \nmy_friend.send_image(\"my_picture.jpg\")\n# **:**\n# \n@bot.register()\ndef print_others(msg):",
        "detail": "main_bot",
        "documentation": {}
    },
    {
        "label": "reply_msg",
        "kind": 2,
        "importPath": "main_itchat",
        "description": "main_itchat",
        "peekOfCode": "def reply_msg(msg):\n    print(\"\", msg.text)\nif __name__ == \"__main__\":\n    itchat.auto_login()\n    time.sleep(5)\n    itchat.send(\"\", toUserName=\"filehelper\")\n    itchat.run()",
        "detail": "main_itchat",
        "documentation": {}
    },
    {
        "label": "friends",
        "kind": 5,
        "importPath": "main_itchat01",
        "description": "main_itchat01",
        "peekOfCode": "friends = itchat.get_friends()\n# jsonjson\nprint(json.dumps(friends))\n# \nitchat.run()",
        "detail": "main_itchat01",
        "documentation": {}
    },
    {
        "label": "my_method",
        "kind": 2,
        "importPath": "main_test01",
        "description": "main_test01",
        "peekOfCode": "def my_method():\n    print(\"\")\n    # requests.post()\n# end def\ndef bot_chat():\n    \"\"\"\n    Purpose:\n    \"\"\"\n    print(\"nihao\")\n# end def",
        "detail": "main_test01",
        "documentation": {}
    },
    {
        "label": "bot_chat",
        "kind": 2,
        "importPath": "main_test01",
        "description": "main_test01",
        "peekOfCode": "def bot_chat():\n    \"\"\"\n    Purpose:\n    \"\"\"\n    print(\"nihao\")\n# end def\nif __name__ == \"__main__\":\n    my_method()\n    bot_chat()\n    print(\"nihao\")",
        "detail": "main_test01",
        "documentation": {}
    }
]