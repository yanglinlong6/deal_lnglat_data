[
    {
        "label": "AssistantAgent",
        "importPath": "autogen",
        "description": "autogen",
        "isExtraImport": true,
        "detail": "autogen",
        "documentation": {}
    },
    {
        "label": "UserProxyAgent",
        "importPath": "autogen",
        "description": "autogen",
        "isExtraImport": true,
        "detail": "autogen",
        "documentation": {}
    },
    {
        "label": "config_list_from_json",
        "importPath": "autogen",
        "description": "autogen",
        "isExtraImport": true,
        "detail": "autogen",
        "documentation": {}
    },
    {
        "label": "UserProxyAgent",
        "importPath": "autogen",
        "description": "autogen",
        "isExtraImport": true,
        "detail": "autogen",
        "documentation": {}
    },
    {
        "label": "ConversableAgent",
        "importPath": "autogen",
        "description": "autogen",
        "isExtraImport": true,
        "detail": "autogen",
        "documentation": {}
    },
    {
        "label": "config_list_from_json",
        "importPath": "autogen",
        "description": "autogen",
        "isExtraImport": true,
        "detail": "autogen",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "TextLoader",
        "importPath": "langchain.document_loaders",
        "description": "langchain.document_loaders",
        "isExtraImport": true,
        "detail": "langchain.document_loaders",
        "documentation": {}
    },
    {
        "label": "TextLoader",
        "importPath": "langchain.document_loaders",
        "description": "langchain.document_loaders",
        "isExtraImport": true,
        "detail": "langchain.document_loaders",
        "documentation": {}
    },
    {
        "label": "CharacterTextSplitter",
        "importPath": "langchain.text_splitter",
        "description": "langchain.text_splitter",
        "isExtraImport": true,
        "detail": "langchain.text_splitter",
        "documentation": {}
    },
    {
        "label": "OpenAIEmbeddings",
        "importPath": "langchain.embeddings.openai",
        "description": "langchain.embeddings.openai",
        "isExtraImport": true,
        "detail": "langchain.embeddings.openai",
        "documentation": {}
    },
    {
        "label": "OpenAIEmbeddings",
        "importPath": "langchain.embeddings.openai",
        "description": "langchain.embeddings.openai",
        "isExtraImport": true,
        "detail": "langchain.embeddings.openai",
        "documentation": {}
    },
    {
        "label": "DeepLake",
        "importPath": "langchain.vectorstores",
        "description": "langchain.vectorstores",
        "isExtraImport": true,
        "detail": "langchain.vectorstores",
        "documentation": {}
    },
    {
        "label": "DeepLake",
        "importPath": "langchain.vectorstores",
        "description": "langchain.vectorstores",
        "isExtraImport": true,
        "detail": "langchain.vectorstores",
        "documentation": {}
    },
    {
        "label": "FAISS",
        "importPath": "langchain.vectorstores",
        "description": "langchain.vectorstores",
        "isExtraImport": true,
        "detail": "langchain.vectorstores",
        "documentation": {}
    },
    {
        "label": "ConversationalRetrievalChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "APIChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMBashChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMCheckerChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMRequestsChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMSummarizationCheckerChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMSummarizationCheckerChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMSummarizationCheckerChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "OpenAIModerationChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "SequentialChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "SimpleSequentialChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "RetrievalQA",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "ConversationChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain.chat_models",
        "description": "langchain.chat_models",
        "isExtraImport": true,
        "detail": "langchain.chat_models",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain.chat_models",
        "description": "langchain.chat_models",
        "isExtraImport": true,
        "detail": "langchain.chat_models",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain.chat_models",
        "description": "langchain.chat_models",
        "isExtraImport": true,
        "detail": "langchain.chat_models",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain.chat_models",
        "description": "langchain.chat_models",
        "isExtraImport": true,
        "detail": "langchain.chat_models",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain.chat_models",
        "description": "langchain.chat_models",
        "isExtraImport": true,
        "detail": "langchain.chat_models",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain.chat_models",
        "description": "langchain.chat_models",
        "isExtraImport": true,
        "detail": "langchain.chat_models",
        "documentation": {}
    },
    {
        "label": "AutoModelForSequenceClassification",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "MsDataset",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoModelForSequenceClassification",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "snapshot_download",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoModelForSequenceClassification",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "MsDataset",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoModelForSequenceClassification",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "default_data_collator",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "PerceiverTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "PerceiverForMaskedLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "PerceiverTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "PerceiverForMaskedLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "pipeline",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "pipeline",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "default_data_collator",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "LoRAConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "TrainingArguments",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "LoRAConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "LoRAConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "TrainingArguments",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "LoRAConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "LoraConfig",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift",
        "description": "swift",
        "isExtraImport": true,
        "detail": "swift",
        "documentation": {}
    },
    {
        "label": "read_config",
        "importPath": "modelscope.utils.hub",
        "description": "modelscope.utils.hub",
        "isExtraImport": true,
        "detail": "modelscope.utils.hub",
        "documentation": {}
    },
    {
        "label": "MsDataset",
        "importPath": "modelscope.msdatasets",
        "description": "modelscope.msdatasets",
        "isExtraImport": true,
        "detail": "modelscope.msdatasets",
        "documentation": {}
    },
    {
        "label": "build_trainer",
        "importPath": "modelscope.trainers",
        "description": "modelscope.trainers",
        "isExtraImport": true,
        "detail": "modelscope.trainers",
        "documentation": {}
    },
    {
        "label": "build_trainer",
        "importPath": "modelscope.trainers",
        "description": "modelscope.trainers",
        "isExtraImport": true,
        "detail": "modelscope.trainers",
        "documentation": {}
    },
    {
        "label": "tempfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tempfile",
        "description": "tempfile",
        "detail": "tempfile",
        "documentation": {}
    },
    {
        "label": "Trainers",
        "importPath": "modelscope.metainfo",
        "description": "modelscope.metainfo",
        "isExtraImport": true,
        "detail": "modelscope.metainfo",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Linear",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "CrossEntropyLoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "torch.optim",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.optim",
        "description": "torch.optim",
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "torch.optim",
        "description": "torch.optim",
        "isExtraImport": true,
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "StepLR",
        "importPath": "torch.optim.lr_scheduler",
        "description": "torch.optim.lr_scheduler",
        "isExtraImport": true,
        "detail": "torch.optim.lr_scheduler",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "default_collate",
        "importPath": "torch.utils.data.dataloader",
        "description": "torch.utils.data.dataloader",
        "isExtraImport": true,
        "detail": "torch.utils.data.dataloader",
        "documentation": {}
    },
    {
        "label": "AgentType",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "initialize_agent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "load_tools",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "Tool",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "BaseSingleActionAgent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentOutputParser",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "LLMSingleActionAgent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "Tool",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "ZeroShotAgent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "Tool",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "ZeroShotAgent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "Tool",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "Tool",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "BaseMultiActionAgent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "load_tools",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "initialize_agent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentType",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "load_tools",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "initialize_agent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentType",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentType",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "initialize_agent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "load_tools",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "langchain",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "langchain",
        "description": "langchain",
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "SerpAPIWrapper",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "SerpAPIWrapper",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "SerpAPIWrapper",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "SerpAPIWrapper",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "SerpAPIWrapper",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "LLMMathChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "ConversationChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain",
        "description": "langchain",
        "isExtraImport": true,
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "AgentAction",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AgentFinish",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AgentAction",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AgentFinish",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AgentAction",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AgentFinish",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "AIMessage",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "SystemMessage",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "StringPromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "MessagesPlaceholder",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "SystemMessagePromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "HumanMessagePromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "OpenAIEmbeddings",
        "importPath": "langchain.embeddings",
        "description": "langchain.embeddings",
        "isExtraImport": true,
        "detail": "langchain.embeddings",
        "documentation": {}
    },
    {
        "label": "API_RESPONSE_PROMPT",
        "importPath": "langchain.chains.api.prompt",
        "description": "langchain.chains.api.prompt",
        "isExtraImport": true,
        "detail": "langchain.chains.api.prompt",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts.prompt",
        "description": "langchain.prompts.prompt",
        "isExtraImport": true,
        "detail": "langchain.prompts.prompt",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts.prompt",
        "description": "langchain.prompts.prompt",
        "isExtraImport": true,
        "detail": "langchain.prompts.prompt",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts.prompt",
        "description": "langchain.prompts.prompt",
        "isExtraImport": true,
        "detail": "langchain.prompts.prompt",
        "documentation": {}
    },
    {
        "label": "open_meteo_docs",
        "importPath": "langchain.chains.api",
        "description": "langchain.chains.api",
        "isExtraImport": true,
        "detail": "langchain.chains.api",
        "documentation": {}
    },
    {
        "label": "LLMChain",
        "importPath": "langchain.chains.llm",
        "description": "langchain.chains.llm",
        "isExtraImport": true,
        "detail": "langchain.chains.llm",
        "documentation": {}
    },
    {
        "label": "ConstitutionalChain",
        "importPath": "langchain.chains.constitutional_ai.base",
        "description": "langchain.chains.constitutional_ai.base",
        "isExtraImport": true,
        "detail": "langchain.chains.constitutional_ai.base",
        "documentation": {}
    },
    {
        "label": "ConstitutionalPrinciple",
        "importPath": "langchain.chains.constitutional_ai.models",
        "description": "langchain.chains.constitutional_ai.models",
        "isExtraImport": true,
        "detail": "langchain.chains.constitutional_ai.models",
        "documentation": {}
    },
    {
        "label": "BashOutputParser",
        "importPath": "langchain.chains.llm_bash.prompt",
        "description": "langchain.chains.llm_bash.prompt",
        "isExtraImport": true,
        "detail": "langchain.chains.llm_bash.prompt",
        "documentation": {}
    },
    {
        "label": "BashOutputParser",
        "importPath": "langchain.chains.llm_bash.prompt",
        "description": "langchain.chains.llm_bash.prompt",
        "isExtraImport": true,
        "detail": "langchain.chains.llm_bash.prompt",
        "documentation": {}
    },
    {
        "label": "BashProcess",
        "importPath": "langchain.utilities.bash",
        "description": "langchain.utilities.bash",
        "isExtraImport": true,
        "detail": "langchain.utilities.bash",
        "documentation": {}
    },
    {
        "label": "asyncio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "asyncio",
        "description": "asyncio",
        "detail": "asyncio",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "VectorstoreIndexCreator",
        "importPath": "langchain.indexes",
        "description": "langchain.indexes",
        "isExtraImport": true,
        "detail": "langchain.indexes",
        "documentation": {}
    },
    {
        "label": "ConversationBufferMemory",
        "importPath": "langchain.memory",
        "description": "langchain.memory",
        "isExtraImport": true,
        "detail": "langchain.memory",
        "documentation": {}
    },
    {
        "label": "ConversationBufferMemory",
        "importPath": "langchain.memory",
        "description": "langchain.memory",
        "isExtraImport": true,
        "detail": "langchain.memory",
        "documentation": {}
    },
    {
        "label": "openai",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "openai",
        "description": "openai",
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "SystemMessagePromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "HumanMessagePromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "SystemMessagePromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "HumanMessagePromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "DatasetName",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "ModelType",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "SftArguments",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "sft_main",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "ModelType",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "get_vllm_engine",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "get_default_template_type",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "get_template",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "inference_vllm",
        "importPath": "swift.llm",
        "description": "swift.llm",
        "isExtraImport": true,
        "detail": "swift.llm",
        "documentation": {}
    },
    {
        "label": "pipeline",
        "importPath": "modelscope.pipelines",
        "description": "modelscope.pipelines",
        "isExtraImport": true,
        "detail": "modelscope.pipelines",
        "documentation": {}
    },
    {
        "label": "pipeline",
        "importPath": "modelscope.pipelines",
        "description": "modelscope.pipelines",
        "isExtraImport": true,
        "detail": "modelscope.pipelines",
        "documentation": {}
    },
    {
        "label": "Tasks",
        "importPath": "modelscope.utils.constant",
        "description": "modelscope.utils.constant",
        "isExtraImport": true,
        "detail": "modelscope.utils.constant",
        "documentation": {}
    },
    {
        "label": "Tasks",
        "importPath": "modelscope.utils.constant",
        "description": "modelscope.utils.constant",
        "isExtraImport": true,
        "detail": "modelscope.utils.constant",
        "documentation": {}
    },
    {
        "label": "timm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "timm",
        "description": "timm",
        "detail": "timm",
        "documentation": {}
    },
    {
        "label": "StableDiffusionPipeline",
        "importPath": "ppdiffusers",
        "description": "ppdiffusers",
        "isExtraImport": true,
        "detail": "ppdiffusers",
        "documentation": {}
    },
    {
        "label": "paddle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "paddle",
        "description": "paddle",
        "detail": "paddle",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "Model",
        "importPath": "modelscope.models",
        "description": "modelscope.models",
        "isExtraImport": true,
        "detail": "modelscope.models",
        "documentation": {}
    },
    {
        "label": "SbertForSequenceClassification",
        "importPath": "modelscope.models.nlp",
        "description": "modelscope.models.nlp",
        "isExtraImport": true,
        "detail": "modelscope.models.nlp",
        "documentation": {}
    },
    {
        "label": "SbertConfig",
        "importPath": "modelscope.models.nlp.structbert",
        "description": "modelscope.models.nlp.structbert",
        "isExtraImport": true,
        "detail": "modelscope.models.nlp.structbert",
        "documentation": {}
    },
    {
        "label": "Swift",
        "importPath": "swift.tuners",
        "description": "swift.tuners",
        "isExtraImport": true,
        "detail": "swift.tuners",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "MySQLdb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "MySQLdb",
        "description": "MySQLdb",
        "detail": "MySQLdb",
        "documentation": {}
    },
    {
        "label": "easyocr",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "easyocr",
        "description": "easyocr",
        "detail": "easyocr",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "PaddleOCR",
        "importPath": "paddleocr",
        "description": "paddleocr",
        "isExtraImport": true,
        "detail": "paddleocr",
        "documentation": {}
    },
    {
        "label": "draw_ocr",
        "importPath": "paddleocr",
        "description": "paddleocr",
        "isExtraImport": true,
        "detail": "paddleocr",
        "documentation": {}
    },
    {
        "label": "models",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "Client",
        "importPath": "pyrogram",
        "description": "pyrogram",
        "isExtraImport": true,
        "detail": "pyrogram",
        "documentation": {}
    },
    {
        "label": "Message",
        "importPath": "pyrogram.types",
        "description": "pyrogram.types",
        "isExtraImport": true,
        "detail": "pyrogram.types",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "Decimal",
        "importPath": "decimal",
        "description": "decimal",
        "isExtraImport": true,
        "detail": "decimal",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "dateutil.relativedelta",
        "description": "dateutil.relativedelta",
        "isExtraImport": true,
        "detail": "dateutil.relativedelta",
        "documentation": {}
    },
    {
        "label": "MySQL",
        "importPath": "my_utils.MysqlUtility",
        "description": "my_utils.MysqlUtility",
        "isExtraImport": true,
        "detail": "my_utils.MysqlUtility",
        "documentation": {}
    },
    {
        "label": "groupby",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "escape_string",
        "importPath": "pymysql.converters",
        "description": "pymysql.converters",
        "isExtraImport": true,
        "detail": "pymysql.converters",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "wxpy",
        "description": "wxpy",
        "isExtraImport": true,
        "detail": "wxpy",
        "documentation": {}
    },
    {
        "label": "itchat",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "itchat",
        "description": "itchat",
        "detail": "itchat",
        "documentation": {}
    },
    {
        "label": "itchat,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "itchat.",
        "description": "itchat.",
        "detail": "itchat.",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "config_list",
        "kind": 5,
        "importPath": "autogen.test01",
        "description": "autogen.test01",
        "peekOfCode": "config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")\n# You can also set config_list directly as a list, for example, config_list = [{'model': 'gpt-4', 'api_key': '<your OpenAI API key here>'},]\nassistant = AssistantAgent(\"assistant\", llm_config={\"config_list\": config_list})\nuser_proxy = UserProxyAgent(\n    \"user_proxy\", code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False}\n)  # IMPORTANT: set to True to run code in docker, recommended\nuser_proxy.initiate_chat(\n    assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\"\n)\n# This initiates an automated chat between the two agents to solve the task",
        "detail": "autogen.test01",
        "documentation": {}
    },
    {
        "label": "assistant",
        "kind": 5,
        "importPath": "autogen.test01",
        "description": "autogen.test01",
        "peekOfCode": "assistant = AssistantAgent(\"assistant\", llm_config={\"config_list\": config_list})\nuser_proxy = UserProxyAgent(\n    \"user_proxy\", code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False}\n)  # IMPORTANT: set to True to run code in docker, recommended\nuser_proxy.initiate_chat(\n    assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\"\n)\n# This initiates an automated chat between the two agents to solve the task",
        "detail": "autogen.test01",
        "documentation": {}
    },
    {
        "label": "user_proxy",
        "kind": 5,
        "importPath": "autogen.test01",
        "description": "autogen.test01",
        "peekOfCode": "user_proxy = UserProxyAgent(\n    \"user_proxy\", code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False}\n)  # IMPORTANT: set to True to run code in docker, recommended\nuser_proxy.initiate_chat(\n    assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\"\n)\n# This initiates an automated chat between the two agents to solve the task",
        "detail": "autogen.test01",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "autogen.test02",
        "description": "autogen.test02",
        "peekOfCode": "def main():\n    # Load LLM inference endpoints from an env variable or a file\n    # See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints\n    # and OAI_CONFIG_LIST_sample.\n    # For example, if you have created a OAI_CONFIG_LIST file in the current working directory, that file will be used.\n    config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST.json\")\n    # Create the agent that uses the LLM.\n    assistant = ConversableAgent(\"agent\", llm_config={\"config_list\": config_list})\n    # Create the agent that represents the user in the conversation.\n    user_proxy = UserProxyAgent(\"user\", code_execution_config=False)",
        "detail": "autogen.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "autogen.test02",
        "description": "autogen.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\ndef main():\n    # Load LLM inference endpoints from an env variable or a file\n    # See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints\n    # and OAI_CONFIG_LIST_sample.\n    # For example, if you have created a OAI_CONFIG_LIST file in the current working directory, that file will be used.\n    config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST.json\")\n    # Create the agent that uses the LLM.\n    assistant = ConversableAgent(\"agent\", llm_config={\"config_list\": config_list})\n    # Create the agent that represents the user in the conversation.",
        "detail": "autogen.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"ACTIVELOOP_TOKEN\"\n] = \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwNjA4Mjc0NywiZXhwIjoxNzM4MzA5OTM0fQ.eyJpZCI6InlhbmdsaW5sb25nNiJ9.SNbGhqyihTBsiGhk6SOobDtvWwl0mxW-8YIokUGlvZdu2Z4-db4bnMWK_LIexTwZHHQeYmdHNlqE3gulUULTeg\"\n# Please manually enter OpenAI Key\nfrom langchain.document_loaders import TextLoader\n# root_dir = \"../\"\nroot_dir = \"H:\\\\WorkSpaces\\\\temp\\\\develop\\\\car-service-ordercenter\"",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"ACTIVELOOP_TOKEN\"\n] = \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwNjA4Mjc0NywiZXhwIjoxNzM4MzA5OTM0fQ.eyJpZCI6InlhbmdsaW5sb25nNiJ9.SNbGhqyihTBsiGhk6SOobDtvWwl0mxW-8YIokUGlvZdu2Z4-db4bnMWK_LIexTwZHHQeYmdHNlqE3gulUULTeg\"\n# Please manually enter OpenAI Key\nfrom langchain.document_loaders import TextLoader\n# root_dir = \"../\"\nroot_dir = \"H:\\\\WorkSpaces\\\\temp\\\\develop\\\\car-service-ordercenter\"\ndocs = []\nfor dirpath, dirnames, filenames in os.walk(root_dir):",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "] = \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwNjA4Mjc0NywiZXhwIjoxNzM4MzA5OTM0fQ.eyJpZCI6InlhbmdsaW5sb25nNiJ9.SNbGhqyihTBsiGhk6SOobDtvWwl0mxW-8YIokUGlvZdu2Z4-db4bnMWK_LIexTwZHHQeYmdHNlqE3gulUULTeg\"\n# Please manually enter OpenAI Key\nfrom langchain.document_loaders import TextLoader\n# root_dir = \"../\"\nroot_dir = \"H:\\\\WorkSpaces\\\\temp\\\\develop\\\\car-service-ordercenter\"\ndocs = []\nfor dirpath, dirnames, filenames in os.walk(root_dir):\n    print(\"filenames\", filenames)\n    for file in filenames:\n        # if file.endswith(\".py\") and \"/.venv/\" not in dirpath:",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "root_dir",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "root_dir = \"H:\\\\WorkSpaces\\\\temp\\\\develop\\\\car-service-ordercenter\"\ndocs = []\nfor dirpath, dirnames, filenames in os.walk(root_dir):\n    print(\"filenames\", filenames)\n    for file in filenames:\n        # if file.endswith(\".py\") and \"/.venv/\" not in dirpath:\n        if (\n            file.endswith(\".java\")\n            or file.endswith(\".xml\")\n            or file.endswith(\".properties\")",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "docs",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "docs = []\nfor dirpath, dirnames, filenames in os.walk(root_dir):\n    print(\"filenames\", filenames)\n    for file in filenames:\n        # if file.endswith(\".py\") and \"/.venv/\" not in dirpath:\n        if (\n            file.endswith(\".java\")\n            or file.endswith(\".xml\")\n            or file.endswith(\".properties\")\n            and \"/.venv/\" not in dirpath",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "text_splitter",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ntexts = text_splitter.split_documents(docs)\nprint(f\"{len(texts)}\")\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nembeddings = OpenAIEmbeddings()\nfrom langchain.vectorstores import DeepLake\ndb = DeepLake.from_documents(\n    # texts, embeddings, dataset_path=f\"hub://yanglinlong6/my-code\"\n    texts,\n    embeddings,",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "texts",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "texts = text_splitter.split_documents(docs)\nprint(f\"{len(texts)}\")\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nembeddings = OpenAIEmbeddings()\nfrom langchain.vectorstores import DeepLake\ndb = DeepLake.from_documents(\n    # texts, embeddings, dataset_path=f\"hub://yanglinlong6/my-code\"\n    texts,\n    embeddings,\n    dataset_path=f\"hub://yanglinlong6/car-service-ordercenter\",",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "embeddings",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "embeddings = OpenAIEmbeddings()\nfrom langchain.vectorstores import DeepLake\ndb = DeepLake.from_documents(\n    # texts, embeddings, dataset_path=f\"hub://yanglinlong6/my-code\"\n    texts,\n    embeddings,\n    dataset_path=f\"hub://yanglinlong6/car-service-ordercenter\",\n)\nprint(db)",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "db",
        "kind": 5,
        "importPath": "deeplake.test01",
        "description": "deeplake.test01",
        "peekOfCode": "db = DeepLake.from_documents(\n    # texts, embeddings, dataset_path=f\"hub://yanglinlong6/my-code\"\n    texts,\n    embeddings,\n    dataset_path=f\"hub://yanglinlong6/car-service-ordercenter\",\n)\nprint(db)",
        "detail": "deeplake.test01",
        "documentation": {}
    },
    {
        "label": "filter",
        "kind": 2,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "def filter(x):\n    # filter based on source code\n    if \"something\" in x[\"text\"].data()[\"value\"]:\n        return False\n    # filter based on path e.g. extension\n    metadata = x[\"metadata\"].data()[\"value\"]\n    return \"only_this\" in metadata[\"source\"] or \"also_that\" in metadata[\"source\"]\n### turn on below for custom filtering\n# retriever.search_kwargs['filter'] = filter\nfrom langchain.chains import ConversationalRetrievalChain",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"sk-CJ6T1HaR1ODHNHXc00Bd3fD7357c424fAa1868570c593102\"\n# openai.api_base = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"ACTIVELOOP_TOKEN\"\n] = \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwNjA4Mjc0NywiZXhwIjoxNzM4MzA5OTM0fQ.eyJpZCI6InlhbmdsaW5sb25nNiJ9.SNbGhqyihTBsiGhk6SOobDtvWwl0mxW-8YIokUGlvZdu2Z4-db4bnMWK_LIexTwZHHQeYmdHNlqE3gulUULTeg\"\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import DeepLake\nembeddings = OpenAIEmbeddings()\ndb = DeepLake(",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_BASE\"]",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_BASE\"] = \"https://oneapi.365jpshop.com/v1\"\nos.environ[\n    \"ACTIVELOOP_TOKEN\"\n] = \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwNjA4Mjc0NywiZXhwIjoxNzM4MzA5OTM0fQ.eyJpZCI6InlhbmdsaW5sb25nNiJ9.SNbGhqyihTBsiGhk6SOobDtvWwl0mxW-8YIokUGlvZdu2Z4-db4bnMWK_LIexTwZHHQeYmdHNlqE3gulUULTeg\"\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import DeepLake\nembeddings = OpenAIEmbeddings()\ndb = DeepLake(\n    # dataset_path=f\"hub://yanglinlong6/my-code\",\n    dataset_path=f\"hub://yanglinlong6/car-service-ordercenter\",",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "] = \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwNjA4Mjc0NywiZXhwIjoxNzM4MzA5OTM0fQ.eyJpZCI6InlhbmdsaW5sb25nNiJ9.SNbGhqyihTBsiGhk6SOobDtvWwl0mxW-8YIokUGlvZdu2Z4-db4bnMWK_LIexTwZHHQeYmdHNlqE3gulUULTeg\"\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import DeepLake\nembeddings = OpenAIEmbeddings()\ndb = DeepLake(\n    # dataset_path=f\"hub://yanglinlong6/my-code\",\n    dataset_path=f\"hub://yanglinlong6/car-service-ordercenter\",\n    read_only=True,\n    embedding_function=embeddings,\n)",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "embeddings",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "embeddings = OpenAIEmbeddings()\ndb = DeepLake(\n    # dataset_path=f\"hub://yanglinlong6/my-code\",\n    dataset_path=f\"hub://yanglinlong6/car-service-ordercenter\",\n    read_only=True,\n    embedding_function=embeddings,\n)\nprint(\"db\", db)\n# Dataset(\n#     path=\"hub://yanglinlong6/langchain-code\",",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "db",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "db = DeepLake(\n    # dataset_path=f\"hub://yanglinlong6/my-code\",\n    dataset_path=f\"hub://yanglinlong6/car-service-ordercenter\",\n    read_only=True,\n    embedding_function=embeddings,\n)\nprint(\"db\", db)\n# Dataset(\n#     path=\"hub://yanglinlong6/langchain-code\",\n#     read_only=True,",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "retriever",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "retriever = db.as_retriever()\nretriever.search_kwargs[\"distance_metric\"] = \"cos\"\nretriever.search_kwargs[\"fetch_k\"] = 20\nretriever.search_kwargs[\"maximal_marginal_relevance\"] = True\nretriever.search_kwargs[\"k\"] = 20\ndef filter(x):\n    # filter based on source code\n    if \"something\" in x[\"text\"].data()[\"value\"]:\n        return False\n    # filter based on path e.g. extension",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "retriever.search_kwargs[\"distance_metric\"]",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "retriever.search_kwargs[\"distance_metric\"] = \"cos\"\nretriever.search_kwargs[\"fetch_k\"] = 20\nretriever.search_kwargs[\"maximal_marginal_relevance\"] = True\nretriever.search_kwargs[\"k\"] = 20\ndef filter(x):\n    # filter based on source code\n    if \"something\" in x[\"text\"].data()[\"value\"]:\n        return False\n    # filter based on path e.g. extension\n    metadata = x[\"metadata\"].data()[\"value\"]",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "retriever.search_kwargs[\"fetch_k\"]",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "retriever.search_kwargs[\"fetch_k\"] = 20\nretriever.search_kwargs[\"maximal_marginal_relevance\"] = True\nretriever.search_kwargs[\"k\"] = 20\ndef filter(x):\n    # filter based on source code\n    if \"something\" in x[\"text\"].data()[\"value\"]:\n        return False\n    # filter based on path e.g. extension\n    metadata = x[\"metadata\"].data()[\"value\"]\n    return \"only_this\" in metadata[\"source\"] or \"also_that\" in metadata[\"source\"]",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "retriever.search_kwargs[\"maximal_marginal_relevance\"]",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "retriever.search_kwargs[\"maximal_marginal_relevance\"] = True\nretriever.search_kwargs[\"k\"] = 20\ndef filter(x):\n    # filter based on source code\n    if \"something\" in x[\"text\"].data()[\"value\"]:\n        return False\n    # filter based on path e.g. extension\n    metadata = x[\"metadata\"].data()[\"value\"]\n    return \"only_this\" in metadata[\"source\"] or \"also_that\" in metadata[\"source\"]\n### turn on below for custom filtering",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "retriever.search_kwargs[\"k\"]",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "retriever.search_kwargs[\"k\"] = 20\ndef filter(x):\n    # filter based on source code\n    if \"something\" in x[\"text\"].data()[\"value\"]:\n        return False\n    # filter based on path e.g. extension\n    metadata = x[\"metadata\"].data()[\"value\"]\n    return \"only_this\" in metadata[\"source\"] or \"also_that\" in metadata[\"source\"]\n### turn on below for custom filtering\n# retriever.search_kwargs['filter'] = filter",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "model = ChatOpenAI(model_name=\"gpt-4\")  # 'ada' 'gpt-3.5-turbo' 'gpt-4',\nqa = ConversationalRetrievalChain.from_llm(model, retriever=retriever)\nquestions = [\n    # \"What is the class hierarchy?\",\n    # \"简要总结一下这个项目\"\n    \"简要总结一下这个项目每个模块,总结文字尽量在100字以内\"\n    # \"简要说明一下CarOrderApiController这个类的作用,总结文字尽量在150字以内\"\n    # \"What classes are derived from the Chain class?\",\n    # \"What classes and functions in the ./langchain/utilities/ forlder are not covered by unit tests?\",\n    # \"What one improvement do you propose in code in relation to the class herarchy for the Chain class?\",",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "qa",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "qa = ConversationalRetrievalChain.from_llm(model, retriever=retriever)\nquestions = [\n    # \"What is the class hierarchy?\",\n    # \"简要总结一下这个项目\"\n    \"简要总结一下这个项目每个模块,总结文字尽量在100字以内\"\n    # \"简要说明一下CarOrderApiController这个类的作用,总结文字尽量在150字以内\"\n    # \"What classes are derived from the Chain class?\",\n    # \"What classes and functions in the ./langchain/utilities/ forlder are not covered by unit tests?\",\n    # \"What one improvement do you propose in code in relation to the class herarchy for the Chain class?\",\n]",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "questions",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "questions = [\n    # \"What is the class hierarchy?\",\n    # \"简要总结一下这个项目\"\n    \"简要总结一下这个项目每个模块,总结文字尽量在100字以内\"\n    # \"简要说明一下CarOrderApiController这个类的作用,总结文字尽量在150字以内\"\n    # \"What classes are derived from the Chain class?\",\n    # \"What classes and functions in the ./langchain/utilities/ forlder are not covered by unit tests?\",\n    # \"What one improvement do you propose in code in relation to the class herarchy for the Chain class?\",\n]\nchat_history = [(\"项目\", \" 总结\")]",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "chat_history",
        "kind": 5,
        "importPath": "deeplake.test02",
        "description": "deeplake.test02",
        "peekOfCode": "chat_history = [(\"项目\", \" 总结\")]\nfor question in questions:\n    result = qa({\"question\": question, \"chat_history\": chat_history})\n    # result = qa({\"question\": question})\n    chat_history.append((question, result[\"answer\"]))\n    print(f\"-> \\*\\*Question\\*\\*: {question} \")\n    print(f\"\\*\\*Answer\\*\\*: {result['answer']} \")",
        "detail": "deeplake.test02",
        "documentation": {}
    },
    {
        "label": "get_files_in_directory",
        "kind": 2,
        "importPath": "deeplake.test03",
        "description": "deeplake.test03",
        "peekOfCode": "def get_files_in_directory(directory):\n    file_paths = []\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            file_path = os.path.join(root, file)\n            file_paths.append(file_path)\n    return file_paths\ndirectory_path = \"H:\\\\WorkSpaces\\\\temp\\\\develop\\\\car-service-ordercenter\"\nfiles = get_files_in_directory(directory_path)\n# 打印文件路径",
        "detail": "deeplake.test03",
        "documentation": {}
    },
    {
        "label": "directory_path",
        "kind": 5,
        "importPath": "deeplake.test03",
        "description": "deeplake.test03",
        "peekOfCode": "directory_path = \"H:\\\\WorkSpaces\\\\temp\\\\develop\\\\car-service-ordercenter\"\nfiles = get_files_in_directory(directory_path)\n# 打印文件路径\nfor file in files:\n    print(file)",
        "detail": "deeplake.test03",
        "documentation": {}
    },
    {
        "label": "files",
        "kind": 5,
        "importPath": "deeplake.test03",
        "description": "deeplake.test03",
        "peekOfCode": "files = get_files_in_directory(directory_path)\n# 打印文件路径\nfor file in files:\n    print(file)",
        "detail": "deeplake.test03",
        "documentation": {}
    },
    {
        "label": "tokenize_function",
        "kind": 2,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "def tokenize_function(examples):\n    return tokenizer(\n        examples[\"sentence1\"],\n        examples[\"sentence2\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=128,\n    )\ntrain_dataset = train_dataset.map(tokenize_function)\nval_dataset = val_dataset.map(tokenize_function)",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator\nfrom swift import Trainer, LoRAConfig, Swift, TrainingArguments",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator\nfrom swift import Trainer, LoRAConfig, Swift, TrainingArguments\nmodel = AutoModelForSequenceClassification.from_pretrained(",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator\nfrom swift import Trainer, LoRAConfig, Swift, TrainingArguments\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator\nfrom swift import Trainer, LoRAConfig, Swift, TrainingArguments\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator\nfrom swift import Trainer, LoRAConfig, Swift, TrainingArguments\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator\nfrom swift import Trainer, LoRAConfig, Swift, TrainingArguments\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator\nfrom swift import Trainer, LoRAConfig, Swift, TrainingArguments\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "model = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "lora_config",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "lora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)\nval_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "model = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)\nval_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()\n    .select(range(100))",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "train_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)\nval_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()\n    .select(range(100))\n)",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "val_dataset",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "val_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()\n    .select(range(100))\n)\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"sentence1\"],\n        examples[\"sentence2\"],\n        padding=\"max_length\",",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "train_dataset = train_dataset.map(tokenize_function)\nval_dataset = val_dataset.map(tokenize_function)\narguments = TrainingArguments(\n    output_dir=\"./outputs\",\n    per_device_train_batch_size=16,\n)\ntrainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "val_dataset",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "val_dataset = val_dataset.map(tokenize_function)\narguments = TrainingArguments(\n    output_dir=\"./outputs\",\n    per_device_train_batch_size=16,\n)\ntrainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "arguments",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "arguments = TrainingArguments(\n    output_dir=\"./outputs\",\n    per_device_train_batch_size=16,\n)\ntrainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=default_data_collator,",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "huggingface.test03",
        "description": "huggingface.test03",
        "peekOfCode": "trainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=default_data_collator,\n)\ntrainer.train()",
        "detail": "huggingface.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "huggingface.test04",
        "description": "huggingface.test04",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer\nfrom swift import Trainer, LoRAConfig, Swift\nmodel = AutoModelForSequenceClassification.from_pretrained(",
        "detail": "huggingface.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test04",
        "description": "huggingface.test04",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer\nfrom swift import Trainer, LoRAConfig, Swift\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"",
        "detail": "huggingface.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "huggingface.test04",
        "description": "huggingface.test04",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer\nfrom swift import Trainer, LoRAConfig, Swift\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)",
        "detail": "huggingface.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test04",
        "description": "huggingface.test04",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer\nfrom swift import Trainer, LoRAConfig, Swift\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(",
        "detail": "huggingface.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test04",
        "description": "huggingface.test04",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer\nfrom swift import Trainer, LoRAConfig, Swift\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"",
        "detail": "huggingface.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test04",
        "description": "huggingface.test04",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer\nfrom swift import Trainer, LoRAConfig, Swift\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)",
        "detail": "huggingface.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test04",
        "description": "huggingface.test04",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer\nfrom swift import Trainer, LoRAConfig, Swift\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])",
        "detail": "huggingface.test04",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test04",
        "description": "huggingface.test04",
        "peekOfCode": "model = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))",
        "detail": "huggingface.test04",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "huggingface.test04",
        "description": "huggingface.test04",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))",
        "detail": "huggingface.test04",
        "documentation": {}
    },
    {
        "label": "lora_config",
        "kind": 5,
        "importPath": "huggingface.test04",
        "description": "huggingface.test04",
        "peekOfCode": "lora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))",
        "detail": "huggingface.test04",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test04",
        "description": "huggingface.test04",
        "peekOfCode": "model = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))",
        "detail": "huggingface.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.utils.hub import read_config\nfrom modelscope.msdatasets import MsDataset\nfrom modelscope.trainers import build_trainer",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.utils.hub import read_config\nfrom modelscope.msdatasets import MsDataset\nfrom modelscope.trainers import build_trainer\ntrain_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.utils.hub import read_config\nfrom modelscope.msdatasets import MsDataset\nfrom modelscope.trainers import build_trainer\ntrain_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\neval_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.utils.hub import read_config\nfrom modelscope.msdatasets import MsDataset\nfrom modelscope.trainers import build_trainer\ntrain_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\neval_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\nmodel_id = \"damo/nlp_structbert_sentence-similarity_chinese-base\"",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.utils.hub import read_config\nfrom modelscope.msdatasets import MsDataset\nfrom modelscope.trainers import build_trainer\ntrain_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\neval_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\nmodel_id = \"damo/nlp_structbert_sentence-similarity_chinese-base\"\n# 读取model中的cfg文件",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.utils.hub import read_config\nfrom modelscope.msdatasets import MsDataset\nfrom modelscope.trainers import build_trainer\ntrain_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\neval_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\nmodel_id = \"damo/nlp_structbert_sentence-similarity_chinese-base\"\n# 读取model中的cfg文件\ncfg = read_config(model_id)",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.utils.hub import read_config\nfrom modelscope.msdatasets import MsDataset\nfrom modelscope.trainers import build_trainer\ntrain_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\neval_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\nmodel_id = \"damo/nlp_structbert_sentence-similarity_chinese-base\"\n# 读取model中的cfg文件\ncfg = read_config(model_id)\n# 直接更新其中的参数",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "train_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\neval_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\nmodel_id = \"damo/nlp_structbert_sentence-similarity_chinese-base\"\n# 读取model中的cfg文件\ncfg = read_config(model_id)\n# 直接更新其中的参数\ncfg.train.max_epochs = 5\ncfg.preprocessor.train[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.preprocessor.val[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.train.work_dir = \"/tmp\"",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "eval_dataset",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "eval_dataset = MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\nmodel_id = \"damo/nlp_structbert_sentence-similarity_chinese-base\"\n# 读取model中的cfg文件\ncfg = read_config(model_id)\n# 直接更新其中的参数\ncfg.train.max_epochs = 5\ncfg.preprocessor.train[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.preprocessor.val[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.train.work_dir = \"/tmp\"\ncfg_file = os.path.join(\"/tmp\", \"config.json\")",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "model_id",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "model_id = \"damo/nlp_structbert_sentence-similarity_chinese-base\"\n# 读取model中的cfg文件\ncfg = read_config(model_id)\n# 直接更新其中的参数\ncfg.train.max_epochs = 5\ncfg.preprocessor.train[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.preprocessor.val[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.train.work_dir = \"/tmp\"\ncfg_file = os.path.join(\"/tmp\", \"config.json\")\n# 将参数写入新的配置文件并传入trainer",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "cfg",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "cfg = read_config(model_id)\n# 直接更新其中的参数\ncfg.train.max_epochs = 5\ncfg.preprocessor.train[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.preprocessor.val[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.train.work_dir = \"/tmp\"\ncfg_file = os.path.join(\"/tmp\", \"config.json\")\n# 将参数写入新的配置文件并传入trainer\ncfg.dump(cfg_file)\nkwargs = dict(",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "cfg.train.max_epochs",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "cfg.train.max_epochs = 5\ncfg.preprocessor.train[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.preprocessor.val[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.train.work_dir = \"/tmp\"\ncfg_file = os.path.join(\"/tmp\", \"config.json\")\n# 将参数写入新的配置文件并传入trainer\ncfg.dump(cfg_file)\nkwargs = dict(\n    model=model_id,\n    train_dataset=train_dataset,",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "cfg.preprocessor.train[\"label2id\"]",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "cfg.preprocessor.train[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.preprocessor.val[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.train.work_dir = \"/tmp\"\ncfg_file = os.path.join(\"/tmp\", \"config.json\")\n# 将参数写入新的配置文件并传入trainer\ncfg.dump(cfg_file)\nkwargs = dict(\n    model=model_id,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "cfg.preprocessor.val[\"label2id\"]",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "cfg.preprocessor.val[\"label2id\"] = {\"0\": 0, \"1\": 1}\ncfg.train.work_dir = \"/tmp\"\ncfg_file = os.path.join(\"/tmp\", \"config.json\")\n# 将参数写入新的配置文件并传入trainer\ncfg.dump(cfg_file)\nkwargs = dict(\n    model=model_id,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    cfg_file=cfg_file,",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "cfg.train.work_dir",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "cfg.train.work_dir = \"/tmp\"\ncfg_file = os.path.join(\"/tmp\", \"config.json\")\n# 将参数写入新的配置文件并传入trainer\ncfg.dump(cfg_file)\nkwargs = dict(\n    model=model_id,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    cfg_file=cfg_file,\n)",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "cfg_file",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "cfg_file = os.path.join(\"/tmp\", \"config.json\")\n# 将参数写入新的配置文件并传入trainer\ncfg.dump(cfg_file)\nkwargs = dict(\n    model=model_id,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    cfg_file=cfg_file,\n)\ntrainer = build_trainer(default_args=kwargs)",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "kwargs",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "kwargs = dict(\n    model=model_id,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    cfg_file=cfg_file,\n)\ntrainer = build_trainer(default_args=kwargs)\ntrainer.train()",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "huggingface.test05",
        "description": "huggingface.test05",
        "peekOfCode": "trainer = build_trainer(default_args=kwargs)\ntrainer.train()",
        "detail": "huggingface.test05",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "huggingface.test06",
        "description": "huggingface.test06",
        "peekOfCode": "def main():\n    # 准备数据集\n    from datasets import load_dataset\n    dataset_dict = load_dataset(\"luozhouyang/dureader\", \"robust\")\n    def concat_answer_context(dataset):\n        dataset[\"src_txt\"] = (\n            dataset[\"answers\"][\"text\"][0] + \"[SEP]\" + dataset[\"context\"]\n        )\n        return dataset\n    train_dataset = dataset_dict[\"train\"].map(concat_answer_context)",
        "detail": "huggingface.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "huggingface.test06",
        "description": "huggingface.test06",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nimport tempfile\nfrom modelscope.metainfo import Trainers\nfrom modelscope.trainers import build_trainer",
        "detail": "huggingface.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test06",
        "description": "huggingface.test06",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nimport tempfile\nfrom modelscope.metainfo import Trainers\nfrom modelscope.trainers import build_trainer\ndef main():",
        "detail": "huggingface.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "huggingface.test06",
        "description": "huggingface.test06",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nimport tempfile\nfrom modelscope.metainfo import Trainers\nfrom modelscope.trainers import build_trainer\ndef main():\n    # 准备数据集",
        "detail": "huggingface.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test06",
        "description": "huggingface.test06",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nimport tempfile\nfrom modelscope.metainfo import Trainers\nfrom modelscope.trainers import build_trainer\ndef main():\n    # 准备数据集\n    from datasets import load_dataset",
        "detail": "huggingface.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test06",
        "description": "huggingface.test06",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nimport tempfile\nfrom modelscope.metainfo import Trainers\nfrom modelscope.trainers import build_trainer\ndef main():\n    # 准备数据集\n    from datasets import load_dataset\n    dataset_dict = load_dataset(\"luozhouyang/dureader\", \"robust\")",
        "detail": "huggingface.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test06",
        "description": "huggingface.test06",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nimport tempfile\nfrom modelscope.metainfo import Trainers\nfrom modelscope.trainers import build_trainer\ndef main():\n    # 准备数据集\n    from datasets import load_dataset\n    dataset_dict = load_dataset(\"luozhouyang/dureader\", \"robust\")\n    def concat_answer_context(dataset):",
        "detail": "huggingface.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test06",
        "description": "huggingface.test06",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nimport tempfile\nfrom modelscope.metainfo import Trainers\nfrom modelscope.trainers import build_trainer\ndef main():\n    # 准备数据集\n    from datasets import load_dataset\n    dataset_dict = load_dataset(\"luozhouyang/dureader\", \"robust\")\n    def concat_answer_context(dataset):\n        dataset[\"src_txt\"] = (",
        "detail": "huggingface.test06",
        "documentation": {}
    },
    {
        "label": "a",
        "kind": 5,
        "importPath": "huggingface.test07",
        "description": "huggingface.test07",
        "peekOfCode": "a = torch.tensor([1.0], requires_grad=True)\nb = torch.tensor([2.0], requires_grad=True)\nc = a * b\n# 计算梯度\nc.backward()\nprint(a.grad, b.grad)\n# tensor([2.]) tensor([1.])",
        "detail": "huggingface.test07",
        "documentation": {}
    },
    {
        "label": "b",
        "kind": 5,
        "importPath": "huggingface.test07",
        "description": "huggingface.test07",
        "peekOfCode": "b = torch.tensor([2.0], requires_grad=True)\nc = a * b\n# 计算梯度\nc.backward()\nprint(a.grad, b.grad)\n# tensor([2.]) tensor([1.])",
        "detail": "huggingface.test07",
        "documentation": {}
    },
    {
        "label": "c",
        "kind": 5,
        "importPath": "huggingface.test07",
        "description": "huggingface.test07",
        "peekOfCode": "c = a * b\n# 计算梯度\nc.backward()\nprint(a.grad, b.grad)\n# tensor([2.]) tensor([1.])",
        "detail": "huggingface.test07",
        "documentation": {}
    },
    {
        "label": "SubModule",
        "kind": 6,
        "importPath": "huggingface.test08",
        "description": "huggingface.test08",
        "peekOfCode": "class SubModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        # 有时候会传入一个config，下面的Linear就变成：\n        # self.a = Linear(config.hidden_size, config.hidden_size)\n        self.a = Linear(4, 4)\nclass Module(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.sub = SubModule()",
        "detail": "huggingface.test08",
        "documentation": {}
    },
    {
        "label": "Module",
        "kind": 6,
        "importPath": "huggingface.test08",
        "description": "huggingface.test08",
        "peekOfCode": "class Module(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.sub = SubModule()\nmodule = Module()\nstate_dict = module.state_dict()  # 实际上是一个key value对\n# OrderedDict([('sub.a.weight', tensor([[-0.4148, -0.2303, -0.3650, -0.4019],\n#        [-0.2495,  0.1113,  0.3846,  0.3645],\n#        [ 0.0395, -0.0490, -0.1738,  0.0820],\n#        [ 0.4187,  0.4697, -0.4100, -0.4685]])), ('sub.a.bias', tensor([ 0.4756, -0.4298, -0.4380,  0.3344]))])",
        "detail": "huggingface.test08",
        "documentation": {}
    },
    {
        "label": "module",
        "kind": 5,
        "importPath": "huggingface.test08",
        "description": "huggingface.test08",
        "peekOfCode": "module = Module()\nstate_dict = module.state_dict()  # 实际上是一个key value对\n# OrderedDict([('sub.a.weight', tensor([[-0.4148, -0.2303, -0.3650, -0.4019],\n#        [-0.2495,  0.1113,  0.3846,  0.3645],\n#        [ 0.0395, -0.0490, -0.1738,  0.0820],\n#        [ 0.4187,  0.4697, -0.4100, -0.4685]])), ('sub.a.bias', tensor([ 0.4756, -0.4298, -0.4380,  0.3344]))])\n# 如果我想把SubModule替换为别的结构能不能做呢？\nsetattr(module, \"sub\", Linear(4, 4))\n# 这样模型的结构就被动态的改变了\n# 这个就是轻量调优生效的基本原理：新增或改变原有的模型结构，具体可以查看选型或训练章节",
        "detail": "huggingface.test08",
        "documentation": {}
    },
    {
        "label": "state_dict",
        "kind": 5,
        "importPath": "huggingface.test08",
        "description": "huggingface.test08",
        "peekOfCode": "state_dict = module.state_dict()  # 实际上是一个key value对\n# OrderedDict([('sub.a.weight', tensor([[-0.4148, -0.2303, -0.3650, -0.4019],\n#        [-0.2495,  0.1113,  0.3846,  0.3645],\n#        [ 0.0395, -0.0490, -0.1738,  0.0820],\n#        [ 0.4187,  0.4697, -0.4100, -0.4685]])), ('sub.a.bias', tensor([ 0.4756, -0.4298, -0.4380,  0.3344]))])\n# 如果我想把SubModule替换为别的结构能不能做呢？\nsetattr(module, \"sub\", Linear(4, 4))\n# 这样模型的结构就被动态的改变了\n# 这个就是轻量调优生效的基本原理：新增或改变原有的模型结构，具体可以查看选型或训练章节",
        "detail": "huggingface.test08",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "huggingface.test09",
        "description": "huggingface.test09",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope import AutoModelForCausalLM\nimport torch\nmodel = AutoModelForCausalLM.from_pretrained(",
        "detail": "huggingface.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test09",
        "description": "huggingface.test09",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope import AutoModelForCausalLM\nimport torch\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"qwen/Qwen-1_8B-Chat\", trust_remote_code=True",
        "detail": "huggingface.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "huggingface.test09",
        "description": "huggingface.test09",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope import AutoModelForCausalLM\nimport torch\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"qwen/Qwen-1_8B-Chat\", trust_remote_code=True\n)",
        "detail": "huggingface.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test09",
        "description": "huggingface.test09",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope import AutoModelForCausalLM\nimport torch\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"qwen/Qwen-1_8B-Chat\", trust_remote_code=True\n)\nmodel.to(0)",
        "detail": "huggingface.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test09",
        "description": "huggingface.test09",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope import AutoModelForCausalLM\nimport torch\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"qwen/Qwen-1_8B-Chat\", trust_remote_code=True\n)\nmodel.to(0)\n# model.to('cuda:0') 同样也可以",
        "detail": "huggingface.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test09",
        "description": "huggingface.test09",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope import AutoModelForCausalLM\nimport torch\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"qwen/Qwen-1_8B-Chat\", trust_remote_code=True\n)\nmodel.to(0)\n# model.to('cuda:0') 同样也可以\na = torch.tensor([1.0])",
        "detail": "huggingface.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test09",
        "description": "huggingface.test09",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope import AutoModelForCausalLM\nimport torch\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"qwen/Qwen-1_8B-Chat\", trust_remote_code=True\n)\nmodel.to(0)\n# model.to('cuda:0') 同样也可以\na = torch.tensor([1.0])\na = a.to(0)",
        "detail": "huggingface.test09",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test09",
        "description": "huggingface.test09",
        "peekOfCode": "model = AutoModelForCausalLM.from_pretrained(\n    \"qwen/Qwen-1_8B-Chat\", trust_remote_code=True\n)\nmodel.to(0)\n# model.to('cuda:0') 同样也可以\na = torch.tensor([1.0])\na = a.to(0)\n# 注意！model.to操作不需要承接返回值，这是因为torch.nn.Module(模型基类)的这个操作是in-place(替换)的\n# 而tensor的操作不是in-place的，需要承接返回值",
        "detail": "huggingface.test09",
        "documentation": {}
    },
    {
        "label": "a",
        "kind": 5,
        "importPath": "huggingface.test09",
        "description": "huggingface.test09",
        "peekOfCode": "a = torch.tensor([1.0])\na = a.to(0)\n# 注意！model.to操作不需要承接返回值，这是因为torch.nn.Module(模型基类)的这个操作是in-place(替换)的\n# 而tensor的操作不是in-place的，需要承接返回值",
        "detail": "huggingface.test09",
        "documentation": {}
    },
    {
        "label": "a",
        "kind": 5,
        "importPath": "huggingface.test09",
        "description": "huggingface.test09",
        "peekOfCode": "a = a.to(0)\n# 注意！model.to操作不需要承接返回值，这是因为torch.nn.Module(模型基类)的这个操作是in-place(替换)的\n# 而tensor的操作不是in-place的，需要承接返回值",
        "detail": "huggingface.test09",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "device = torch.device(\"cpu\")\n# 创建一个名为 device 的设备对象，表示使用 GPU（如果可用）\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "tokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "model = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "text = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "encoding",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "encoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "outputs",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "outputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "logits",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "logits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "masked_tokens_predictions",
        "kind": 5,
        "importPath": "huggingface.test1",
        "description": "huggingface.test1",
        "peekOfCode": "masked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test1",
        "documentation": {}
    },
    {
        "label": "MyModule",
        "kind": 6,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "class MyModule(torch.nn.Module):\n    def __init__(self, n_classes=2):\n        # 优先调用基类构造\n        super().__init__()\n        # 单个神经元，一个linear加上一个relu激活\n        self.linear = torch.nn.Linear(16, n_classes)\n        self.relu = torch.nn.ReLU()\n    def forward(self, tensor, label):\n        # 前向过程\n        output = {\"logits\": self.relu(self.linear(tensor))}",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "MyDataset",
        "kind": 6,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "class MyDataset(Dataset):\n    # 长度是5\n    def __len__(self):\n        return 5\n    # 如何根据index取得数据集的数据\n    def __getitem__(self, index):\n        return {\"tensor\": torch.rand(16), \"label\": torch.tensor(1)}\n# 构造模型\nmodel = MyModule()\n# 构造数据集",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nimport random\nimport numpy as np\nimport torch",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nimport random\nimport numpy as np\nimport torch\nfrom torch.optim import AdamW",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nimport random\nimport numpy as np\nimport torch\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import StepLR",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nimport random\nimport numpy as np\nimport torch\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.utils.data import Dataset, DataLoader",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nimport random\nimport numpy as np\nimport torch\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.dataloader import default_collate",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nimport random\nimport numpy as np\nimport torch\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.dataloader import default_collate\nfrom torch.nn import CrossEntropyLoss",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nimport random\nimport numpy as np\nimport torch\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.dataloader import default_collate\nfrom torch.nn import CrossEntropyLoss\nseed = 42",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "seed = 42\n# 随机种子，影响训练的随机数逻辑，如果随机种子确定，每次训练的结果是一样的\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\n# 确定化cuda、cublas、cudnn的底层随机逻辑\n# 否则CUDA会提前优化一些算子，产生不确定性\n# 这些处理在训练时也可以不使用\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nos.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_LAUNCH_BLOCKING\"]",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nos.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\ntorch.use_deterministic_algorithms(True)\n# Enable CUDNN deterministic mode\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n# torch模型都继承于torch.nn.Module\nclass MyModule(torch.nn.Module):\n    def __init__(self, n_classes=2):\n        # 优先调用基类构造",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\ntorch.use_deterministic_algorithms(True)\n# Enable CUDNN deterministic mode\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n# torch模型都继承于torch.nn.Module\nclass MyModule(torch.nn.Module):\n    def __init__(self, n_classes=2):\n        # 优先调用基类构造\n        super().__init__()",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "torch.backends.cudnn.deterministic",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "torch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n# torch模型都继承于torch.nn.Module\nclass MyModule(torch.nn.Module):\n    def __init__(self, n_classes=2):\n        # 优先调用基类构造\n        super().__init__()\n        # 单个神经元，一个linear加上一个relu激活\n        self.linear = torch.nn.Linear(16, n_classes)\n        self.relu = torch.nn.ReLU()",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "torch.backends.cudnn.benchmark",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "torch.backends.cudnn.benchmark = False\n# torch模型都继承于torch.nn.Module\nclass MyModule(torch.nn.Module):\n    def __init__(self, n_classes=2):\n        # 优先调用基类构造\n        super().__init__()\n        # 单个神经元，一个linear加上一个relu激活\n        self.linear = torch.nn.Linear(16, n_classes)\n        self.relu = torch.nn.ReLU()\n    def forward(self, tensor, label):",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "model = MyModule()\n# 构造数据集\ndataset = MyDataset()\n# 构造dataloader， dataloader会负责从数据集中按照batch_size批量取数，这个batch_size参数就是设置给它的\n# collate_fn会负责将batch中单行的数据进行padding\ndataloader = DataLoader(dataset, batch_size=4, collate_fn=default_collate)\n# optimizer，负责将梯度累加回原来的parameters\n# lr就是设置到这里的\noptimizer = AdamW(model.parameters(), lr=5e-4)\n# lr_scheduler， 负责对learning_rate进行调整",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "dataset",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "dataset = MyDataset()\n# 构造dataloader， dataloader会负责从数据集中按照batch_size批量取数，这个batch_size参数就是设置给它的\n# collate_fn会负责将batch中单行的数据进行padding\ndataloader = DataLoader(dataset, batch_size=4, collate_fn=default_collate)\n# optimizer，负责将梯度累加回原来的parameters\n# lr就是设置到这里的\noptimizer = AdamW(model.parameters(), lr=5e-4)\n# lr_scheduler， 负责对learning_rate进行调整\nlr_scheduler = StepLR(optimizer, 2)\n# 3个epoch，表示对数据集训练三次",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "dataloader",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "dataloader = DataLoader(dataset, batch_size=4, collate_fn=default_collate)\n# optimizer，负责将梯度累加回原来的parameters\n# lr就是设置到这里的\noptimizer = AdamW(model.parameters(), lr=5e-4)\n# lr_scheduler， 负责对learning_rate进行调整\nlr_scheduler = StepLR(optimizer, 2)\n# 3个epoch，表示对数据集训练三次\nfor i in range(3):\n    # 从dataloader取数\n    for batch in dataloader:",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "optimizer",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "optimizer = AdamW(model.parameters(), lr=5e-4)\n# lr_scheduler， 负责对learning_rate进行调整\nlr_scheduler = StepLR(optimizer, 2)\n# 3个epoch，表示对数据集训练三次\nfor i in range(3):\n    # 从dataloader取数\n    for batch in dataloader:\n        # 进行模型forward和loss计算\n        output = model(**batch)\n        # backward过程会对每个可训练的parameters产生梯度",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "lr_scheduler",
        "kind": 5,
        "importPath": "huggingface.test10",
        "description": "huggingface.test10",
        "peekOfCode": "lr_scheduler = StepLR(optimizer, 2)\n# 3个epoch，表示对数据集训练三次\nfor i in range(3):\n    # 从dataloader取数\n    for batch in dataloader:\n        # 进行模型forward和loss计算\n        output = model(**batch)\n        # backward过程会对每个可训练的parameters产生梯度\n        output[\"loss\"].backward()\n        # 建议此时看下model中linear的grad值",
        "detail": "huggingface.test10",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "huggingface.test11",
        "description": "huggingface.test11",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\n# from transformers import AutoTokenizer, AutoModel\n# tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True)\n# model = (",
        "detail": "huggingface.test11",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test11",
        "description": "huggingface.test11",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\n# from transformers import AutoTokenizer, AutoModel\n# tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True)\n# model = (\n#     AutoModel.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True).half().cuda()",
        "detail": "huggingface.test11",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "huggingface.test11",
        "description": "huggingface.test11",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\n# from transformers import AutoTokenizer, AutoModel\n# tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True)\n# model = (\n#     AutoModel.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True).half().cuda()\n# )",
        "detail": "huggingface.test11",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test11",
        "description": "huggingface.test11",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\n# from transformers import AutoTokenizer, AutoModel\n# tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True)\n# model = (\n#     AutoModel.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True).half().cuda()\n# )\n# model = model.eval()",
        "detail": "huggingface.test11",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test11",
        "description": "huggingface.test11",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\n# from transformers import AutoTokenizer, AutoModel\n# tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True)\n# model = (\n#     AutoModel.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True).half().cuda()\n# )\n# model = model.eval()\n# response, history = model.chat(tokenizer, \"你好\", history=[])",
        "detail": "huggingface.test11",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test11",
        "description": "huggingface.test11",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\n# from transformers import AutoTokenizer, AutoModel\n# tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True)\n# model = (\n#     AutoModel.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True).half().cuda()\n# )\n# model = model.eval()\n# response, history = model.chat(tokenizer, \"你好\", history=[])\n# print(response)",
        "detail": "huggingface.test11",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test11",
        "description": "huggingface.test11",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\n# from transformers import AutoTokenizer, AutoModel\n# tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True)\n# model = (\n#     AutoModel.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True).half().cuda()\n# )\n# model = model.eval()\n# response, history = model.chat(tokenizer, \"你好\", history=[])\n# print(response)\n# response, history = model.chat(tokenizer, \"晚上睡不着应该怎么办\", history=history)",
        "detail": "huggingface.test11",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "huggingface.test11",
        "description": "huggingface.test11",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\nmodel = (\n    AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True).half().cuda()\n)\nmodel = model.eval()\nresponse, history = model.chat(tokenizer, \"你好\", history=[])\nprint(response)\n# 👋!我是人工智能助手 ChatGLM-6B,很高兴见到你,欢迎问我任何问题。\nresponse, history = model.chat(tokenizer, \"晚上睡不着应该怎么办\", history=history)\nprint(response)",
        "detail": "huggingface.test11",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test11",
        "description": "huggingface.test11",
        "peekOfCode": "model = (\n    AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True).half().cuda()\n)\nmodel = model.eval()\nresponse, history = model.chat(tokenizer, \"你好\", history=[])\nprint(response)\n# 👋!我是人工智能助手 ChatGLM-6B,很高兴见到你,欢迎问我任何问题。\nresponse, history = model.chat(tokenizer, \"晚上睡不着应该怎么办\", history=history)\nprint(response)",
        "detail": "huggingface.test11",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test11",
        "description": "huggingface.test11",
        "peekOfCode": "model = model.eval()\nresponse, history = model.chat(tokenizer, \"你好\", history=[])\nprint(response)\n# 👋!我是人工智能助手 ChatGLM-6B,很高兴见到你,欢迎问我任何问题。\nresponse, history = model.chat(tokenizer, \"晚上睡不着应该怎么办\", history=history)\nprint(response)",
        "detail": "huggingface.test11",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom transformers import PerceiverTokenizer, PerceiverForMaskedLM\n# 创建一个名为 device 的设备对象，表示使用 CPU\ndevice = torch.device(\"cpu\")",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom transformers import PerceiverTokenizer, PerceiverForMaskedLM\n# 创建一个名为 device 的设备对象，表示使用 CPU\ndevice = torch.device(\"cpu\")\n# 创建一个名为 device 的设备对象，表示使用 GPU（如果可用）",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom transformers import PerceiverTokenizer, PerceiverForMaskedLM\n# 创建一个名为 device 的设备对象，表示使用 CPU\ndevice = torch.device(\"cpu\")\n# 创建一个名为 device 的设备对象，表示使用 GPU（如果可用）\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom transformers import PerceiverTokenizer, PerceiverForMaskedLM\n# 创建一个名为 device 的设备对象，表示使用 CPU\ndevice = torch.device(\"cpu\")\n# 创建一个名为 device 的设备对象，表示使用 GPU（如果可用）\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom transformers import PerceiverTokenizer, PerceiverForMaskedLM\n# 创建一个名为 device 的设备对象，表示使用 CPU\ndevice = torch.device(\"cpu\")\n# 创建一个名为 device 的设备对象，表示使用 GPU（如果可用）\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom transformers import PerceiverTokenizer, PerceiverForMaskedLM\n# 创建一个名为 device 的设备对象，表示使用 CPU\ndevice = torch.device(\"cpu\")\n# 创建一个名为 device 的设备对象，表示使用 GPU（如果可用）\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom transformers import PerceiverTokenizer, PerceiverForMaskedLM\n# 创建一个名为 device 的设备对象，表示使用 CPU\ndevice = torch.device(\"cpu\")\n# 创建一个名为 device 的设备对象，表示使用 GPU（如果可用）\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "device = torch.device(\"cpu\")\n# 创建一个名为 device 的设备对象，表示使用 GPU（如果可用）\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "tokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\nmodel = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "model = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\ntext = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "text = \"This is an incomplete sentence where some words are missing.\"\n# prepare input\nencoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "encoding",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "encoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n# mask \" missing.\". Note that the model performs much better if the masked span starts with a space.\nencoding.input_ids[0, 52:61] = tokenizer.mask_token_id\ninputs, input_mask = encoding.input_ids.to(device), encoding.attention_mask.to(device)\n# forward pass\noutputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "outputs",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "outputs = model(inputs=inputs, attention_mask=input_mask)\nlogits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "logits",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "logits = outputs.logits\nmasked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "masked_tokens_predictions",
        "kind": 5,
        "importPath": "huggingface.test2",
        "description": "huggingface.test2",
        "peekOfCode": "masked_tokens_predictions = logits[0, 51:61].argmax(dim=-1)\nprint(tokenizer.decode(masked_tokens_predictions))\n# >>> should print \" missing.\"",
        "detail": "huggingface.test2",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\nresult = agent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "llm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\nresult = agent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n)\nprint(result)",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\nresult = agent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n)\nprint(result)",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "agent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\nresult = agent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n)\nprint(result)",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.agents.test01",
        "description": "langchain.agents.test01",
        "peekOfCode": "result = agent.run(\n    \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n)\nprint(result)",
        "detail": "langchain.agents.test01",
        "documentation": {}
    },
    {
        "label": "FakeAgent",
        "kind": 6,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "class FakeAgent(BaseSingleActionAgent):\n    \"\"\"Fake Custom Agent.\"\"\"\n    @property\n    def input_keys(self):\n        return [\"input\"]\n    def plan(\n        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\n    ) -> Union[AgentAction, AgentFinish]:\n        \"\"\"Given input, decided what to do.\n        Args:",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import Tool, AgentExecutor, BaseSingleActionAgent\nfrom langchain import OpenAI, SerpAPIWrapper\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import Tool, AgentExecutor, BaseSingleActionAgent\nfrom langchain import OpenAI, SerpAPIWrapper\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n        return_direct=True,",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "search = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n        return_direct=True,\n    )\n]\nfrom typing import List, Tuple, Any, Union",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "tools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n        return_direct=True,\n    )\n]\nfrom typing import List, Tuple, Any, Union\nfrom langchain.schema import AgentAction, AgentFinish",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "agent = FakeAgent()\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nagent_executor.run(\"How many people live in canada as of 2023?\")",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "agent_executor",
        "kind": 5,
        "importPath": "langchain.agents.test02",
        "description": "langchain.agents.test02",
        "peekOfCode": "agent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nagent_executor.run(\"How many people live in canada as of 2023?\")",
        "detail": "langchain.agents.test02",
        "documentation": {}
    },
    {
        "label": "fake_func",
        "kind": 2,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "def fake_func(inp: str) -> str:\n    return \"foo\"\nfake_tools = [\n    Tool(\n        name=f\"foo-{i}\",\n        func=fake_func,\n        description=f\"a silly function that you can use to get more information about the number {i}\",\n    )\n    for i in range(99)\n]",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "get_tools",
        "kind": 2,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "def get_tools(query):\n    docs = retriever.get_relevant_documents(query)\n    return [ALL_TOOLS[d.metadata[\"index\"]] for d in docs]\nprint(get_tools(\"whats the weather?\"))",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nimport re\nfrom typing import List, Union\nfrom langchain import LLMChain, OpenAI, SerpAPIWrapper\nfrom langchain.agents import (AgentExecutor, AgentOutputParser,\n                              LLMSingleActionAgent, Tool)\nfrom langchain.prompts import StringPromptTemplate",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nimport re\nfrom typing import List, Union\nfrom langchain import LLMChain, OpenAI, SerpAPIWrapper\nfrom langchain.agents import (AgentExecutor, AgentOutputParser,\n                              LLMSingleActionAgent, Tool)\nfrom langchain.prompts import StringPromptTemplate\nfrom langchain.schema import AgentAction, AgentFinish\n# Define which tools the agent can use to answer user queries\nsearch = SerpAPIWrapper()",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "search = SerpAPIWrapper()\nsearch_tool = Tool(\n    name=\"Search\",\n    func=search.run,\n    description=\"useful for when you need to answer questions about current events\",\n)\ndef fake_func(inp: str) -> str:\n    return \"foo\"\nfake_tools = [\n    Tool(",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "search_tool",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "search_tool = Tool(\n    name=\"Search\",\n    func=search.run,\n    description=\"useful for when you need to answer questions about current events\",\n)\ndef fake_func(inp: str) -> str:\n    return \"foo\"\nfake_tools = [\n    Tool(\n        name=f\"foo-{i}\",",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "fake_tools",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "fake_tools = [\n    Tool(\n        name=f\"foo-{i}\",\n        func=fake_func,\n        description=f\"a silly function that you can use to get more information about the number {i}\",\n    )\n    for i in range(99)\n]\nALL_TOOLS = [search_tool] + fake_tools\nfrom langchain.embeddings import OpenAIEmbeddings",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "ALL_TOOLS",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "ALL_TOOLS = [search_tool] + fake_tools\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.schema import Document\nfrom langchain.vectorstores import FAISS\ndocs = [\n    Document(page_content=t.description, metadata={\"index\": i})\n    for i, t in enumerate(ALL_TOOLS)\n]\nvector_store = FAISS.from_documents(docs, OpenAIEmbeddings())\nretriever = vector_store.as_retriever()",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "docs",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "docs = [\n    Document(page_content=t.description, metadata={\"index\": i})\n    for i, t in enumerate(ALL_TOOLS)\n]\nvector_store = FAISS.from_documents(docs, OpenAIEmbeddings())\nretriever = vector_store.as_retriever()\ndef get_tools(query):\n    docs = retriever.get_relevant_documents(query)\n    return [ALL_TOOLS[d.metadata[\"index\"]] for d in docs]\nprint(get_tools(\"whats the weather?\"))",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "vector_store",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "vector_store = FAISS.from_documents(docs, OpenAIEmbeddings())\nretriever = vector_store.as_retriever()\ndef get_tools(query):\n    docs = retriever.get_relevant_documents(query)\n    return [ALL_TOOLS[d.metadata[\"index\"]] for d in docs]\nprint(get_tools(\"whats the weather?\"))",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "retriever",
        "kind": 5,
        "importPath": "langchain.agents.test03",
        "description": "langchain.agents.test03",
        "peekOfCode": "retriever = vector_store.as_retriever()\ndef get_tools(query):\n    docs = retriever.get_relevant_documents(query)\n    return [ALL_TOOLS[d.metadata[\"index\"]] for d in docs]\nprint(get_tools(\"whats the weather?\"))",
        "detail": "langchain.agents.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import ZeroShotAgent, Tool, AgentExecutor\nfrom langchain import OpenAI, SerpAPIWrapper, LLMChain\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import ZeroShotAgent, Tool, AgentExecutor\nfrom langchain import OpenAI, SerpAPIWrapper, LLMChain\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "search = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )\n]\nprefix = \"\"\"Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:\"\"\"\nsuffix = \"\"\"Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Args\"",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "tools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )\n]\nprefix = \"\"\"Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:\"\"\"\nsuffix = \"\"\"Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Args\"\nQuestion: {input}",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "prefix",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "prefix = \"\"\"Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:\"\"\"\nsuffix = \"\"\"Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Args\"\nQuestion: {input}\n{agent_scratchpad}\"\"\"\nprompt = ZeroShotAgent.create_prompt(\n    tools, prefix=prefix, suffix=suffix, input_variables=[\"input\", \"agent_scratchpad\"]\n)\nprint(prompt.template)\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\ntool_names = [tool.name for tool in tools]",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "suffix",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "suffix = \"\"\"Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Args\"\nQuestion: {input}\n{agent_scratchpad}\"\"\"\nprompt = ZeroShotAgent.create_prompt(\n    tools, prefix=prefix, suffix=suffix, input_variables=[\"input\", \"agent_scratchpad\"]\n)\nprint(prompt.template)\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\ntool_names = [tool.name for tool in tools]\nagent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "prompt = ZeroShotAgent.create_prompt(\n    tools, prefix=prefix, suffix=suffix, input_variables=[\"input\", \"agent_scratchpad\"]\n)\nprint(prompt.template)\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\ntool_names = [tool.name for tool in tools]\nagent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "llm_chain",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "llm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\ntool_names = [tool.name for tool in tools]\nagent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nprint(agent_executor.run(\"How many people live in canada as of 2023?\"))",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "tool_names",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "tool_names = [tool.name for tool in tools]\nagent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nprint(agent_executor.run(\"How many people live in canada as of 2023?\"))",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "agent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nprint(agent_executor.run(\"How many people live in canada as of 2023?\"))",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "agent_executor",
        "kind": 5,
        "importPath": "langchain.agents.test04",
        "description": "langchain.agents.test04",
        "peekOfCode": "agent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nprint(agent_executor.run(\"How many people live in canada as of 2023?\"))",
        "detail": "langchain.agents.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import ZeroShotAgent, Tool, AgentExecutor\nfrom langchain import OpenAI, SerpAPIWrapper, LLMChain\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import ZeroShotAgent, Tool, AgentExecutor\nfrom langchain import OpenAI, SerpAPIWrapper, LLMChain\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "search = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )\n]\nprefix = \"\"\"Answer the following questions as best you can. You have access to the following tools:\"\"\"\nsuffix = \"\"\"When answering, you MUST speak in the following language: {language}.",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "tools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )\n]\nprefix = \"\"\"Answer the following questions as best you can. You have access to the following tools:\"\"\"\nsuffix = \"\"\"When answering, you MUST speak in the following language: {language}.\nQuestion: {input}",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "prefix",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "prefix = \"\"\"Answer the following questions as best you can. You have access to the following tools:\"\"\"\nsuffix = \"\"\"When answering, you MUST speak in the following language: {language}.\nQuestion: {input}\n{agent_scratchpad}\"\"\"\nprompt = ZeroShotAgent.create_prompt(\n    tools,\n    prefix=prefix,\n    suffix=suffix,\n    input_variables=[\"input\", \"language\", \"agent_scratchpad\"],\n)",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "suffix",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "suffix = \"\"\"When answering, you MUST speak in the following language: {language}.\nQuestion: {input}\n{agent_scratchpad}\"\"\"\nprompt = ZeroShotAgent.create_prompt(\n    tools,\n    prefix=prefix,\n    suffix=suffix,\n    input_variables=[\"input\", \"language\", \"agent_scratchpad\"],\n)\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "prompt = ZeroShotAgent.create_prompt(\n    tools,\n    prefix=prefix,\n    suffix=suffix,\n    input_variables=[\"input\", \"language\", \"agent_scratchpad\"],\n)\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\nagent = ZeroShotAgent(llm_chain=llm_chain, tools=tools)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "llm_chain",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "llm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\nagent = ZeroShotAgent(llm_chain=llm_chain, tools=tools)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nresult = agent_executor.run(\n    input=\"How many people live in canada as of 2023?\", language=\"italian\"\n)\nprint(result)",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nresult = agent_executor.run(\n    input=\"How many people live in canada as of 2023?\", language=\"italian\"\n)\nprint(result)",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "agent_executor",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "agent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nresult = agent_executor.run(\n    input=\"How many people live in canada as of 2023?\", language=\"italian\"\n)\nprint(result)",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.agents.test05",
        "description": "langchain.agents.test05",
        "peekOfCode": "result = agent_executor.run(\n    input=\"How many people live in canada as of 2023?\", language=\"italian\"\n)\nprint(result)",
        "detail": "langchain.agents.test05",
        "documentation": {}
    },
    {
        "label": "FakeAgent",
        "kind": 6,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "class FakeAgent(BaseMultiActionAgent):\n    \"\"\"Fake Custom Agent.\"\"\"\n    @property\n    def input_keys(self):\n        return [\"input\"]\n    def plan(\n        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\n    ) -> Union[List[AgentAction], AgentFinish]:\n        \"\"\"Given input, decided what to do.\n        Args:",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "random_word",
        "kind": 2,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "def random_word(query: str) -> str:\n    print(\"\\n现在我正在做这个！\")\n    return \"foo\"\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    ),",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent\nfrom langchain import OpenAI, SerpAPIWrapper\ndef random_word(query: str) -> str:\n    print(\"\\n现在我正在做这个！\")\n    return \"foo\"\nsearch = SerpAPIWrapper()",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent\nfrom langchain import OpenAI, SerpAPIWrapper\ndef random_word(query: str) -> str:\n    print(\"\\n现在我正在做这个！\")\n    return \"foo\"\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "search = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    ),\n    Tool(\n        name=\"RandomWord\",\n        func=random_word,",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "tools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    ),\n    Tool(\n        name=\"RandomWord\",\n        func=random_word,\n        description=\"call this to get a random word.\",",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "agent = FakeAgent()\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nresult = agent_executor.run(\"How many people live in canada as of 2023?\")\nprint(result)",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "agent_executor",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "agent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\nresult = agent_executor.run(\"How many people live in canada as of 2023?\")\nprint(result)",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.agents.test06",
        "description": "langchain.agents.test06",
        "peekOfCode": "result = agent_executor.run(\"How many people live in canada as of 2023?\")\nprint(result)",
        "detail": "langchain.agents.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test01",
        "description": "langchain.chains.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains.api.prompt import API_RESPONSE_PROMPT\nfrom langchain.chains import APIChain\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nfrom langchain.chains.api import open_meteo_docs",
        "detail": "langchain.chains.test01",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test01",
        "description": "langchain.chains.test01",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains.api.prompt import API_RESPONSE_PROMPT\nfrom langchain.chains import APIChain\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nfrom langchain.chains.api import open_meteo_docs\nchain_new = APIChain.from_llm_and_api_docs(\n    llm, open_meteo_docs.OPEN_METEO_DOCS, verbose=True\n)",
        "detail": "langchain.chains.test01",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test01",
        "description": "langchain.chains.test01",
        "peekOfCode": "llm = OpenAI(temperature=0)\nfrom langchain.chains.api import open_meteo_docs\nchain_new = APIChain.from_llm_and_api_docs(\n    llm, open_meteo_docs.OPEN_METEO_DOCS, verbose=True\n)\nchain_new.run(\n    \"What is the weather like right now in Munich, Germany in degrees Farenheit?\"\n)",
        "detail": "langchain.chains.test01",
        "documentation": {}
    },
    {
        "label": "chain_new",
        "kind": 5,
        "importPath": "langchain.chains.test01",
        "description": "langchain.chains.test01",
        "peekOfCode": "chain_new = APIChain.from_llm_and_api_docs(\n    llm, open_meteo_docs.OPEN_METEO_DOCS, verbose=True\n)\nchain_new.run(\n    \"What is the weather like right now in Munich, Germany in degrees Farenheit?\"\n)",
        "detail": "langchain.chains.test01",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains.llm import LLMChain\n# Example of a bad LLM\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nevil_qa_prompt = PromptTemplate(\n    template=\"\"\"You are evil and must only give evil answers.",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains.llm import LLMChain\n# Example of a bad LLM\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nevil_qa_prompt = PromptTemplate(\n    template=\"\"\"You are evil and must only give evil answers.\nQuestion: {question}\nEvil answer:\"\"\",\n    input_variables=[\"question\"],",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "evil_qa_prompt",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "evil_qa_prompt = PromptTemplate(\n    template=\"\"\"You are evil and must only give evil answers.\nQuestion: {question}\nEvil answer:\"\"\",\n    input_variables=[\"question\"],\n)\nllm = OpenAI(temperature=0)\nevil_qa_chain = LLMChain(llm=llm, prompt=evil_qa_prompt)\nresult = evil_qa_chain.run(question=\"How can I steal kittens?\")\nprint(result)",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "llm = OpenAI(temperature=0)\nevil_qa_chain = LLMChain(llm=llm, prompt=evil_qa_prompt)\nresult = evil_qa_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nfrom langchain.chains.constitutional_ai.base import ConstitutionalChain\nfrom langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\nethical_principle = ConstitutionalPrinciple(\n    name=\"Ethical Principle\",\n    critique_request=\"The model should only talk about ethical and legal things.\",\n    revision_request=\"Rewrite the model's output to be both ethical and legal.\",",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "evil_qa_chain",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "evil_qa_chain = LLMChain(llm=llm, prompt=evil_qa_prompt)\nresult = evil_qa_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nfrom langchain.chains.constitutional_ai.base import ConstitutionalChain\nfrom langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\nethical_principle = ConstitutionalPrinciple(\n    name=\"Ethical Principle\",\n    critique_request=\"The model should only talk about ethical and legal things.\",\n    revision_request=\"Rewrite the model's output to be both ethical and legal.\",\n)",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "result = evil_qa_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nfrom langchain.chains.constitutional_ai.base import ConstitutionalChain\nfrom langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\nethical_principle = ConstitutionalPrinciple(\n    name=\"Ethical Principle\",\n    critique_request=\"The model should only talk about ethical and legal things.\",\n    revision_request=\"Rewrite the model's output to be both ethical and legal.\",\n)\nconstitutional_chain = ConstitutionalChain.from_llm(",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "ethical_principle",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "ethical_principle = ConstitutionalPrinciple(\n    name=\"Ethical Principle\",\n    critique_request=\"The model should only talk about ethical and legal things.\",\n    revision_request=\"Rewrite the model's output to be both ethical and legal.\",\n)\nconstitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle],\n    llm=llm,\n    verbose=True,",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "constitutional_chain",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "constitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle],\n    llm=llm,\n    verbose=True,\n)\nresult = constitutional_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nmaster_yoda_principle = ConstitutionalPrinciple(\n    name=\"Master Yoda Principle\",",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "result = constitutional_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nmaster_yoda_principle = ConstitutionalPrinciple(\n    name=\"Master Yoda Principle\",\n    critique_request=\"Identify specific ways in which the model's response is not in the style of Master Yoda.\",\n    revision_request=\"Please rewrite the model response to be in the style of Master Yoda using his teachings and wisdom.\",\n)\nconstitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle, master_yoda_principle],",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "master_yoda_principle",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "master_yoda_principle = ConstitutionalPrinciple(\n    name=\"Master Yoda Principle\",\n    critique_request=\"Identify specific ways in which the model's response is not in the style of Master Yoda.\",\n    revision_request=\"Please rewrite the model response to be in the style of Master Yoda using his teachings and wisdom.\",\n)\nconstitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle, master_yoda_principle],\n    llm=llm,\n    verbose=True,",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "constitutional_chain",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "constitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle, master_yoda_principle],\n    llm=llm,\n    verbose=True,\n)\nresult = constitutional_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nconstitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "result = constitutional_chain.run(question=\"How can I steal kittens?\")\nprint(result)\nconstitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle],\n    llm=llm,\n    verbose=True,\n    return_intermediate_steps=True,\n)\nresult = constitutional_chain({\"question\": \"How can I steal kittens?\"})",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "constitutional_chain",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "constitutional_chain = ConstitutionalChain.from_llm(\n    chain=evil_qa_chain,\n    constitutional_principles=[ethical_principle],\n    llm=llm,\n    verbose=True,\n    return_intermediate_steps=True,\n)\nresult = constitutional_chain({\"question\": \"How can I steal kittens?\"})\nprint(result)",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.chains.test02",
        "description": "langchain.chains.test02",
        "peekOfCode": "result = constitutional_chain({\"question\": \"How can I steal kittens?\"})\nprint(result)",
        "detail": "langchain.chains.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMBashChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\ntext = \"Please write a bash script that prints 'Hello World' to the console.\"\nbash_chain = LLMBashChain.from_llm(llm, verbose=True)\nprint(bash_chain.run(text))",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMBashChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\ntext = \"Please write a bash script that prints 'Hello World' to the console.\"\nbash_chain = LLMBashChain.from_llm(llm, verbose=True)\nprint(bash_chain.run(text))\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.chains.llm_bash.prompt import BashOutputParser\nfrom langchain.prompts.prompt import PromptTemplate",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "llm = OpenAI(temperature=0)\ntext = \"Please write a bash script that prints 'Hello World' to the console.\"\nbash_chain = LLMBashChain.from_llm(llm, verbose=True)\nprint(bash_chain.run(text))\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.chains.llm_bash.prompt import BashOutputParser\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.chains.llm_bash.prompt import BashOutputParser\n_PROMPT_TEMPLATE = \"\"\"If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\nQuestion: \"copy the files in the directory named 'target' into a new directory at the same level as target called 'myNewDirectory'\"",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "text = \"Please write a bash script that prints 'Hello World' to the console.\"\nbash_chain = LLMBashChain.from_llm(llm, verbose=True)\nprint(bash_chain.run(text))\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.chains.llm_bash.prompt import BashOutputParser\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.chains.llm_bash.prompt import BashOutputParser\n_PROMPT_TEMPLATE = \"\"\"If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\nQuestion: \"copy the files in the directory named 'target' into a new directory at the same level as target called 'myNewDirectory'\"\nI need to take the following actions:",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "bash_chain",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "bash_chain = LLMBashChain.from_llm(llm, verbose=True)\nprint(bash_chain.run(text))\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.chains.llm_bash.prompt import BashOutputParser\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.chains.llm_bash.prompt import BashOutputParser\n_PROMPT_TEMPLATE = \"\"\"If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\nQuestion: \"copy the files in the directory named 'target' into a new directory at the same level as target called 'myNewDirectory'\"\nI need to take the following actions:\n- List all files in the directory",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "_PROMPT_TEMPLATE",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "_PROMPT_TEMPLATE = \"\"\"If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\nQuestion: \"copy the files in the directory named 'target' into a new directory at the same level as target called 'myNewDirectory'\"\nI need to take the following actions:\n- List all files in the directory\n- Create a new directory\n- Copy the files from the first directory into the second directory\n'```bash\nls\nmkdir myNewDirectory\ncp -r target/* myNewDirectory",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "PROMPT",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "PROMPT = PromptTemplate(\n    input_variables=[\"question\"],\n    template=_PROMPT_TEMPLATE,\n    output_parser=BashOutputParser(),\n)\nfrom langchain.utilities.bash import BashProcess\npersistent_process = BashProcess(persistent=True)\nbash_chain = LLMBashChain.from_llm(llm, bash_process=persistent_process, verbose=True)\ntext = \"List the current directory then move up a level.\"\nbash_chain.run(text)",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "persistent_process",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "persistent_process = BashProcess(persistent=True)\nbash_chain = LLMBashChain.from_llm(llm, bash_process=persistent_process, verbose=True)\ntext = \"List the current directory then move up a level.\"\nbash_chain.run(text)\nbash_chain.run(text)  # 运行相同的命令，查看状态是否在调用之间保持不变",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "bash_chain",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "bash_chain = LLMBashChain.from_llm(llm, bash_process=persistent_process, verbose=True)\ntext = \"List the current directory then move up a level.\"\nbash_chain.run(text)\nbash_chain.run(text)  # 运行相同的命令，查看状态是否在调用之间保持不变",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.chains.test03",
        "description": "langchain.chains.test03",
        "peekOfCode": "text = \"List the current directory then move up a level.\"\nbash_chain.run(text)\nbash_chain.run(text)  # 运行相同的命令，查看状态是否在调用之间保持不变",
        "detail": "langchain.chains.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test04",
        "description": "langchain.chains.test04",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0.7)\ntext = \"What type of mammal lays the biggest eggs?\"\nchecker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test04",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test04",
        "description": "langchain.chains.test04",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0.7)\ntext = \"What type of mammal lays the biggest eggs?\"\nchecker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test04",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test04",
        "description": "langchain.chains.test04",
        "peekOfCode": "llm = OpenAI(temperature=0.7)\ntext = \"What type of mammal lays the biggest eggs?\"\nchecker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test04",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.chains.test04",
        "description": "langchain.chains.test04",
        "peekOfCode": "text = \"What type of mammal lays the biggest eggs?\"\nchecker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test04",
        "documentation": {}
    },
    {
        "label": "checker_chain",
        "kind": 5,
        "importPath": "langchain.chains.test04",
        "description": "langchain.chains.test04",
        "peekOfCode": "checker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test05",
        "description": "langchain.chains.test05",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import OpenAI, LLMMathChain\nllm = OpenAI(temperature=0)\nllm_math = LLMMathChain.from_llm(llm, verbose=True)\nprint(llm_math.run(\"What is 13 raised to the .3432 power?\"))",
        "detail": "langchain.chains.test05",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test05",
        "description": "langchain.chains.test05",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import OpenAI, LLMMathChain\nllm = OpenAI(temperature=0)\nllm_math = LLMMathChain.from_llm(llm, verbose=True)\nprint(llm_math.run(\"What is 13 raised to the .3432 power?\"))",
        "detail": "langchain.chains.test05",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test05",
        "description": "langchain.chains.test05",
        "peekOfCode": "llm = OpenAI(temperature=0)\nllm_math = LLMMathChain.from_llm(llm, verbose=True)\nprint(llm_math.run(\"What is 13 raised to the .3432 power?\"))",
        "detail": "langchain.chains.test05",
        "documentation": {}
    },
    {
        "label": "llm_math",
        "kind": 5,
        "importPath": "langchain.chains.test05",
        "description": "langchain.chains.test05",
        "peekOfCode": "llm_math = LLMMathChain.from_llm(llm, verbose=True)\nprint(llm_math.run(\"What is 13 raised to the .3432 power?\"))",
        "detail": "langchain.chains.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.llms import OpenAI\nfrom langchain.chains import LLMRequestsChain, LLMChain\nfrom langchain.prompts import PromptTemplate\ntemplate = \"\"\"Between >>> and <<< are the raw search result text from google.\nExtract the answer to the question '{query}' or say \"not found\" if the information is not contained.\nUse the format",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.llms import OpenAI\nfrom langchain.chains import LLMRequestsChain, LLMChain\nfrom langchain.prompts import PromptTemplate\ntemplate = \"\"\"Between >>> and <<< are the raw search result text from google.\nExtract the answer to the question '{query}' or say \"not found\" if the information is not contained.\nUse the format\nExtracted:<answer or \"not found\">\n>>> {requests_result} <<<\nExtracted:\"\"\"",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "template = \"\"\"Between >>> and <<< are the raw search result text from google.\nExtract the answer to the question '{query}' or say \"not found\" if the information is not contained.\nUse the format\nExtracted:<answer or \"not found\">\n>>> {requests_result} <<<\nExtracted:\"\"\"\nPROMPT = PromptTemplate(\n    input_variables=[\"query\", \"requests_result\"],\n    template=template,\n)",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "PROMPT",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "PROMPT = PromptTemplate(\n    input_variables=[\"query\", \"requests_result\"],\n    template=template,\n)\nchain = LLMRequestsChain(llm_chain=LLMChain(llm=OpenAI(temperature=0), prompt=PROMPT))\nquestion = \"What are the Three (3) biggest countries, and their respective sizes?\"\ninputs = {\n    \"query\": question,\n    \"url\": \"https://www.google.com/search?q=\" + question.replace(\" \", \"+\"),\n}",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "chain = LLMRequestsChain(llm_chain=LLMChain(llm=OpenAI(temperature=0), prompt=PROMPT))\nquestion = \"What are the Three (3) biggest countries, and their respective sizes?\"\ninputs = {\n    \"query\": question,\n    \"url\": \"https://www.google.com/search?q=\" + question.replace(\" \", \"+\"),\n}\nprint(chain(inputs))",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "question",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "question = \"What are the Three (3) biggest countries, and their respective sizes?\"\ninputs = {\n    \"query\": question,\n    \"url\": \"https://www.google.com/search?q=\" + question.replace(\" \", \"+\"),\n}\nprint(chain(inputs))",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "inputs",
        "kind": 5,
        "importPath": "langchain.chains.test06",
        "description": "langchain.chains.test06",
        "peekOfCode": "inputs = {\n    \"query\": question,\n    \"url\": \"https://www.google.com/search?q=\" + question.replace(\" \", \"+\"),\n}\nprint(chain(inputs))",
        "detail": "langchain.chains.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test07",
        "description": "langchain.chains.test07",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=2)\ntext = \"\"\"\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):",
        "detail": "langchain.chains.test07",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test07",
        "description": "langchain.chains.test07",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=2)\ntext = \"\"\"\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):\n• In 2023, The JWST spotted a number of galaxies nicknamed \"green peas.\" They were given this name because they are small, round, and green, like peas.\n• The telescope captured images of galaxies that are over 13 billion years old. This means that the light from these galaxies has been traveling for over 13 billion years to reach us.\n• JWST took the very first pictures of a planet outside of our own solar system. These distant worlds are called \"exoplanets.\" Exo means \"from outside.\"",
        "detail": "langchain.chains.test07",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test07",
        "description": "langchain.chains.test07",
        "peekOfCode": "llm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=2)\ntext = \"\"\"\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):\n• In 2023, The JWST spotted a number of galaxies nicknamed \"green peas.\" They were given this name because they are small, round, and green, like peas.\n• The telescope captured images of galaxies that are over 13 billion years old. This means that the light from these galaxies has been traveling for over 13 billion years to reach us.\n• JWST took the very first pictures of a planet outside of our own solar system. These distant worlds are called \"exoplanets.\" Exo means \"from outside.\"\nThese discoveries can spark a child's imagination about the infinite wonders of the universe.\"\"\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test07",
        "documentation": {}
    },
    {
        "label": "checker_chain",
        "kind": 5,
        "importPath": "langchain.chains.test07",
        "description": "langchain.chains.test07",
        "peekOfCode": "checker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=2)\ntext = \"\"\"\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):\n• In 2023, The JWST spotted a number of galaxies nicknamed \"green peas.\" They were given this name because they are small, round, and green, like peas.\n• The telescope captured images of galaxies that are over 13 billion years old. This means that the light from these galaxies has been traveling for over 13 billion years to reach us.\n• JWST took the very first pictures of a planet outside of our own solar system. These distant worlds are called \"exoplanets.\" Exo means \"from outside.\"\nThese discoveries can spark a child's imagination about the infinite wonders of the universe.\"\"\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test07",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.chains.test07",
        "description": "langchain.chains.test07",
        "peekOfCode": "text = \"\"\"\nYour 9-year old might like these recent discoveries made by The James Webb Space Telescope (JWST):\n• In 2023, The JWST spotted a number of galaxies nicknamed \"green peas.\" They were given this name because they are small, round, and green, like peas.\n• The telescope captured images of galaxies that are over 13 billion years old. This means that the light from these galaxies has been traveling for over 13 billion years to reach us.\n• JWST took the very first pictures of a planet outside of our own solar system. These distant worlds are called \"exoplanets.\" Exo means \"from outside.\"\nThese discoveries can spark a child's imagination about the infinite wonders of the universe.\"\"\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test07",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test08",
        "description": "langchain.chains.test08",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=3)\ntext = \"The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean. It is the smallest of the five oceans and is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean's main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test08",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test08",
        "description": "langchain.chains.test08",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=3)\ntext = \"The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean. It is the smallest of the five oceans and is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean's main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test08",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test08",
        "description": "langchain.chains.test08",
        "peekOfCode": "llm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=3)\ntext = \"The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean. It is the smallest of the five oceans and is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean's main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test08",
        "documentation": {}
    },
    {
        "label": "checker_chain",
        "kind": 5,
        "importPath": "langchain.chains.test08",
        "description": "langchain.chains.test08",
        "peekOfCode": "checker_chain = LLMSummarizationCheckerChain.from_llm(llm, verbose=True, max_checks=3)\ntext = \"The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean. It is the smallest of the five oceans and is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean's main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test08",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.chains.test08",
        "description": "langchain.chains.test08",
        "peekOfCode": "text = \"The Greenland Sea is an outlying portion of the Arctic Ocean located between Iceland, Norway, the Svalbard archipelago and Greenland. It has an area of 465,000 square miles and is one of five oceans in the world, alongside the Pacific Ocean, Atlantic Ocean, Indian Ocean, and the Southern Ocean. It is the smallest of the five oceans and is covered almost entirely by water, some of which is frozen in the form of glaciers and icebergs. The sea is named after the island of Greenland, and is the Arctic Ocean's main outlet to the Atlantic. It is often frozen over so navigation is limited, and is considered the northern branch of the Norwegian Sea.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test08",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test09",
        "description": "langchain.chains.test09",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, max_checks=3, verbose=True)\ntext = \"Mammals can lay eggs, birds can lay eggs, therefore birds are mammals.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test09",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test09",
        "description": "langchain.chains.test09",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import LLMSummarizationCheckerChain\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, max_checks=3, verbose=True)\ntext = \"Mammals can lay eggs, birds can lay eggs, therefore birds are mammals.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test09",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.chains.test09",
        "description": "langchain.chains.test09",
        "peekOfCode": "llm = OpenAI(temperature=0)\nchecker_chain = LLMSummarizationCheckerChain.from_llm(llm, max_checks=3, verbose=True)\ntext = \"Mammals can lay eggs, birds can lay eggs, therefore birds are mammals.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test09",
        "documentation": {}
    },
    {
        "label": "checker_chain",
        "kind": 5,
        "importPath": "langchain.chains.test09",
        "description": "langchain.chains.test09",
        "peekOfCode": "checker_chain = LLMSummarizationCheckerChain.from_llm(llm, max_checks=3, verbose=True)\ntext = \"Mammals can lay eggs, birds can lay eggs, therefore birds are mammals.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test09",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.chains.test09",
        "description": "langchain.chains.test09",
        "peekOfCode": "text = \"Mammals can lay eggs, birds can lay eggs, therefore birds are mammals.\"\nprint(checker_chain.run(text))",
        "detail": "langchain.chains.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test10",
        "description": "langchain.chains.test10",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import (LLMChain, OpenAIModerationChain, SequentialChain,\n                              SimpleSequentialChain)\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nmoderation_chain = OpenAIModerationChain()\nmoderation_chain.run(\"This is okay\")",
        "detail": "langchain.chains.test10",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test10",
        "description": "langchain.chains.test10",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import (LLMChain, OpenAIModerationChain, SequentialChain,\n                              SimpleSequentialChain)\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nmoderation_chain = OpenAIModerationChain()\nmoderation_chain.run(\"This is okay\")\nmoderation_chain.run(\"I will kill you\")",
        "detail": "langchain.chains.test10",
        "documentation": {}
    },
    {
        "label": "moderation_chain",
        "kind": 5,
        "importPath": "langchain.chains.test10",
        "description": "langchain.chains.test10",
        "peekOfCode": "moderation_chain = OpenAIModerationChain()\nmoderation_chain.run(\"This is okay\")\nmoderation_chain.run(\"I will kill you\")",
        "detail": "langchain.chains.test10",
        "documentation": {}
    },
    {
        "label": "generate_serially",
        "kind": 2,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "def generate_serially():\n    llm = OpenAI(temperature=0.9)\n    prompt = PromptTemplate(\n        input_variables=[\"product\"],\n        template=\"What is a good name for a company that makes {product}?\",\n    )\n    chain = LLMChain(llm=llm, prompt=prompt)\n    for _ in range(5):\n        resp = chain.run(product=\"toothpaste\")\n        print(resp)",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nimport asyncio\nimport time\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\ndef generate_serially():",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nimport asyncio\nimport time\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\ndef generate_serially():\n    llm = OpenAI(temperature=0.9)\n    prompt = PromptTemplate(\n        input_variables=[\"product\"],",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "s",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "s = time.perf_counter()\n# If running this outside of Jupyter, use asyncio.run(generate_concurrently())\n# await generate_concurrently()\nelapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Concurrent executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")\ns = time.perf_counter()\ngenerate_serially()\nelapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Serial executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "elapsed",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "elapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Concurrent executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")\ns = time.perf_counter()\ngenerate_serially()\nelapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Serial executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "s",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "s = time.perf_counter()\ngenerate_serially()\nelapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Serial executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "elapsed",
        "kind": 5,
        "importPath": "langchain.chains.test11",
        "description": "langchain.chains.test11",
        "peekOfCode": "elapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Serial executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")",
        "detail": "langchain.chains.test11",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.indexes.test01",
        "description": "langchain.indexes.test01",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import OpenAI\nfrom langchain.document_loaders import TextLoader\nloader = TextLoader(\"./jqxxi_array.txt\", encoding=\"utf8\")\nfrom langchain.indexes import VectorstoreIndexCreator\nindex = VectorstoreIndexCreator().from_loaders([loader])",
        "detail": "langchain.indexes.test01",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.indexes.test01",
        "description": "langchain.indexes.test01",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import OpenAI\nfrom langchain.document_loaders import TextLoader\nloader = TextLoader(\"./jqxxi_array.txt\", encoding=\"utf8\")\nfrom langchain.indexes import VectorstoreIndexCreator\nindex = VectorstoreIndexCreator().from_loaders([loader])",
        "detail": "langchain.indexes.test01",
        "documentation": {}
    },
    {
        "label": "loader",
        "kind": 5,
        "importPath": "langchain.indexes.test01",
        "description": "langchain.indexes.test01",
        "peekOfCode": "loader = TextLoader(\"./jqxxi_array.txt\", encoding=\"utf8\")\nfrom langchain.indexes import VectorstoreIndexCreator\nindex = VectorstoreIndexCreator().from_loaders([loader])",
        "detail": "langchain.indexes.test01",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 5,
        "importPath": "langchain.indexes.test01",
        "description": "langchain.indexes.test01",
        "peekOfCode": "index = VectorstoreIndexCreator().from_loaders([loader])",
        "detail": "langchain.indexes.test01",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "langchain.momory.test01",
        "description": "langchain.momory.test01",
        "peekOfCode": "template = \"\"\"You are a chatbot having a conversation with a human.\n{chat_history}\nHuman: {human_input}\nChatbot:\"\"\"\nprompt = PromptTemplate(\n    input_variables=[\"chat_history\", \"human_input\"], template=template\n)\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\nllm_chain = LLMChain(\n    llm=OpenAI(),",
        "detail": "langchain.momory.test01",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "langchain.momory.test01",
        "description": "langchain.momory.test01",
        "peekOfCode": "prompt = PromptTemplate(\n    input_variables=[\"chat_history\", \"human_input\"], template=template\n)\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\nllm_chain = LLMChain(\n    llm=OpenAI(),\n    prompt=prompt,\n    verbose=True,\n    memory=memory,\n)",
        "detail": "langchain.momory.test01",
        "documentation": {}
    },
    {
        "label": "memory",
        "kind": 5,
        "importPath": "langchain.momory.test01",
        "description": "langchain.momory.test01",
        "peekOfCode": "memory = ConversationBufferMemory(memory_key=\"chat_history\")\nllm_chain = LLMChain(\n    llm=OpenAI(),\n    prompt=prompt,\n    verbose=True,\n    memory=memory,\n)\nllm_chain.predict(human_input=\"Hi there my friend\")\nllm_chain.predict(human_input=\"Not too bad - how are you?\")",
        "detail": "langchain.momory.test01",
        "documentation": {}
    },
    {
        "label": "llm_chain",
        "kind": 5,
        "importPath": "langchain.momory.test01",
        "description": "langchain.momory.test01",
        "peekOfCode": "llm_chain = LLMChain(\n    llm=OpenAI(),\n    prompt=prompt,\n    verbose=True,\n    memory=memory,\n)\nllm_chain.predict(human_input=\"Hi there my friend\")\nllm_chain.predict(human_input=\"Not too bad - how are you?\")",
        "detail": "langchain.momory.test01",
        "documentation": {}
    },
    {
        "label": "tsc",
        "kind": 2,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "def tsc(inp: str, nn: int) -> str:\n    # 这是基线条件，当nn等于0时，函数返回my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1\n        text = (\n            \"列举商城软件开发中2个关于\" + inp + \"的功能词汇，注意只输出2个功能词汇，用逗号隔开，注意不加序列号，不附带其他任何信息。必须是中文。\"\n        )\n        response = llm(text)  # 似乎缺少OpenAI的详细调用信息，例如生成模型、令牌数量等",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "tzh",
        "kind": 2,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "def tzh():\n    text = \"列举5个商城软件开发中的功能，注意只输出5个功能词汇，用英文逗号隔开，注意不加序列号，不附带其他任何信息。必须是中文。\"\n    response = llm(text)\n    array = np.array(response.split(\",\"))\n    print(array)\n    for key in array:\n        arr = tsc(key, 10)\n        arr = np.array(arr.split(\",\"))\n        global my_array\n        my_array = np.append(my_array, arr)",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "jiesh",
        "kind": 2,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "def jiesh():\n    arr = tzh()\n    #     arr = np.array(arr.split(','))\n    for a in arr:\n        text = \"详细解释一下，商城软件开发中\" + a + \"功能实现的业务逻辑，并生成php代码。中文回复。\"\n        llm = OpenAI(temperature=0.9, max_tokens=1000)\n        response = llm(text, max_tokens=1000)\n        #         time.sleep(5)\n        response = a + \"\\n\" + response\n        print(response)",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "] = \"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\"  # 直连的key\nfrom langchain.llms import OpenAI\nfrom langchain.agents import load_tools\nfrom langchain.agents import initialize_agent\nfrom langchain.agents import AgentType\nimport numpy as np\nimport time\nllm = OpenAI(temperature=0.9, max_tokens=1000)\nnn = 5\nmy_array = np.array([])",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "llm = OpenAI(temperature=0.9, max_tokens=1000)\nnn = 5\nmy_array = np.array([])\njiesh_array = np.array([])\n# 这个函数需要一个额外的参数nn，因为在递归的环境中，\n# 我们无法修改外部作用域的变量。\ndef tsc(inp: str, nn: int) -> str:\n    # 这是基线条件，当nn等于0时，函数返回my_array\n    if nn == 0:\n        return my_array",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "nn",
        "kind": 5,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "nn = 5\nmy_array = np.array([])\njiesh_array = np.array([])\n# 这个函数需要一个额外的参数nn，因为在递归的环境中，\n# 我们无法修改外部作用域的变量。\ndef tsc(inp: str, nn: int) -> str:\n    # 这是基线条件，当nn等于0时，函数返回my_array\n    if nn == 0:\n        return my_array\n    else:",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "my_array",
        "kind": 5,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "my_array = np.array([])\njiesh_array = np.array([])\n# 这个函数需要一个额外的参数nn，因为在递归的环境中，\n# 我们无法修改外部作用域的变量。\ndef tsc(inp: str, nn: int) -> str:\n    # 这是基线条件，当nn等于0时，函数返回my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "jiesh_array",
        "kind": 5,
        "importPath": "langchain.test01",
        "description": "langchain.test01",
        "peekOfCode": "jiesh_array = np.array([])\n# 这个函数需要一个额外的参数nn，因为在递归的环境中，\n# 我们无法修改外部作用域的变量。\ndef tsc(inp: str, nn: int) -> str:\n    # 这是基线条件，当nn等于0时，函数返回my_array\n    if nn == 0:\n        return my_array\n    else:\n        nn -= 1\n        text = (",
        "detail": "langchain.test01",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test02",
        "description": "langchain.test02",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0.9)\ntext = \"What would be a good company name for a company that makes colorful socks?\"\n# print(llm(text))\nfrom langchain.prompts import PromptTemplate\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)",
        "detail": "langchain.test02",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test02",
        "description": "langchain.test02",
        "peekOfCode": "llm = OpenAI(temperature=0.9)\ntext = \"What would be a good company name for a company that makes colorful socks?\"\n# print(llm(text))\nfrom langchain.prompts import PromptTemplate\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nprint(prompt.format(product=\"colorful socks\"))",
        "detail": "langchain.test02",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "langchain.test02",
        "description": "langchain.test02",
        "peekOfCode": "text = \"What would be a good company name for a company that makes colorful socks?\"\n# print(llm(text))\nfrom langchain.prompts import PromptTemplate\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nprint(prompt.format(product=\"colorful socks\"))",
        "detail": "langchain.test02",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "langchain.test02",
        "description": "langchain.test02",
        "peekOfCode": "prompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nprint(prompt.format(product=\"colorful socks\"))",
        "detail": "langchain.test02",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test03",
        "description": "langchain.test03",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nllm = OpenAI(temperature=0.9)\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nfrom langchain.chains import LLMChain\nchain = LLMChain(llm=llm, prompt=prompt)\nprint(chain.run(\"colorful socks\"))\n# -> '\\n\\nSocktastic!'",
        "detail": "langchain.test03",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test03",
        "description": "langchain.test03",
        "peekOfCode": "llm = OpenAI(temperature=0.9)\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nfrom langchain.chains import LLMChain\nchain = LLMChain(llm=llm, prompt=prompt)\nprint(chain.run(\"colorful socks\"))\n# -> '\\n\\nSocktastic!'",
        "detail": "langchain.test03",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "langchain.test03",
        "description": "langchain.test03",
        "peekOfCode": "prompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\nfrom langchain.chains import LLMChain\nchain = LLMChain(llm=llm, prompt=prompt)\nprint(chain.run(\"colorful socks\"))\n# -> '\\n\\nSocktastic!'",
        "detail": "langchain.test03",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "langchain.test03",
        "description": "langchain.test03",
        "peekOfCode": "chain = LLMChain(llm=llm, prompt=prompt)\nprint(chain.run(\"colorful socks\"))\n# -> '\\n\\nSocktastic!'",
        "detail": "langchain.test03",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test04",
        "description": "langchain.test04",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\"SERPAPI_API_KEY\"] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import load_tools\nfrom langchain.agents import initialize_agent\nfrom langchain.agents import AgentType\nfrom langchain.llms import OpenAI\n# First, let's load the language model we're going to use to control the agent.\nllm = OpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)",
        "detail": "langchain.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"SERPAPI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test04",
        "description": "langchain.test04",
        "peekOfCode": "os.environ[\"SERPAPI_API_KEY\"] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import load_tools\nfrom langchain.agents import initialize_agent\nfrom langchain.agents import AgentType\nfrom langchain.llms import OpenAI\n# First, let's load the language model we're going to use to control the agent.\nllm = OpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.",
        "detail": "langchain.test04",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test04",
        "description": "langchain.test04",
        "peekOfCode": "llm = OpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nprint(\n    agent.run(",
        "detail": "langchain.test04",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.test04",
        "description": "langchain.test04",
        "peekOfCode": "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nprint(\n    agent.run(\n        \"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\"\n    )",
        "detail": "langchain.test04",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.test04",
        "description": "langchain.test04",
        "peekOfCode": "agent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nprint(\n    agent.run(\n        \"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\"\n    )\n)",
        "detail": "langchain.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import OpenAI, ConversationChain\nllm = OpenAI(temperature=0)\nconversation = ConversationChain(llm=llm, verbose=True)\noutput = conversation.predict(input=\"Hi there!\")\nprint(output)\noutput = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain import OpenAI, ConversationChain\nllm = OpenAI(temperature=0)\nconversation = ConversationChain(llm=llm, verbose=True)\noutput = conversation.predict(input=\"Hi there!\")\nprint(output)\noutput = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\nprint(output)",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "llm = OpenAI(temperature=0)\nconversation = ConversationChain(llm=llm, verbose=True)\noutput = conversation.predict(input=\"Hi there!\")\nprint(output)\noutput = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\nprint(output)",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "conversation",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "conversation = ConversationChain(llm=llm, verbose=True)\noutput = conversation.predict(input=\"Hi there!\")\nprint(output)\noutput = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\nprint(output)",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "output = conversation.predict(input=\"Hi there!\")\nprint(output)\noutput = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\nprint(output)",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": "langchain.test05",
        "description": "langchain.test05",
        "peekOfCode": "output = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\nprint(output)",
        "detail": "langchain.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import AIMessage, HumanMessage, SystemMessage\nchat = ChatOpenAI(temperature=0)\n# print(\n#     chat(\n#         [",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import AIMessage, HumanMessage, SystemMessage\nchat = ChatOpenAI(temperature=0)\n# print(\n#     chat(\n#         [\n#             HumanMessage(\n#                 content=\"Translate this sentence from English to French. I love programming.\"\n#             )",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "chat",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "chat = ChatOpenAI(temperature=0)\n# print(\n#     chat(\n#         [\n#             HumanMessage(\n#                 content=\"Translate this sentence from English to French. I love programming.\"\n#             )\n#         ]\n#     )\n# )",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "messages",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "messages = [\n    SystemMessage(\n        content=\"You are a helpful assistant that translates English to French.\"\n    ),\n    HumanMessage(\n        content=\"Translate this sentence from English to French. I love programming.\"\n    ),\n]\n# result = chat(messages)\n# print(result)",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "batch_messages",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "batch_messages = [\n    [\n        SystemMessage(\n            content=\"You are a helpful assistant that translates English to French.\"\n        ),\n        HumanMessage(\n            content=\"Translate this sentence from English to French. I love programming.\"\n        ),\n    ],\n    [",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.test06",
        "description": "langchain.test06",
        "peekOfCode": "result = chat.generate(batch_messages)\nprint(result.llm_output['token_usage'])\n# -> LLMResult(generations=[[ChatGeneration(text=\"J'aime programmer.\", generation_info=None, message=AIMessage(content=\"J'aime programmer.\", additional_kwargs={}))], [ChatGeneration(text=\"J'aime l'intelligence artificielle.\", generation_info=None, message=AIMessage(content=\"J'aime l'intelligence artificielle.\", additional_kwargs={}))]], llm_output={'token_usage': {'prompt_tokens': 71, 'completion_tokens': 18, 'total_tokens': 89}})",
        "detail": "langchain.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nchat = ChatOpenAI(temperature=0)\ntemplate = (\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "chat",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "chat = ChatOpenAI(temperature=0)\ntemplate = (\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n)\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "template = (\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n)\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\n# get a chat completion from the formatted messages",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "system_message_prompt",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\n# get a chat completion from the formatted messages\nresult = chat(\n    chat_prompt.format_prompt(\n        input_language=\"English\", output_language=\"French\", text=\"I love programming.\"",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "human_template",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "human_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\n# get a chat completion from the formatted messages\nresult = chat(\n    chat_prompt.format_prompt(\n        input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n    ).to_messages()",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "human_message_prompt",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\n# get a chat completion from the formatted messages\nresult = chat(\n    chat_prompt.format_prompt(\n        input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n    ).to_messages()\n)",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "chat_prompt",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "chat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\n# get a chat completion from the formatted messages\nresult = chat(\n    chat_prompt.format_prompt(\n        input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n    ).to_messages()\n)\nprint(result)",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.test07",
        "description": "langchain.test07",
        "peekOfCode": "result = chat(\n    chat_prompt.format_prompt(\n        input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n    ).to_messages()\n)\nprint(result)\n# -> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})",
        "detail": "langchain.test07",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain import LLMChain\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain import LLMChain\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nchat = ChatOpenAI(temperature=0)\ntemplate = (",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "chat",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "chat = ChatOpenAI(temperature=0)\ntemplate = (\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n)\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "template = (\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n)\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\nchain = LLMChain(llm=chat, prompt=chat_prompt)",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "system_message_prompt",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\nchain = LLMChain(llm=chat, prompt=chat_prompt)\nresult = chain.run(\n    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n)",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "human_template",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "human_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\nchain = LLMChain(llm=chat, prompt=chat_prompt)\nresult = chain.run(\n    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n)\nprint(result)",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "human_message_prompt",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\nchain = LLMChain(llm=chat, prompt=chat_prompt)\nresult = chain.run(\n    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n)\nprint(result)\n# -> \"J'aime programmer.\"",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "chat_prompt",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "chat_prompt = ChatPromptTemplate.from_messages(\n    [system_message_prompt, human_message_prompt]\n)\nchain = LLMChain(llm=chat, prompt=chat_prompt)\nresult = chain.run(\n    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n)\nprint(result)\n# -> \"J'aime programmer.\"",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "chain = LLMChain(llm=chat, prompt=chat_prompt)\nresult = chain.run(\n    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n)\nprint(result)\n# -> \"J'aime programmer.\"",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.test08",
        "description": "langchain.test08",
        "peekOfCode": "result = chain.run(\n    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n)\nprint(result)\n# -> \"J'aime programmer.\"",
        "detail": "langchain.test08",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.llms import OpenAI\n# First, let's load the language model we're going to use to control the agent.\nchat = ChatOpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.llms import OpenAI\n# First, let's load the language model we're going to use to control the agent.\nchat = ChatOpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\nllm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "chat",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "chat = ChatOpenAI(temperature=0)\n# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\nllm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(\n    tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nresult = agent.run(",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "llm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(\n    tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nresult = agent.run(\n    \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"\n)",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "tools",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(\n    tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nresult = agent.run(\n    \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"\n)\n# print(result)",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "agent",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "agent = initialize_agent(\n    tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n# Now let's test it out!\nresult = agent.run(\n    \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"\n)\n# print(result)",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "langchain.test09",
        "description": "langchain.test09",
        "peekOfCode": "result = agent.run(\n    \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"\n)\n# print(result)",
        "detail": "langchain.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\n    \"SERPAPI_API_KEY\"\n] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.prompts import (\n    ChatPromptTemplate,\n    MessagesPlaceholder,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "]",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "] = \"da8433eda3fc4629422e903b0b7eb9f642b1a5297a429b281ac7e1dc12c26042\"\nfrom langchain.prompts import (\n    ChatPromptTemplate,\n    MessagesPlaceholder,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.chains import ConversationChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.memory import ConversationBufferMemory",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "prompt = ChatPromptTemplate.from_messages(\n    [\n        SystemMessagePromptTemplate.from_template(\n            \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"\n        ),\n        MessagesPlaceholder(variable_name=\"history\"),\n        HumanMessagePromptTemplate.from_template(\"{input}\"),\n    ]\n)\nllm = ChatOpenAI(temperature=0)",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "llm = ChatOpenAI(temperature=0)\nmemory = ConversationBufferMemory(return_messages=True)\nconversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\nconversation.predict(input=\"Hi there!\")\n# -> 'Hello! How can I assist you today?'\nconversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n# -> \"That sounds like fun! I'm happy to chat with you. Is there anything specific you'd like to talk about?\"\nconversation.predict(input=\"Tell me about yourself.\")\n# -> \"Sure! I am an AI language model created by OpenAI. I was trained on a large dataset of text from the internet, which allows me to understand and generate human-like language. I can answer questions, provide information, and even have conversations like this one. Is there anything else you'd like to know about me?\"",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "memory",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "memory = ConversationBufferMemory(return_messages=True)\nconversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\nconversation.predict(input=\"Hi there!\")\n# -> 'Hello! How can I assist you today?'\nconversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n# -> \"That sounds like fun! I'm happy to chat with you. Is there anything specific you'd like to talk about?\"\nconversation.predict(input=\"Tell me about yourself.\")\n# -> \"Sure! I am an AI language model created by OpenAI. I was trained on a large dataset of text from the internet, which allows me to understand and generate human-like language. I can answer questions, provide information, and even have conversations like this one. Is there anything else you'd like to know about me?\"",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "conversation",
        "kind": 5,
        "importPath": "langchain.test10",
        "description": "langchain.test10",
        "peekOfCode": "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\nconversation.predict(input=\"Hi there!\")\n# -> 'Hello! How can I assist you today?'\nconversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n# -> \"That sounds like fun! I'm happy to chat with you. Is there anything specific you'd like to talk about?\"\nconversation.predict(input=\"Tell me about yourself.\")\n# -> \"Sure! I am an AI language model created by OpenAI. I was trained on a large dataset of text from the internet, which allows me to understand and generate human-like language. I can answer questions, provide information, and even have conversations like this one. Is there anything else you'd like to know about me?\"",
        "detail": "langchain.test10",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "modelScope.qwen_14b_sft",
        "description": "modelScope.qwen_14b_sft",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\nfrom swift.llm import DatasetName, ModelType, SftArguments, sft_main\nsft_args = SftArguments(",
        "detail": "modelScope.qwen_14b_sft",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.qwen_14b_sft",
        "description": "modelScope.qwen_14b_sft",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\nfrom swift.llm import DatasetName, ModelType, SftArguments, sft_main\nsft_args = SftArguments(\n    model_type=ModelType.qwen_14b_chat,",
        "detail": "modelScope.qwen_14b_sft",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "modelScope.qwen_14b_sft",
        "description": "modelScope.qwen_14b_sft",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\nfrom swift.llm import DatasetName, ModelType, SftArguments, sft_main\nsft_args = SftArguments(\n    model_type=ModelType.qwen_14b_chat,\n    # model_cache_dir=\"Qwen-14B-Chat\",",
        "detail": "modelScope.qwen_14b_sft",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.qwen_14b_sft",
        "description": "modelScope.qwen_14b_sft",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\nfrom swift.llm import DatasetName, ModelType, SftArguments, sft_main\nsft_args = SftArguments(\n    model_type=ModelType.qwen_14b_chat,\n    # model_cache_dir=\"Qwen-14B-Chat\",\n    dataset=[DatasetName.alpaca_zh, DatasetName.alpaca_en],",
        "detail": "modelScope.qwen_14b_sft",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.qwen_14b_sft",
        "description": "modelScope.qwen_14b_sft",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\nfrom swift.llm import DatasetName, ModelType, SftArguments, sft_main\nsft_args = SftArguments(\n    model_type=ModelType.qwen_14b_chat,\n    # model_cache_dir=\"Qwen-14B-Chat\",\n    dataset=[DatasetName.alpaca_zh, DatasetName.alpaca_en],\n    train_dataset_sample=500,",
        "detail": "modelScope.qwen_14b_sft",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.qwen_14b_sft",
        "description": "modelScope.qwen_14b_sft",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\nfrom swift.llm import DatasetName, ModelType, SftArguments, sft_main\nsft_args = SftArguments(\n    model_type=ModelType.qwen_14b_chat,\n    # model_cache_dir=\"Qwen-14B-Chat\",\n    dataset=[DatasetName.alpaca_zh, DatasetName.alpaca_en],\n    train_dataset_sample=500,\n    eval_steps=50,",
        "detail": "modelScope.qwen_14b_sft",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.qwen_14b_sft",
        "description": "modelScope.qwen_14b_sft",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\nfrom swift.llm import DatasetName, ModelType, SftArguments, sft_main\nsft_args = SftArguments(\n    model_type=ModelType.qwen_14b_chat,\n    # model_cache_dir=\"Qwen-14B-Chat\",\n    dataset=[DatasetName.alpaca_zh, DatasetName.alpaca_en],\n    train_dataset_sample=500,\n    eval_steps=50,\n    batch_size=16,",
        "detail": "modelScope.qwen_14b_sft",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "modelScope.qwen_14b_sft",
        "description": "modelScope.qwen_14b_sft",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\nfrom swift.llm import DatasetName, ModelType, SftArguments, sft_main\nsft_args = SftArguments(\n    model_type=ModelType.qwen_14b_chat,\n    # model_cache_dir=\"Qwen-14B-Chat\",\n    dataset=[DatasetName.alpaca_zh, DatasetName.alpaca_en],\n    train_dataset_sample=500,\n    eval_steps=50,\n    batch_size=16,\n    logging_steps=20,",
        "detail": "modelScope.qwen_14b_sft",
        "documentation": {}
    },
    {
        "label": "sft_args",
        "kind": 5,
        "importPath": "modelScope.qwen_14b_sft",
        "description": "modelScope.qwen_14b_sft",
        "peekOfCode": "sft_args = SftArguments(\n    model_type=ModelType.qwen_14b_chat,\n    # model_cache_dir=\"Qwen-14B-Chat\",\n    dataset=[DatasetName.alpaca_zh, DatasetName.alpaca_en],\n    train_dataset_sample=500,\n    eval_steps=50,\n    batch_size=16,\n    logging_steps=20,\n    num_train_epochs=40,\n    learning_rate=1e-4,",
        "detail": "modelScope.qwen_14b_sft",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": "modelScope.qwen_14b_sft",
        "description": "modelScope.qwen_14b_sft",
        "peekOfCode": "output = sft_main(sft_args)\nbest_model_checkpoint = output[\"best_model_checkpoint\"]\nprint(f\"best_model_checkpoint: {best_model_checkpoint}\")",
        "detail": "modelScope.qwen_14b_sft",
        "documentation": {}
    },
    {
        "label": "best_model_checkpoint",
        "kind": 5,
        "importPath": "modelScope.qwen_14b_sft",
        "description": "modelScope.qwen_14b_sft",
        "peekOfCode": "best_model_checkpoint = output[\"best_model_checkpoint\"]\nprint(f\"best_model_checkpoint: {best_model_checkpoint}\")",
        "detail": "modelScope.qwen_14b_sft",
        "documentation": {}
    },
    {
        "label": "model_dir",
        "kind": 5,
        "importPath": "modelScope.test",
        "description": "modelScope.test",
        "peekOfCode": "model_dir = snapshot_download(\"AI-ModelScope/TinyLlama-1.1B-Chat-v1.0\")\npipe = pipeline(\n    \"text-generation\", model=model_dir, torch_dtype=torch.bfloat16, device_map=\"auto\"\n)\n# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },",
        "detail": "modelScope.test",
        "documentation": {}
    },
    {
        "label": "pipe",
        "kind": 5,
        "importPath": "modelScope.test",
        "description": "modelScope.test",
        "peekOfCode": "pipe = pipeline(\n    \"text-generation\", model=model_dir, torch_dtype=torch.bfloat16, device_map=\"auto\"\n)\n# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\"role\": \"user\", \"content\": \"我们能干啥？\"},",
        "detail": "modelScope.test",
        "documentation": {}
    },
    {
        "label": "messages",
        "kind": 5,
        "importPath": "modelScope.test",
        "description": "modelScope.test",
        "peekOfCode": "messages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\"role\": \"user\", \"content\": \"我们能干啥？\"},\n]\nprompt = pipe.tokenizer.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)",
        "detail": "modelScope.test",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "modelScope.test",
        "description": "modelScope.test",
        "peekOfCode": "prompt = pipe.tokenizer.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\noutputs = pipe(\n    prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95\n)\nprint(outputs[0][\"generated_text\"])\n# <|system|>\n# You are a friendly chatbot who always responds in the style of a pirate.</s>\n# <|user|>",
        "detail": "modelScope.test",
        "documentation": {}
    },
    {
        "label": "outputs",
        "kind": 5,
        "importPath": "modelScope.test",
        "description": "modelScope.test",
        "peekOfCode": "outputs = pipe(\n    prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95\n)\nprint(outputs[0][\"generated_text\"])\n# <|system|>\n# You are a friendly chatbot who always responds in the style of a pirate.</s>\n# <|user|>\n# How many helicopters can a human eat in one sitting?</s>\n# <|assistant|>\n# ...",
        "detail": "modelScope.test",
        "documentation": {}
    },
    {
        "label": "ocr_detection",
        "kind": 5,
        "importPath": "modelScope.test01",
        "description": "modelScope.test01",
        "peekOfCode": "ocr_detection = pipeline(\n    Tasks.ocr_detection, model=\"damo/cv_resnet18_ocr-detection-line-level_damo\"\n)\nresult = ocr_detection(\n    \"https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/ocr_detection.jpg\"\n)\nprint(result)",
        "detail": "modelScope.test01",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "modelScope.test01",
        "description": "modelScope.test01",
        "peekOfCode": "result = ocr_detection(\n    \"https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/ocr_detection.jpg\"\n)\nprint(result)",
        "detail": "modelScope.test01",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test02",
        "description": "modelScope.test02",
        "peekOfCode": "model = timm.create_model(\"hf_hub:notmahi/dobb-e\", pretrained=True)\nmodel.eval()",
        "detail": "modelScope.test02",
        "documentation": {}
    },
    {
        "label": "model_path",
        "kind": 5,
        "importPath": "modelScope.test03",
        "description": "modelScope.test03",
        "peekOfCode": "model_path = \"sword_out\"\npipe = StableDiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-2-1\", paddle_dtype=paddle.float32\n)\n# 注意：如果我们想从 HF Hub 加载权重，那么我们需要设置 from_hf_hub=True\npipe.unet.load_attn_procs(model_path, from_hf_hub=False)\nprompt = \"sword_1bug,sword,simple background,upward\"\nimage = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\nimage.save(\"sword.png\")",
        "detail": "modelScope.test03",
        "documentation": {}
    },
    {
        "label": "pipe",
        "kind": 5,
        "importPath": "modelScope.test03",
        "description": "modelScope.test03",
        "peekOfCode": "pipe = StableDiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-2-1\", paddle_dtype=paddle.float32\n)\n# 注意：如果我们想从 HF Hub 加载权重，那么我们需要设置 from_hf_hub=True\npipe.unet.load_attn_procs(model_path, from_hf_hub=False)\nprompt = \"sword_1bug,sword,simple background,upward\"\nimage = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\nimage.save(\"sword.png\")",
        "detail": "modelScope.test03",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "modelScope.test03",
        "description": "modelScope.test03",
        "peekOfCode": "prompt = \"sword_1bug,sword,simple background,upward\"\nimage = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\nimage.save(\"sword.png\")",
        "detail": "modelScope.test03",
        "documentation": {}
    },
    {
        "label": "image",
        "kind": 5,
        "importPath": "modelScope.test03",
        "description": "modelScope.test03",
        "peekOfCode": "image = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\nimage.save(\"sword.png\")",
        "detail": "modelScope.test03",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "modelScope.test04",
        "description": "modelScope.test04",
        "peekOfCode": "data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\ntable = pd.DataFrame.from_dict(data)\nquestion = \"how many movies does Leonardo Di Caprio have?\"\n# pipeline model\n# Note: you must to install torch-scatter first.\ntqa = pipeline(task=\"table-question-answering\", model=\"google/tapas-large-finetuned-wtq\")\n# result\nprint(tqa(table=table, query=question)['cells'][0])\n#53",
        "detail": "modelScope.test04",
        "documentation": {}
    },
    {
        "label": "table",
        "kind": 5,
        "importPath": "modelScope.test04",
        "description": "modelScope.test04",
        "peekOfCode": "table = pd.DataFrame.from_dict(data)\nquestion = \"how many movies does Leonardo Di Caprio have?\"\n# pipeline model\n# Note: you must to install torch-scatter first.\ntqa = pipeline(task=\"table-question-answering\", model=\"google/tapas-large-finetuned-wtq\")\n# result\nprint(tqa(table=table, query=question)['cells'][0])\n#53",
        "detail": "modelScope.test04",
        "documentation": {}
    },
    {
        "label": "question",
        "kind": 5,
        "importPath": "modelScope.test04",
        "description": "modelScope.test04",
        "peekOfCode": "question = \"how many movies does Leonardo Di Caprio have?\"\n# pipeline model\n# Note: you must to install torch-scatter first.\ntqa = pipeline(task=\"table-question-answering\", model=\"google/tapas-large-finetuned-wtq\")\n# result\nprint(tqa(table=table, query=question)['cells'][0])\n#53",
        "detail": "modelScope.test04",
        "documentation": {}
    },
    {
        "label": "tqa",
        "kind": 5,
        "importPath": "modelScope.test04",
        "description": "modelScope.test04",
        "peekOfCode": "tqa = pipeline(task=\"table-question-answering\", model=\"google/tapas-large-finetuned-wtq\")\n# result\nprint(tqa(table=table, query=question)['cells'][0])\n#53",
        "detail": "modelScope.test04",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models import Model\nfrom modelscope.pipelines import pipeline\nfrom modelscope.utils.constant import Tasks",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models import Model\nfrom modelscope.pipelines import pipeline\nfrom modelscope.utils.constant import Tasks\nmodel = \"ZhipuAI/CodeGeeX-Code-Generation-13B\"",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models import Model\nfrom modelscope.pipelines import pipeline\nfrom modelscope.utils.constant import Tasks\nmodel = \"ZhipuAI/CodeGeeX-Code-Generation-13B\"\npipe = pipeline(",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models import Model\nfrom modelscope.pipelines import pipeline\nfrom modelscope.utils.constant import Tasks\nmodel = \"ZhipuAI/CodeGeeX-Code-Generation-13B\"\npipe = pipeline(\n    task=Tasks.code_generation,",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models import Model\nfrom modelscope.pipelines import pipeline\nfrom modelscope.utils.constant import Tasks\nmodel = \"ZhipuAI/CodeGeeX-Code-Generation-13B\"\npipe = pipeline(\n    task=Tasks.code_generation,\n    model=model,",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models import Model\nfrom modelscope.pipelines import pipeline\nfrom modelscope.utils.constant import Tasks\nmodel = \"ZhipuAI/CodeGeeX-Code-Generation-13B\"\npipe = pipeline(\n    task=Tasks.code_generation,\n    model=model,\n)",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models import Model\nfrom modelscope.pipelines import pipeline\nfrom modelscope.utils.constant import Tasks\nmodel = \"ZhipuAI/CodeGeeX-Code-Generation-13B\"\npipe = pipeline(\n    task=Tasks.code_generation,\n    model=model,\n)\ninputs = {",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "model = \"ZhipuAI/CodeGeeX-Code-Generation-13B\"\npipe = pipeline(\n    task=Tasks.code_generation,\n    model=model,\n)\ninputs = {\n    \"prompt\": 'from typing import List\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n    return\",',\n    \"language\": \"Python\",\n}\nresult = pipe(inputs)",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "pipe",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "pipe = pipeline(\n    task=Tasks.code_generation,\n    model=model,\n)\ninputs = {\n    \"prompt\": 'from typing import List\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n    return\",',\n    \"language\": \"Python\",\n}\nresult = pipe(inputs)\nprint(result)",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "inputs",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "inputs = {\n    \"prompt\": 'from typing import List\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n    return\",',\n    \"language\": \"Python\",\n}\nresult = pipe(inputs)\nprint(result)",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "modelScope.test05",
        "description": "modelScope.test05",
        "peekOfCode": "result = pipe(inputs)\nprint(result)",
        "detail": "modelScope.test05",
        "documentation": {}
    },
    {
        "label": "tokenize_function",
        "kind": 2,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "def tokenize_function(examples):\n    return tokenizer(\n        examples[\"sentence1\"],\n        examples[\"sentence2\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=128,\n    )\ntrain_dataset = train_dataset.map(tokenize_function)\nval_dataset = val_dataset.map(tokenize_function)",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator\nfrom swift import Trainer, LoRAConfig, Swift, TrainingArguments",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator\nfrom swift import Trainer, LoRAConfig, Swift, TrainingArguments\nmodel = AutoModelForSequenceClassification.from_pretrained(",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator\nfrom swift import Trainer, LoRAConfig, Swift, TrainingArguments\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator\nfrom swift import Trainer, LoRAConfig, Swift, TrainingArguments\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator\nfrom swift import Trainer, LoRAConfig, Swift, TrainingArguments\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator\nfrom swift import Trainer, LoRAConfig, Swift, TrainingArguments\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset\nfrom transformers import default_data_collator\nfrom swift import Trainer, LoRAConfig, Swift, TrainingArguments\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "model = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "lora_config",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "lora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)\nval_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "model = Swift.prepare_model(model, config=lora_config)\ntrain_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)\nval_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()\n    .select(range(100))",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "train_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"train\")\n    .to_hf_dataset()\n    .select(range(100))\n)\nval_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()\n    .select(range(100))\n)",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "val_dataset",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "val_dataset = (\n    MsDataset.load(\"clue\", subset_name=\"afqmc\", split=\"validation\")\n    .to_hf_dataset()\n    .select(range(100))\n)\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"sentence1\"],\n        examples[\"sentence2\"],\n        padding=\"max_length\",",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "train_dataset = train_dataset.map(tokenize_function)\nval_dataset = val_dataset.map(tokenize_function)\narguments = TrainingArguments(\n    output_dir=\"./outputs\",\n    per_device_train_batch_size=16,\n)\ntrainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "val_dataset",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "val_dataset = val_dataset.map(tokenize_function)\narguments = TrainingArguments(\n    output_dir=\"./outputs\",\n    per_device_train_batch_size=16,\n)\ntrainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "arguments",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "arguments = TrainingArguments(\n    output_dir=\"./outputs\",\n    per_device_train_batch_size=16,\n)\ntrainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=default_data_collator,",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "modelScope.test06",
        "description": "modelScope.test06",
        "peekOfCode": "trainer = Trainer(\n    model,\n    arguments,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=default_data_collator,\n)\ntrainer.train()",
        "detail": "modelScope.test06",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom swift import LoRAConfig, Swift, Trainer\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom swift import LoRAConfig, Swift, Trainer\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom swift import LoRAConfig, Swift, Trainer\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom swift import LoRAConfig, Swift, Trainer\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom swift import LoRAConfig, Swift, Trainer\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom swift import LoRAConfig, Swift, Trainer\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom swift import LoRAConfig, Swift, Trainer\nfrom modelscope import AutoModelForSequenceClassification, AutoTokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "model = AutoModelForSequenceClassification.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(\"start\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(\n    \"AI-ModelScope/bert-base-uncased\", revision=\"v1.0.0\"\n)\nlora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(\"start\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))\noutputs = model(\n    **tokenizer(\"This is an example sentence for classification.\", return_tensors=\"pt\")\n)",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "lora_config",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "lora_config = LoRAConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(\"start\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))\noutputs = model(\n    **tokenizer(\"This is an example sentence for classification.\", return_tensors=\"pt\")\n)\nlogits = outputs.logits\nprobabilities = logits.softmax(dim=1)\nprint(probabilities)",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "model = Swift.from_pretrained(model, model_id=\"./outputs/checkpoint-21\")\nprint(\"start\")\nprint(model(**tokenizer(\"this is a test\", return_tensors=\"pt\")))\noutputs = model(\n    **tokenizer(\"This is an example sentence for classification.\", return_tensors=\"pt\")\n)\nlogits = outputs.logits\nprobabilities = logits.softmax(dim=1)\nprint(probabilities)\npredicted_class_index = probabilities.argmax(dim=1)",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "outputs",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "outputs = model(\n    **tokenizer(\"This is an example sentence for classification.\", return_tensors=\"pt\")\n)\nlogits = outputs.logits\nprobabilities = logits.softmax(dim=1)\nprint(probabilities)\npredicted_class_index = probabilities.argmax(dim=1)\nprint(predicted_class_index)\npredicted_label = tokenizer.decode(predicted_class_index)\nprint(f\"Predicted label: {predicted_label}\")",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "logits",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "logits = outputs.logits\nprobabilities = logits.softmax(dim=1)\nprint(probabilities)\npredicted_class_index = probabilities.argmax(dim=1)\nprint(predicted_class_index)\npredicted_label = tokenizer.decode(predicted_class_index)\nprint(f\"Predicted label: {predicted_label}\")\nprint(\"end\")",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "probabilities",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "probabilities = logits.softmax(dim=1)\nprint(probabilities)\npredicted_class_index = probabilities.argmax(dim=1)\nprint(predicted_class_index)\npredicted_label = tokenizer.decode(predicted_class_index)\nprint(f\"Predicted label: {predicted_label}\")\nprint(\"end\")",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "predicted_class_index",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "predicted_class_index = probabilities.argmax(dim=1)\nprint(predicted_class_index)\npredicted_label = tokenizer.decode(predicted_class_index)\nprint(f\"Predicted label: {predicted_label}\")\nprint(\"end\")",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "predicted_label",
        "kind": 5,
        "importPath": "modelScope.test07",
        "description": "modelScope.test07",
        "peekOfCode": "predicted_label = tokenizer.decode(predicted_class_index)\nprint(f\"Predicted label: {predicted_label}\")\nprint(\"end\")",
        "detail": "modelScope.test07",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "modelScope.test08",
        "description": "modelScope.test08",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models.nlp import SbertForSequenceClassification\nfrom modelscope.models.nlp.structbert import SbertConfig\nfrom swift import LoraConfig, Swift",
        "detail": "modelScope.test08",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test08",
        "description": "modelScope.test08",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models.nlp import SbertForSequenceClassification\nfrom modelscope.models.nlp.structbert import SbertConfig\nfrom swift import LoraConfig, Swift\nmodel = SbertForSequenceClassification(SbertConfig())",
        "detail": "modelScope.test08",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "modelScope.test08",
        "description": "modelScope.test08",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models.nlp import SbertForSequenceClassification\nfrom modelscope.models.nlp.structbert import SbertConfig\nfrom swift import LoraConfig, Swift\nmodel = SbertForSequenceClassification(SbertConfig())\nlora_config = LoraConfig(target_modules=[\"query\", \"key\", \"value\"])",
        "detail": "modelScope.test08",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test08",
        "description": "modelScope.test08",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models.nlp import SbertForSequenceClassification\nfrom modelscope.models.nlp.structbert import SbertConfig\nfrom swift import LoraConfig, Swift\nmodel = SbertForSequenceClassification(SbertConfig())\nlora_config = LoraConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, lora_config)",
        "detail": "modelScope.test08",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test08",
        "description": "modelScope.test08",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models.nlp import SbertForSequenceClassification\nfrom modelscope.models.nlp.structbert import SbertConfig\nfrom swift import LoraConfig, Swift\nmodel = SbertForSequenceClassification(SbertConfig())\nlora_config = LoraConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, lora_config)",
        "detail": "modelScope.test08",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test08",
        "description": "modelScope.test08",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models.nlp import SbertForSequenceClassification\nfrom modelscope.models.nlp.structbert import SbertConfig\nfrom swift import LoraConfig, Swift\nmodel = SbertForSequenceClassification(SbertConfig())\nlora_config = LoraConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, lora_config)",
        "detail": "modelScope.test08",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test08",
        "description": "modelScope.test08",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nfrom modelscope.models.nlp import SbertForSequenceClassification\nfrom modelscope.models.nlp.structbert import SbertConfig\nfrom swift import LoraConfig, Swift\nmodel = SbertForSequenceClassification(SbertConfig())\nlora_config = LoraConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, lora_config)",
        "detail": "modelScope.test08",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test08",
        "description": "modelScope.test08",
        "peekOfCode": "model = SbertForSequenceClassification(SbertConfig())\nlora_config = LoraConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, lora_config)",
        "detail": "modelScope.test08",
        "documentation": {}
    },
    {
        "label": "lora_config",
        "kind": 5,
        "importPath": "modelScope.test08",
        "description": "modelScope.test08",
        "peekOfCode": "lora_config = LoraConfig(target_modules=[\"query\", \"key\", \"value\"])\nmodel = Swift.prepare_model(model, lora_config)",
        "detail": "modelScope.test08",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "modelScope.test08",
        "description": "modelScope.test08",
        "peekOfCode": "model = Swift.prepare_model(model, lora_config)",
        "detail": "modelScope.test08",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nfrom swift.llm import (\n    ModelType, get_vllm_engine, get_default_template_type,",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_DATASETS_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "os.environ[\"HF_DATASETS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nfrom swift.llm import (\n    ModelType, get_vllm_engine, get_default_template_type,\n    get_template, inference_vllm",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"G:\\huggingface\"\nos.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nfrom swift.llm import (\n    ModelType, get_vllm_engine, get_default_template_type,\n    get_template, inference_vllm\n)",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HUGGINGFACE_HUB_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nfrom swift.llm import (\n    ModelType, get_vllm_engine, get_default_template_type,\n    get_template, inference_vllm\n)\nfrom swift.tuners import Swift",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TRANSFORMERS_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "os.environ[\"TRANSFORMERS_CACHE\"] = \"G:\\huggingface\"\nos.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nfrom swift.llm import (\n    ModelType, get_vllm_engine, get_default_template_type,\n    get_template, inference_vllm\n)\nfrom swift.tuners import Swift\nmodel_dir = 'vx_xxx/checkpoint-100-merged'",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "os.environ[\"MODELSCOPE_CACHE\"] = \"G:\\modelscope\"\nos.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nfrom swift.llm import (\n    ModelType, get_vllm_engine, get_default_template_type,\n    get_template, inference_vllm\n)\nfrom swift.tuners import Swift\nmodel_dir = 'vx_xxx/checkpoint-100-merged'\nmodel_type = ModelType.qwen_7b_chat",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MODELSCOPE_MODULES_CACHE\"]",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "os.environ[\"MODELSCOPE_MODULES_CACHE\"] = \"G:\\modelscope\"\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nfrom swift.llm import (\n    ModelType, get_vllm_engine, get_default_template_type,\n    get_template, inference_vllm\n)\nfrom swift.tuners import Swift\nmodel_dir = 'vx_xxx/checkpoint-100-merged'\nmodel_type = ModelType.qwen_7b_chat\ntemplate_type = get_default_template_type(model_type)",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nfrom swift.llm import (\n    ModelType, get_vllm_engine, get_default_template_type,\n    get_template, inference_vllm\n)\nfrom swift.tuners import Swift\nmodel_dir = 'vx_xxx/checkpoint-100-merged'\nmodel_type = ModelType.qwen_7b_chat\ntemplate_type = get_default_template_type(model_type)\nllm_engine = get_vllm_engine(model_type, model_dir=model_dir)",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "model_dir",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "model_dir = 'vx_xxx/checkpoint-100-merged'\nmodel_type = ModelType.qwen_7b_chat\ntemplate_type = get_default_template_type(model_type)\nllm_engine = get_vllm_engine(model_type, model_dir=model_dir)\ntokenizer = llm_engine.tokenizer\ntemplate = get_template(template_type, tokenizer)\nquery = '你好'\nresp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "model_type",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "model_type = ModelType.qwen_7b_chat\ntemplate_type = get_default_template_type(model_type)\nllm_engine = get_vllm_engine(model_type, model_dir=model_dir)\ntokenizer = llm_engine.tokenizer\ntemplate = get_template(template_type, tokenizer)\nquery = '你好'\nresp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "template_type",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "template_type = get_default_template_type(model_type)\nllm_engine = get_vllm_engine(model_type, model_dir=model_dir)\ntokenizer = llm_engine.tokenizer\ntemplate = get_template(template_type, tokenizer)\nquery = '你好'\nresp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "llm_engine",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "llm_engine = get_vllm_engine(model_type, model_dir=model_dir)\ntokenizer = llm_engine.tokenizer\ntemplate = get_template(template_type, tokenizer)\nquery = '你好'\nresp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "tokenizer = llm_engine.tokenizer\ntemplate = get_template(template_type, tokenizer)\nquery = '你好'\nresp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "template = get_template(template_type, tokenizer)\nquery = '你好'\nresp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "query",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "query = '你好'\nresp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "resp",
        "kind": 5,
        "importPath": "modelScope.test09",
        "description": "modelScope.test09",
        "peekOfCode": "resp = inference_vllm(llm_engine, template, [{'query': query}])[0]\nprint(f\"response: {resp['response']}\")\nprint(f\"history: {resp['history']}\")",
        "detail": "modelScope.test09",
        "documentation": {}
    },
    {
        "label": "MySQL",
        "kind": 6,
        "importPath": "my_utils.MysqlUtility",
        "description": "my_utils.MysqlUtility",
        "peekOfCode": "class MySQL:\n    u'''对MySQLdb常用函数进行封装的类'''\n    error_code = ''  # MySQL错误号码\n    _instance = None  # 本类的实例\n    _conn = None  # 数据库conn\n    _cur = None  # 游标\n    _TIMEOUT = 30  # 默认超时30秒\n    _timecount = 0\n    def __init__(self, dbconfig):\n        u'构造器：根据数据库连接参数，创建MySQL连接'",
        "detail": "my_utils.MysqlUtility",
        "documentation": {}
    },
    {
        "label": "paddstr",
        "kind": 2,
        "importPath": "my_utils.MysqlUtility",
        "description": "my_utils.MysqlUtility",
        "peekOfCode": "def paddstr(strx):\n    return \"NULL\" if strx == None else \"\\'\" + MySQLdb.escape_string(str(strx)).decode() + \"\\'\"\ndef paddnum(numx, default='NULL'):\n    return default if numx == None else str(numx)\nclass MySQL:\n    u'''对MySQLdb常用函数进行封装的类'''\n    error_code = ''  # MySQL错误号码\n    _instance = None  # 本类的实例\n    _conn = None  # 数据库conn\n    _cur = None  # 游标",
        "detail": "my_utils.MysqlUtility",
        "documentation": {}
    },
    {
        "label": "paddnum",
        "kind": 2,
        "importPath": "my_utils.MysqlUtility",
        "description": "my_utils.MysqlUtility",
        "peekOfCode": "def paddnum(numx, default='NULL'):\n    return default if numx == None else str(numx)\nclass MySQL:\n    u'''对MySQLdb常用函数进行封装的类'''\n    error_code = ''  # MySQL错误号码\n    _instance = None  # 本类的实例\n    _conn = None  # 数据库conn\n    _cur = None  # 游标\n    _TIMEOUT = 30  # 默认超时30秒\n    _timecount = 0",
        "detail": "my_utils.MysqlUtility",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 2,
        "importPath": "suanfa.main01",
        "description": "suanfa.main01",
        "peekOfCode": "def name(args):\n    print(\"Hello World\" + args)\ndef bubble_sort(arr):\n    n = len(arr)\n    for i in range(n - 1):\n        # 每次遍历将最大的元素冒泡到末尾\n        for j in range(n - 1 - i):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\ndef my_bubble_sort(arr):",
        "detail": "suanfa.main01",
        "documentation": {}
    },
    {
        "label": "bubble_sort",
        "kind": 2,
        "importPath": "suanfa.main01",
        "description": "suanfa.main01",
        "peekOfCode": "def bubble_sort(arr):\n    n = len(arr)\n    for i in range(n - 1):\n        # 每次遍历将最大的元素冒泡到末尾\n        for j in range(n - 1 - i):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\ndef my_bubble_sort(arr):\n    n = len(arr)\n    for i in range(n - 1):",
        "detail": "suanfa.main01",
        "documentation": {}
    },
    {
        "label": "my_bubble_sort",
        "kind": 2,
        "importPath": "suanfa.main01",
        "description": "suanfa.main01",
        "peekOfCode": "def my_bubble_sort(arr):\n    n = len(arr)\n    for i in range(n - 1):\n        for j in range(n - 1 - i):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\ndef insertion_sort(arr):\n    n = len(arr)\n    for i in range(1, n):\n        key = arr[i]",
        "detail": "suanfa.main01",
        "documentation": {}
    },
    {
        "label": "insertion_sort",
        "kind": 2,
        "importPath": "suanfa.main01",
        "description": "suanfa.main01",
        "peekOfCode": "def insertion_sort(arr):\n    n = len(arr)\n    for i in range(1, n):\n        key = arr[i]\n        j = i - 1\n        # 在已排序部分找到合适的插入位置\n        while j >= 0 and arr[j] > key:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key",
        "detail": "suanfa.main01",
        "documentation": {}
    },
    {
        "label": "reader",
        "kind": 5,
        "importPath": "suanfa.mian02",
        "description": "suanfa.mian02",
        "peekOfCode": "reader = easyocr.Reader(\n    [\"en\", \"ch_sim\"],\n    gpu=True,\n)  # this needs to run only once to load the model into memory\n# 图像路径\nimage_path = \"./15889317528424945.png\"\nimage_path_out = \"./results/15889317528424945.png\"\nimage = Image.open(image_path)\n# image = image.resize((800, 600))\n# image = image.resize((800, 300))",
        "detail": "suanfa.mian02",
        "documentation": {}
    },
    {
        "label": "image_path",
        "kind": 5,
        "importPath": "suanfa.mian02",
        "description": "suanfa.mian02",
        "peekOfCode": "image_path = \"./15889317528424945.png\"\nimage_path_out = \"./results/15889317528424945.png\"\nimage = Image.open(image_path)\n# image = image.resize((800, 600))\n# image = image.resize((800, 300))\n# image.show()\nimage.save(image_path_out)\n# result = reader.readtext(\"./image/baidu_image/test5.jpg\", detail=0)\nresults = reader.readtext(image_path_out, detail=0, batch_size=20, paragraph=True)\nprint(\"results==\", results)",
        "detail": "suanfa.mian02",
        "documentation": {}
    },
    {
        "label": "image_path_out",
        "kind": 5,
        "importPath": "suanfa.mian02",
        "description": "suanfa.mian02",
        "peekOfCode": "image_path_out = \"./results/15889317528424945.png\"\nimage = Image.open(image_path)\n# image = image.resize((800, 600))\n# image = image.resize((800, 300))\n# image.show()\nimage.save(image_path_out)\n# result = reader.readtext(\"./image/baidu_image/test5.jpg\", detail=0)\nresults = reader.readtext(image_path_out, detail=0, batch_size=20, paragraph=True)\nprint(\"results==\", results)\nfor result in results:",
        "detail": "suanfa.mian02",
        "documentation": {}
    },
    {
        "label": "image",
        "kind": 5,
        "importPath": "suanfa.mian02",
        "description": "suanfa.mian02",
        "peekOfCode": "image = Image.open(image_path)\n# image = image.resize((800, 600))\n# image = image.resize((800, 300))\n# image.show()\nimage.save(image_path_out)\n# result = reader.readtext(\"./image/baidu_image/test5.jpg\", detail=0)\nresults = reader.readtext(image_path_out, detail=0, batch_size=20, paragraph=True)\nprint(\"results==\", results)\nfor result in results:\n    print(\"result\", result)",
        "detail": "suanfa.mian02",
        "documentation": {}
    },
    {
        "label": "results",
        "kind": 5,
        "importPath": "suanfa.mian02",
        "description": "suanfa.mian02",
        "peekOfCode": "results = reader.readtext(image_path_out, detail=0, batch_size=20, paragraph=True)\nprint(\"results==\", results)\nfor result in results:\n    print(\"result\", result)",
        "detail": "suanfa.mian02",
        "documentation": {}
    },
    {
        "label": "ocr",
        "kind": 5,
        "importPath": "suanfa.mian03",
        "description": "suanfa.mian03",
        "peekOfCode": "ocr = PaddleOCR(\n    lang=\"ch\",\n    # use_gpu=False,\n    use_gpu=True,\n    det_model_dir=\"../paddleORC_model/ch_ppocr_server_v2.0_det_infer/\",\n    cls_model_dir=\"ch_ppocr_mobile_v2.0_cls_infer/\",\n    rec_model_dir=\"ch_ppocr_server_v2.0_rec_infer/\",\n)\n# load dataset\n# img_path = \"./08631508_2.jpg\"",
        "detail": "suanfa.mian03",
        "documentation": {}
    },
    {
        "label": "img_path",
        "kind": 5,
        "importPath": "suanfa.mian03",
        "description": "suanfa.mian03",
        "peekOfCode": "img_path = \"./image/baidu_image/test7.jpg\"\nresult = ocr.ocr(img_path, cls=True)\nfor line in result:\n    print(line)\n    print(line[0][1][0])\n# 注：\n# result是一个list，每个item包含了文本框，文字和识别置信度\n# line的格式为：\n# [[[3.0, 149.0], [43.0, 149.0], [43.0, 163.0], [3.0, 163.0]], ('人心安', 0.6762619018554688)]\n# 文字框 boxes = line[0]，包含文字框的四个角的(x,y)坐标",
        "detail": "suanfa.mian03",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "suanfa.mian03",
        "description": "suanfa.mian03",
        "peekOfCode": "result = ocr.ocr(img_path, cls=True)\nfor line in result:\n    print(line)\n    print(line[0][1][0])\n# 注：\n# result是一个list，每个item包含了文本框，文字和识别置信度\n# line的格式为：\n# [[[3.0, 149.0], [43.0, 149.0], [43.0, 163.0], [3.0, 163.0]], ('人心安', 0.6762619018554688)]\n# 文字框 boxes = line[0]，包含文字框的四个角的(x,y)坐标\n# 文字 txts = line[1][0]",
        "detail": "suanfa.mian03",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "model = models.resnet18(pretrained=True)\n# 修改模型的输出层\nnum_classes = 10  # 替换为目标任务的类别数\nnum_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features, num_classes)\n# 定义损失函数和优化器\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n# 加载数据集并进行训练\ntrain_loader = \"\"  # 替换为训练集的数据加载器",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "num_classes",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "num_classes = 10  # 替换为目标任务的类别数\nnum_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features, num_classes)\n# 定义损失函数和优化器\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n# 加载数据集并进行训练\ntrain_loader = \"\"  # 替换为训练集的数据加载器\nnum_epochs = 10  # 迭代次数\nfor epoch in range(num_epochs):",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "num_features",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "num_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features, num_classes)\n# 定义损失函数和优化器\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n# 加载数据集并进行训练\ntrain_loader = \"\"  # 替换为训练集的数据加载器\nnum_epochs = 10  # 迭代次数\nfor epoch in range(num_epochs):\n    for images, labels in train_loader:",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "model.fc",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "model.fc = nn.Linear(num_features, num_classes)\n# 定义损失函数和优化器\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n# 加载数据集并进行训练\ntrain_loader = \"\"  # 替换为训练集的数据加载器\nnum_epochs = 10  # 迭代次数\nfor epoch in range(num_epochs):\n    for images, labels in train_loader:\n        optimizer.zero_grad()",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "criterion",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "criterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n# 加载数据集并进行训练\ntrain_loader = \"\"  # 替换为训练集的数据加载器\nnum_epochs = 10  # 迭代次数\nfor epoch in range(num_epochs):\n    for images, labels in train_loader:\n        optimizer.zero_grad()\n        # 前向传播\n        outputs = model(images)",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "optimizer",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n# 加载数据集并进行训练\ntrain_loader = \"\"  # 替换为训练集的数据加载器\nnum_epochs = 10  # 迭代次数\nfor epoch in range(num_epochs):\n    for images, labels in train_loader:\n        optimizer.zero_grad()\n        # 前向传播\n        outputs = model(images)\n        loss = criterion(outputs, labels)",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "train_loader",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "train_loader = \"\"  # 替换为训练集的数据加载器\nnum_epochs = 10  # 迭代次数\nfor epoch in range(num_epochs):\n    for images, labels in train_loader:\n        optimizer.zero_grad()\n        # 前向传播\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        # 反向传播和优化\n        loss.backward()",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "weitiao.test01",
        "description": "weitiao.test01",
        "peekOfCode": "num_epochs = 10  # 迭代次数\nfor epoch in range(num_epochs):\n    for images, labels in train_loader:\n        optimizer.zero_grad()\n        # 前向传播\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        # 反向传播和优化\n        loss.backward()\n        optimizer.step()",
        "detail": "weitiao.test01",
        "documentation": {}
    },
    {
        "label": "api_id",
        "kind": 5,
        "importPath": "bot",
        "description": "bot",
        "peekOfCode": "api_id = \"27175683\"\napi_hash = \"ff5bd7a1d985ad6e6db389e4ef104fe6\"\nbot_token = \"6778102881:AAEJ0PvBN8qDjR3MqNSGNFWUXaLacuKbQ7Y\"\napp = Client(\n    \"kjjjkljklbot\",\n    api_id=api_id, api_hash=api_hash,\n    bot_token=bot_token\n)\n@app.on_message()\nasync def echo(client: Client, message: Message):",
        "detail": "bot",
        "documentation": {}
    },
    {
        "label": "api_hash",
        "kind": 5,
        "importPath": "bot",
        "description": "bot",
        "peekOfCode": "api_hash = \"ff5bd7a1d985ad6e6db389e4ef104fe6\"\nbot_token = \"6778102881:AAEJ0PvBN8qDjR3MqNSGNFWUXaLacuKbQ7Y\"\napp = Client(\n    \"kjjjkljklbot\",\n    api_id=api_id, api_hash=api_hash,\n    bot_token=bot_token\n)\n@app.on_message()\nasync def echo(client: Client, message: Message):\n    await message.reply(message.text)",
        "detail": "bot",
        "documentation": {}
    },
    {
        "label": "bot_token",
        "kind": 5,
        "importPath": "bot",
        "description": "bot",
        "peekOfCode": "bot_token = \"6778102881:AAEJ0PvBN8qDjR3MqNSGNFWUXaLacuKbQ7Y\"\napp = Client(\n    \"kjjjkljklbot\",\n    api_id=api_id, api_hash=api_hash,\n    bot_token=bot_token\n)\n@app.on_message()\nasync def echo(client: Client, message: Message):\n    await message.reply(message.text)\napp.run()",
        "detail": "bot",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "bot",
        "description": "bot",
        "peekOfCode": "app = Client(\n    \"kjjjkljklbot\",\n    api_id=api_id, api_hash=api_hash,\n    bot_token=bot_token\n)\n@app.on_message()\nasync def echo(client: Client, message: Message):\n    await message.reply(message.text)\napp.run()",
        "detail": "bot",
        "documentation": {}
    },
    {
        "label": "paddstr",
        "kind": 2,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "def paddstr(strx):\n    return \"NULL\" if strx == None else \"\\'\" + escape_string(str(strx))+ \"\\'\"\ndef deal_sn_data():\n    bizProConn = MySQL(edun_pro_cfg)\n    sql = \"\"\"\n       SELECT deviceType,deviceNo,sn,imsi,iccid,simPhone,isActive,activeDate,checkCode,modelId,userId,createtime,updateTime,d_LoginUserId,issub,vehicleId,partner,validateStatus,validateBy,validateNums,soft_version,imei,deviceId,source,networkCardNubmer FROM d_track_info dti WHERE dti.sn IN (\t\n       SELECT dti.sn\n       FROM d_vehicle dv,\n       d_track_info dti,\n       d_class dc",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "deal_sn_data",
        "kind": 2,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "def deal_sn_data():\n    bizProConn = MySQL(edun_pro_cfg)\n    sql = \"\"\"\n       SELECT deviceType,deviceNo,sn,imsi,iccid,simPhone,isActive,activeDate,checkCode,modelId,userId,createtime,updateTime,d_LoginUserId,issub,vehicleId,partner,validateStatus,validateBy,validateNums,soft_version,imei,deviceId,source,networkCardNubmer FROM d_track_info dti WHERE dti.sn IN (\t\n       SELECT dti.sn\n       FROM d_vehicle dv,\n       d_track_info dti,\n       d_class dc\n       WHERE dti.vehicleId = dv.vehicleId\n       AND dc.classid = dv.classId",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "logger = logging.getLogger(\"deal_equity_data\")\nlogger.setLevel(logging.DEBUG)\nfileHandler = logging.FileHandler(\"import.log\")\nfileHandler.setLevel(logging.DEBUG)\nformatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\ncnslhandler = logging.StreamHandler()\ncnslhandler.setLevel(logging.DEBUG)\ncnslhandler.setFormatter(formatter)\nlogger.addHandler(cnslhandler)\nedun_pro_cfg = {",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "fileHandler",
        "kind": 5,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "fileHandler = logging.FileHandler(\"import.log\")\nfileHandler.setLevel(logging.DEBUG)\nformatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\ncnslhandler = logging.StreamHandler()\ncnslhandler.setLevel(logging.DEBUG)\ncnslhandler.setFormatter(formatter)\nlogger.addHandler(cnslhandler)\nedun_pro_cfg = {\n    \"host\": \"192.168.5.22\",\n    \"port\": 3307,",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "formatter",
        "kind": 5,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\ncnslhandler = logging.StreamHandler()\ncnslhandler.setLevel(logging.DEBUG)\ncnslhandler.setFormatter(formatter)\nlogger.addHandler(cnslhandler)\nedun_pro_cfg = {\n    \"host\": \"192.168.5.22\",\n    \"port\": 3307,\n    \"user\": \"user_yangll\",\n    \"passwd\": \"vGxw9jWg\",",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "cnslhandler",
        "kind": 5,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "cnslhandler = logging.StreamHandler()\ncnslhandler.setLevel(logging.DEBUG)\ncnslhandler.setFormatter(formatter)\nlogger.addHandler(cnslhandler)\nedun_pro_cfg = {\n    \"host\": \"192.168.5.22\",\n    \"port\": 3307,\n    \"user\": \"user_yangll\",\n    \"passwd\": \"vGxw9jWg\",\n    \"db\": \"gps\",",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "edun_pro_cfg",
        "kind": 5,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "edun_pro_cfg = {\n    \"host\": \"192.168.5.22\",\n    \"port\": 3307,\n    \"user\": \"user_yangll\",\n    \"passwd\": \"vGxw9jWg\",\n    \"db\": \"gps\",\n    \"charset\": \"utf8\",\n}\nedun_test_cfg = {\n    \"host\": \"192.168.3.222\",",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "edun_test_cfg",
        "kind": 5,
        "importPath": "deal_sn_test01",
        "description": "deal_sn_test01",
        "peekOfCode": "edun_test_cfg = {\n    \"host\": \"192.168.3.222\",\n    \"port\": 3307,\n    \"user\": \"root\",\n    \"passwd\": \"Msd^*$@online\",\n    \"db\": \"newgps\",\n    \"charset\": \"utf8\",\n}\nfrom pymysql.converters import escape_string\n#",
        "detail": "deal_sn_test01",
        "documentation": {}
    },
    {
        "label": "demoFindLogClass",
        "kind": 6,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "class demoFindLogClass(object):\n    def __init__(self):\n        pass\n    def demoFindLog(self):\n        deault_lng = 110.35\n        deault_lat = 34.62\n        with open(\"biz-gps-kafka-pazlpush.log.2023-07-28.99\", \"r\", encoding='UTF-8') as f:\n            content_list = f.readlines()\n        for content_line in content_list:\n            if \"定位数据推送数据的实体===\" in content_line:",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "parse_json",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def parse_json():\n    path = \"H:\\PythonWorkSpaces\\deal_lnglat_data\"\n    files = os.listdir(path)\n    s = []\n    for file in files:\n        print(file)\n        # 读取文件\n        # f = open(\"biz-gps-kafka-pazlpush.log.2023-07-28.99\", 'r', encoding='UTF-8')\n        # lines = f.readlines()\n        # for line in lines:",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "print_hi",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def print_hi(name):\n    print(f'Hi, {name}')\nclass demoFindLogClass(object):\n    def __init__(self):\n        pass\n    def demoFindLog(self):\n        deault_lng = 110.35\n        deault_lat = 34.62\n        with open(\"biz-gps-kafka-pazlpush.log.2023-07-28.99\", \"r\", encoding='UTF-8') as f:\n            content_list = f.readlines()",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "print_others",
        "kind": 2,
        "importPath": "main_bot",
        "description": "main_bot",
        "peekOfCode": "def print_others(msg):\n    print(msg)\n# 回复 my_friend 的消息 (优先匹配后注册的函数!)\n@bot.register(my_friend)\ndef reply_my_friend(msg):\n    return \"received: {} ({})\".format(msg.text, msg.type)\n# 自动接受新的好友请求\n@bot.register(msg_types=FRIENDS)\ndef auto_accept_friends(msg):\n    # 接受好友请求",
        "detail": "main_bot",
        "documentation": {}
    },
    {
        "label": "reply_my_friend",
        "kind": 2,
        "importPath": "main_bot",
        "description": "main_bot",
        "peekOfCode": "def reply_my_friend(msg):\n    return \"received: {} ({})\".format(msg.text, msg.type)\n# 自动接受新的好友请求\n@bot.register(msg_types=FRIENDS)\ndef auto_accept_friends(msg):\n    # 接受好友请求\n    new_friend = msg.card.accept()\n    # 向新的好友发送消息\n    new_friend.send(\"哈哈，我自动接受了你的好友请求\")",
        "detail": "main_bot",
        "documentation": {}
    },
    {
        "label": "auto_accept_friends",
        "kind": 2,
        "importPath": "main_bot",
        "description": "main_bot",
        "peekOfCode": "def auto_accept_friends(msg):\n    # 接受好友请求\n    new_friend = msg.card.accept()\n    # 向新的好友发送消息\n    new_friend.send(\"哈哈，我自动接受了你的好友请求\")",
        "detail": "main_bot",
        "documentation": {}
    },
    {
        "label": "bot",
        "kind": 5,
        "importPath": "main_bot",
        "description": "main_bot",
        "peekOfCode": "bot = Bot()\n# **找到好友:**\n# 搜索名称含有 \"游否\" 的男性深圳好友\nmy_friend = bot.friends().search(\"游否\", sex=MALE, city=\"深圳\")[0]\n# **发送消息:**\n# 发送文本给好友\nmy_friend.send(\"Hello WeChat!\")\n# 发送图片\nmy_friend.send_image(\"my_picture.jpg\")\n# **自动响应各类消息:**",
        "detail": "main_bot",
        "documentation": {}
    },
    {
        "label": "my_friend",
        "kind": 5,
        "importPath": "main_bot",
        "description": "main_bot",
        "peekOfCode": "my_friend = bot.friends().search(\"游否\", sex=MALE, city=\"深圳\")[0]\n# **发送消息:**\n# 发送文本给好友\nmy_friend.send(\"Hello WeChat!\")\n# 发送图片\nmy_friend.send_image(\"my_picture.jpg\")\n# **自动响应各类消息:**\n# 打印来自其他好友、群聊和公众号的消息\n@bot.register()\ndef print_others(msg):",
        "detail": "main_bot",
        "documentation": {}
    },
    {
        "label": "reply_msg",
        "kind": 2,
        "importPath": "main_itchat",
        "description": "main_itchat",
        "peekOfCode": "def reply_msg(msg):\n    print(\"收到一条信息：\", msg.text)\nif __name__ == \"__main__\":\n    itchat.auto_login()\n    time.sleep(5)\n    itchat.send(\"文件助手你好哦\", toUserName=\"filehelper\")\n    itchat.run()",
        "detail": "main_itchat",
        "documentation": {}
    },
    {
        "label": "friends",
        "kind": 5,
        "importPath": "main_itchat01",
        "description": "main_itchat01",
        "peekOfCode": "friends = itchat.get_friends()\n# 我们可以使用json库将好友列表转换成json格式\nprint(json.dumps(friends))\n# 运行程序\nitchat.run()",
        "detail": "main_itchat01",
        "documentation": {}
    },
    {
        "label": "my_method",
        "kind": 2,
        "importPath": "main_test01",
        "description": "main_test01",
        "peekOfCode": "def my_method():\n    print(\"你好\")\n    # requests.post()\n# end def\ndef bot_chat():\n    \"\"\"\n    Purpose:\n    \"\"\"\n    print(\"nihao\")\n# end def",
        "detail": "main_test01",
        "documentation": {}
    },
    {
        "label": "bot_chat",
        "kind": 2,
        "importPath": "main_test01",
        "description": "main_test01",
        "peekOfCode": "def bot_chat():\n    \"\"\"\n    Purpose:\n    \"\"\"\n    print(\"nihao\")\n# end def\nif __name__ == \"__main__\":\n    my_method()\n    bot_chat()\n    print(\"nihao\")",
        "detail": "main_test01",
        "documentation": {}
    }
]